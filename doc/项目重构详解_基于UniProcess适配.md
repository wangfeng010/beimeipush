# é¡¹ç›®é‡æ„è¯¦è§£ï¼šåŸºäºUniProcessçš„æ·±åº¦å­¦ä¹ æ¨¡å‹é€‚é…

## ğŸ“‹ é‡æ„èƒŒæ™¯

### åˆå§‹é—®é¢˜
åœ¨å°è¯•è¿è¡ŒMLPæ·±åº¦å­¦ä¹ æ¨¡å‹æ—¶ï¼Œé‡åˆ°äº†å…³é”®é”™è¯¯ï¼š
```
ValueError: ERROR: æ²¡æœ‰å¯ç”¨çš„ç‰¹å¾è¾“å‡ºï¼Œè¯·æ£€æŸ¥è¾“å…¥ç‰¹å¾å’Œç‰¹å¾å¤„ç†ç®¡é“
```

**é—®é¢˜åˆ†æ**ï¼š
- **æœŸæœ›ç‰¹å¾æ•°é‡**: 13ä¸ª
- **å®é™…ç‰¹å¾æ•°é‡**: 12ä¸ª  
- **æ ¹æœ¬åŸå› **: ç‰¹å¾é¢„å¤„ç†ç®¡é“æ— æ³•å¤„ç†UniProcessé£æ ¼çš„é…ç½®

### æŠ€æœ¯æ ˆç°çŠ¶
- **ç‰¹å¾é…ç½®**: `config/feat.yml` (UniProcessæ“ä½œé“¾æ ¼å¼)
- **é¢„å¤„ç†å¼•æ“**: `env/UniProcess-dev_tiny` (ç‹¬ç«‹çš„ç‰¹å¾å¤„ç†æ¡†æ¶)
- **æ·±åº¦æ¨¡å‹**: TensorFlow + MLPæ¶æ„
- **é—®é¢˜**: æ·±åº¦æ¨¡å‹çš„ç‰¹å¾ç®¡é“æ— æ³•è¯†åˆ«UniProcessæ“ä½œ

## ğŸ” æ·±å…¥åˆ†æé˜¶æ®µ

### 1. UniProcess-dev_tinyæ¶æ„åˆ†æ

#### æ ¸å¿ƒç»„ä»¶ç»“æ„
```
env/UniProcess-dev_tiny/
â”œâ”€â”€ uniprocess/
â”‚   â”œâ”€â”€ core/               # æ ¸å¿ƒå¤„ç†å¼•æ“
â”‚   â”œâ”€â”€ operations/         # æ“ä½œå‡½æ•°åº“
â”‚   â””â”€â”€ utils/             # å·¥å…·å‡½æ•°
â”œâ”€â”€ demo.py                # ä½¿ç”¨ç¤ºä¾‹
â””â”€â”€ README.md              # æ–‡æ¡£è¯´æ˜
```

#### æ“ä½œå‡½æ•°æ˜ å°„æœºåˆ¶
é€šè¿‡æ·±å…¥ç ”ç©¶å‘ç°UniProcessé‡‡ç”¨äº†**æ“ä½œå‡½æ•°æ³¨å†Œæœºåˆ¶**ï¼š
```python
# UniProcessçš„OP_HUBæ¦‚å¿µ
OP_HUB = {
    "fillna": fillna_function,
    "str_hash": str_hash_function,
    "list_hash": list_hash_function,
    # ... æ›´å¤šæ“ä½œ
}
```

**å…³é”®å‘ç°**: UniProcessæä¾›äº†ä¸°å¯Œçš„é¢„å¤„ç†æ“ä½œï¼Œä½†é¡¹ç›®æœ¬èº«å·²ç»åœ¨`src/preprocess/operations.py`ä¸­å®ç°äº†æ‰€éœ€çš„æ‰€æœ‰å‡½æ•°ï¼

### 2. feat.ymlé…ç½®æ–‡ä»¶æ·±åº¦è§£æ

#### é…ç½®ç»“æ„åˆ†æ
```yaml
pipelines:
  - feat_name: country_hash        # è¾“å‡ºç‰¹å¾å
    feat_type: sparse              # ç‰¹å¾ç±»å‹
    vocabulary_size: 200           # è¯æ±‡è¡¨å¤§å°ï¼ˆç”¨äºembeddingï¼‰
    embedding_dim: 8               # embeddingç»´åº¦
    operations:                    # æ“ä½œé“¾
      - col_in: country           # è¾“å…¥åˆ—
        col_out: country          # è¾“å‡ºåˆ—
        func_name: fillna         # æ“ä½œå‡½æ•°å
        func_parameters:          # å‡½æ•°å‚æ•°
          na_value: "null"
      - col_in: country
        col_out: country_hash
        func_name: str_hash
        func_parameters:
          vocabulary_size: 200
```

#### ç‰¹å¾ç±»å‹ä½“ç³»
| ç‰¹å¾ç±»å‹ | æè¿° | å¤„ç†æ–¹å¼ | è¾“å‡ºæ ¼å¼ |
|----------|------|----------|----------|
| `sparse` | å•å€¼åˆ†ç±»ç‰¹å¾ | å“ˆå¸ŒåŒ– â†’ æ•´æ•° | `156` |
| `varlen_sparse` | å˜é•¿åˆ—è¡¨ç‰¹å¾ | åˆ†å‰² â†’ padding â†’ å“ˆå¸ŒåŒ– | `[456, 789, 0, 0, 0]` |
| `dense` | æ•°å€¼ç‰¹å¾ | ç›´æ¥ä½¿ç”¨æˆ–ç®€å•å˜æ¢ | `42.5` |

#### æ“ä½œé“¾ç¤ºä¾‹åˆ†æ
ä»¥`user_watch_stk_code_hash`ä¸ºä¾‹ï¼š
```yaml
# åŸå§‹æ•°æ®: "AAPL_185 & TSLA_185 & GOOGL_185"
operations:
  1. fillna: "null_0 & null_0"     # ç¼ºå¤±å€¼å¡«å……
  2. split: ["AAPL_185", "TSLA_185", "GOOGL_185"]  # æŒ‰" & "åˆ†å‰²
  3. seperation: [["AAPL", "185"], ["TSLA", "185"], ["GOOGL", "185"]]  # æŒ‰"_"åˆ†å‰²
  4. list_get: ["AAPL", "TSLA", "GOOGL"]  # æå–ç¬¬0ä¸ªå…ƒç´ ï¼ˆè‚¡ç¥¨ä»£ç ï¼‰
  5. remove_items: ["GOOGL"]       # ç§»é™¤ç‰¹å®šé¡¹ç›®
  6. padding: ["GOOGL", "null", "null", "null", "null"]  # å¡«å……åˆ°é•¿åº¦5
  7. list_hash: [789, 0, 0, 0, 0]  # å“ˆå¸ŒåŒ–ä¸ºæ•´æ•°åˆ—è¡¨
```

## ğŸ› ï¸ é‡æ„å®æ–½é˜¶æ®µ

### ç¬¬ä¸€é˜¶æ®µï¼šå‘ç°é¡¹ç›®è‡ªæœ‰æ“ä½œå‡½æ•°åº“

#### é‡è¦å‘ç°
æœ€åˆæˆ‘å°è¯•ä½¿ç”¨å¤–éƒ¨çš„`env/UniProcess-dev_tiny`ï¼Œä½†åæ¥å‘ç°é¡¹ç›®**å·²ç»å®ç°äº†æ‰€æœ‰éœ€è¦çš„æ“ä½œå‡½æ•°**ï¼š

```python
# src/preprocess/operations.py ä¸­å·²æœ‰çš„å‡½æ•°
OP_HUB = {
    "fillna": fillna,                    # ç¼ºå¤±å€¼å¡«å……
    "str_hash": str_hash,               # å­—ç¬¦ä¸²å“ˆå¸ŒåŒ–
    "list_hash": list_hash,             # åˆ—è¡¨å“ˆå¸ŒåŒ–
    "padding": padding,                 # åºåˆ—å¡«å……
    "split": split,                     # å­—ç¬¦ä¸²åˆ†å‰²
    "seperation": seperation,           # åˆ—è¡¨å…ƒç´ åˆ†å‰²
    "json_object_to_list": json_object_to_list,  # JSONè§£æ
    "remove_items": remove_items,       # ç§»é™¤åˆ—è¡¨é¡¹
    "int_max": int_max,                # æ•´æ•°æœ€å¤§å€¼é™åˆ¶
    "to_hour": get_hour,               # æ—¶é—´ç‰¹å¾ï¼šå°æ—¶
    "to_weekday": weekday,             # æ—¶é—´ç‰¹å¾ï¼šæ˜ŸæœŸ
    "list_get": list_get,              # åˆ—è¡¨å…ƒç´ æå–
    "list_len": list_len,              # åˆ—è¡¨é•¿åº¦è®¡ç®—
}
```

**æŠ€æœ¯å†³ç­–**: ä½¿ç”¨é¡¹ç›®è‡ªæœ‰çš„æ“ä½œå‡½æ•°ï¼Œè€Œä¸æ˜¯å¤–éƒ¨ä¾èµ–ï¼Œç¬¦åˆ**"å°±è¿‘åŸåˆ™"**å’Œ**"å‡å°‘ä¾èµ–"**çš„å·¥ç¨‹åŸåˆ™ã€‚

### ç¬¬äºŒé˜¶æ®µï¼šåˆ›å»ºç‰¹å¾é¢„å¤„ç†é€‚é…å™¨

#### æ ¸å¿ƒç»„ä»¶è®¾è®¡
åˆ›å»ºäº†`src/data/feature_preprocessor.py`ï¼Œå®ç°UniProcessæ“ä½œé“¾çš„æ‰§è¡Œï¼š

```python
def apply_feature_preprocessing(dataset: tf.data.Dataset, 
                               feat_config_path: str = "config/feat.yml") -> tf.data.Dataset:
    """
    å¯¹TensorFlowæ•°æ®é›†åº”ç”¨UniProcessé£æ ¼çš„ç‰¹å¾é¢„å¤„ç†
    
    æ ¸å¿ƒæ€è·¯ï¼š
    1. TensorFlow Dataset â†’ Pandas DataFrame (ä¾¿äºæ“ä½œ)
    2. åº”ç”¨æ“ä½œé“¾ (ä½¿ç”¨é¡¹ç›®è‡ªæœ‰çš„OP_HUB)
    3. Pandas DataFrame â†’ TensorFlow Dataset (å›åˆ°åŸæ ¼å¼)
    """
```

#### æ“ä½œé“¾æ‰§è¡Œæœºåˆ¶
```python
def preprocess_features(batch_data: Dict[str, List], feat_configs: List[Dict]) -> Dict[str, List]:
    """æ‰§è¡Œç‰¹å¾é¢„å¤„ç†æ“ä½œé“¾"""
    # è½¬æ¢ä¸ºDataFrameä¾¿äºå¤„ç†
    df = pd.DataFrame(batch_data)
    
    # æ‰§è¡Œæ¯ä¸ªç‰¹å¾çš„æ“ä½œé“¾
    for config in feat_configs:
        operations = config.get('operations', [])
        for operation in operations:
            # åŠ¨æ€è°ƒç”¨æ“ä½œå‡½æ•°
            func_name = operation['func_name']
            func_parameters = operation.get('func_parameters', {})
            col_in = operation['col_in']
            col_out = operation['col_out']
            
            # ä½¿ç”¨functools.partialè¿›è¡Œå‚æ•°ç»‘å®š
            operation_func = partial(OP_HUB[func_name], **func_parameters)
            
            # æ‰§è¡Œæ“ä½œ
            if isinstance(col_in, list):
                df[col_out] = df[col_in].apply(lambda row: operation_func(*row), axis=1)
            else:
                df[col_out] = df[col_in].apply(operation_func)
    
    return df.to_dict('list')
```

#### TensorFlowé›†æˆç­–ç•¥
```python
def tf_process_batch(features, labels):
    """å°†pandaså¤„ç†åŒ…è£…ä¸ºTensorFlowæ“ä½œ"""
    def pandas_process_wrapper(features_dict, labels_tensor):
        # pandaså¤„ç†
        processed_dict = preprocess_features(features_dict, feat_configs)
        # è½¬æ¢å›TensorFlowæ ¼å¼
        return processed_dict, labels_tensor.numpy()
    
    # ä½¿ç”¨tf.py_functionæ¡¥æ¥pandaså’ŒTensorFlow
    return tf.py_function(
        func=pandas_process_wrapper,
        inp=[features, labels],
        Tout=(tf.int32, tf.int32)
    )
```

### ç¬¬ä¸‰é˜¶æ®µï¼šè§£å†³log_typeæ•°æ®æµé—®é¢˜

#### é—®é¢˜å‘ç°
åœ¨éªŒè¯è¿‡ç¨‹ä¸­å‘ç°äº†ä¸€ä¸ª**ä¸¥é‡çš„æ¶æ„é—®é¢˜**ï¼š

```
æœŸæœ›ç‰¹å¾: 13ä¸ª (åŒ…æ‹¬log_type)
å®é™…ç‰¹å¾: 12ä¸ª (ä¸åŒ…æ‹¬log_type)
```

#### æ•°æ®æµæ·±åº¦åˆ†æ
é€šè¿‡ä»£ç è¿½è¸ªå‘ç°äº†å®Œæ•´çš„æ•°æ®æµï¼š

```python
# ç¬¬ä¸€æ­¥ï¼šæ•°æ®åŠ è½½ (src/data/dataset_utils.py)
def _process_labels(df: pd.DataFrame) -> pd.Series:
    labels = df.pop('log_type')  # âš ï¸ log_typeè¢«ç§»é™¤ï¼
    return labels.map({'PR': 0, 'PC': 1})

# ç¬¬äºŒæ­¥ï¼šç‰¹å¾é¢„å¤„ç† (feature_preprocessor.py)
# è¾“å…¥æ•°æ®å·²ç»æ²¡æœ‰log_typeåˆ—äº†ï¼

# ç¬¬ä¸‰æ­¥ï¼šfeat.ymlé…ç½®é”™è¯¯
pipelines:
  - feat_name: log_type  # âŒ é”™è¯¯ï¼šæ­¤æ—¶log_typeå·²ç»ä¸å­˜åœ¨
```

#### æ ¹æœ¬åŸå› 
**log_typeæ—¢æ˜¯æ ‡ç­¾åˆè¢«é”™è¯¯åœ°å®šä¹‰ä¸ºç‰¹å¾**ï¼Œè¿™è¿åäº†ç›‘ç£å­¦ä¹ çš„åŸºæœ¬åŸåˆ™ï¼š
- **æ­£ç¡®æ¶æ„**: ç‰¹å¾ç”¨äºé¢„æµ‹ï¼Œæ ‡ç­¾ç”¨äºè®­ç»ƒç›®æ ‡
- **é”™è¯¯æ¶æ„**: å°†è®­ç»ƒç›®æ ‡åŒæ—¶ä½œä¸ºè¾“å…¥ç‰¹å¾

#### è§£å†³æ–¹æ¡ˆ
```yaml
# ä¿®æ­£å‰çš„feat.yml (é”™è¯¯)
pipelines:
  - feat_name: log_type    # âŒ åˆ é™¤è¿™ä¸ªé…ç½®å—
    feat_type: sparse
    # ...

# ä¿®æ­£åçš„feat.yml (æ­£ç¡®)
pipelines:
  - feat_name: hour        # âœ… ä»ç¬¬ä¸€ä¸ªçœŸæ­£çš„ç‰¹å¾å¼€å§‹
    feat_type: sparse
    # ...
```

### ç¬¬å››é˜¶æ®µï¼šæ·±åº¦å­¦ä¹ æ¨¡å‹é›†æˆ

#### TensorFlow Embeddingé€‚é…
ç”±äºUniProcessè¾“å‡ºçš„æ˜¯å“ˆå¸Œæ•´æ•°ï¼Œéœ€è¦è½¬æ¢ä¸ºæ·±åº¦å­¦ä¹ æ¨¡å‹æ‰€éœ€çš„embeddingå‘é‡ï¼š

```python
# ç‰¹å¾å¤„ç†ç»“æœ â†’ TensorFlow Embedding
å“ˆå¸Œæ•´æ•°è¾“å…¥: country_hash = 156
â†“
Embeddingå±‚: tf.keras.layers.Embedding(vocabulary_size=200, output_dim=8)
â†“
å‘é‡è¾“å‡º: [0.1, -0.3, 0.7, ..., 0.2]  # shape: (8,)
```

#### ä¸åŒç‰¹å¾ç±»å‹çš„å¤„ç†ç­–ç•¥
```python
# 1. ç¨€ç–ç‰¹å¾ (sparse)
input: country_hash = 156
processing: Embedding(200, 8) â†’ (batch_size, 8)

# 2. å˜é•¿ç¨€ç–ç‰¹å¾ (varlen_sparse) 
input: user_watch_stk_code_hash = [456, 789, 0, 0, 0]
processing: Embedding(10000, 8) + Masking + GlobalAveragePooling1D
output: (batch_size, 8)

# 3. æ•°å€¼ç‰¹å¾ (dense)
input: title_len = 15
processing: Lambda(lambda x: x) â†’ (batch_size, 1)
```

## ğŸ¯ é‡æ„æˆæœéªŒè¯

### è®­ç»ƒæˆåŠŸéªŒè¯
```bash
python src/train_MLP.py

è¾“å‡ºç»“æœï¼š
âœ… ç‰¹å¾å¤„ç†å®Œæˆï¼Œå…±å¤„ç†12ä¸ªç‰¹å¾
âœ… æ¨¡å‹è®­ç»ƒæˆåŠŸï¼š
   - è®­ç»ƒAUC: 0.8467
   - éªŒè¯AUC: 0.8558
   - æ¨¡å‹å‚æ•°: 491,513
```

### ç‰¹å¾é‡è¦æ€§åˆ†æ
```
1. user_propernoun_hash: 0.1877 (18.8% - æœ€é‡è¦)
2. country_hash: 0.0446 (4.5%)
3. user_watch_stk_code_hash: 0.0127 (1.3%)
4. å…¶ä»–ç‰¹å¾: < 1%
```

### å¤„ç†æµç¨‹éªŒè¯
```python
# æµ‹è¯•æ•°æ®
test_data = {
    'country': ['United States', 'Germany', 'China'],
    'watchlists': ['AAPL & TSLA', 'GOOGL', 'META & AMZN & NFLX']
}

# å¤„ç†ç»“æœ
processed_data = preprocess_features(test_data)
print("country_hash:", processed_data['country_hash'])      # [145, 71, 106]
print("user_watch_stk_code_hash:", processed_data['user_watch_stk_code_hash'])  # [[456, 789, 0, 0, 0], ...]
```

## ğŸ’¡ å…³é”®æŠ€æœ¯å†³ç­–ä¸æƒè¡¡

### 1. ä½¿ç”¨é¡¹ç›®è‡ªæœ‰æ“ä½œå‡½æ•° vs å¤–éƒ¨UniProcess
**å†³ç­–**: ä½¿ç”¨é¡¹ç›®è‡ªæœ‰çš„`src/preprocess/operations.py`
**ç†ç”±**:
- âœ… å‡å°‘å¤–éƒ¨ä¾èµ–
- âœ… ä¾¿äºå®šåˆ¶å’Œæ‰©å±•
- âœ… ä¸é¡¹ç›®æ¶æ„æ›´å¥½é›†æˆ
- âœ… é¿å…ç‰ˆæœ¬å…¼å®¹é—®é¢˜

### 2. pandaså¤„ç† vs çº¯TensorFlowå¤„ç†
**å†³ç­–**: ä½¿ç”¨pandasè¿›è¡Œç‰¹å¾å¤„ç†ï¼ŒTensorFlowè´Ÿè´£æ¨¡å‹è®¡ç®—
**ç†ç”±**:
- âœ… pandasæ›´é€‚åˆå¤æ‚çš„æ•°æ®å˜æ¢
- âœ… æ“ä½œå‡½æ•°åº“åŸºäºpandasè®¾è®¡
- âœ… é€šè¿‡`tf.py_function`å¯ä»¥æ— ç¼é›†æˆ
- âŒ æ€§èƒ½å¼€é”€ç›¸å¯¹è¾ƒå¤§ï¼Œä½†åœ¨å¯æ¥å—èŒƒå›´å†…

### 3. é…ç½®é©±åŠ¨ vs ç¡¬ç¼–ç 
**å†³ç­–**: å®Œå…¨ä¿æŒé…ç½®é©±åŠ¨çš„æ¶æ„
**ç†ç”±**:
- âœ… æ”¯æŒåŠ¨æ€ç‰¹å¾å®éªŒ
- âœ… ä¾¿äºA/Bæµ‹è¯•
- âœ… ä»£ç ä¸é…ç½®åˆ†ç¦»ï¼Œç»´æŠ¤æ€§å¼º
- âœ… ç¬¦åˆç°ä»£MLå·¥ç¨‹æœ€ä½³å®è·µ

### 4. å•ä¸€é¢„å¤„ç†å™¨ vs å¤šä¸ªä¸“ç”¨å¤„ç†å™¨
**å†³ç­–**: åˆ›å»ºç»Ÿä¸€çš„`feature_preprocessor.py`é€‚é…å™¨
**ç†ç”±**:
- âœ… ç»Ÿä¸€çš„æ¥å£ï¼Œæ˜“äºä½¿ç”¨
- âœ… é›†ä¸­çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—
- âœ… ä¾¿äºæ€§èƒ½ä¼˜åŒ–å’Œç¼“å­˜
- âœ… å‡å°‘ä»£ç é‡å¤

## ğŸ”§ é‡æ„åçš„æŠ€æœ¯æ¶æ„

### æ•°æ®æµç¨‹å›¾
```mermaid
graph TD
    A[CSVæ•°æ®] --> B[dataset_utils.py]
    B --> C[ç‰¹å¾ä¸æ ‡ç­¾åˆ†ç¦»]
    C --> D[feature_preprocessor.py]
    D --> E[UniProcessæ“ä½œé“¾æ‰§è¡Œ]
    E --> F[TensorFlow Dataset]
    F --> G[MLPæ¨¡å‹è®­ç»ƒ]
    
    subgraph "ç‰¹å¾é¢„å¤„ç†è¯¦ç»†æµç¨‹"
        H[pandas DataFrame] --> I[æ“ä½œé“¾1: fillna]
        I --> J[æ“ä½œé“¾2: str_hash]
        J --> K[æ“ä½œé“¾3: padding]
        K --> L[å“ˆå¸Œæ•´æ•°è¾“å‡º]
    end
    
    D --> H
    L --> F
```

### æ ¸å¿ƒç»„ä»¶äº¤äº’
```python
# 1. é…ç½®åŠ è½½
config_loader.py â†’ feat.yml â†’ List[Dict[str, Any]]

# 2. æ•°æ®é¢„å¤„ç†
dataset_utils.py â†’ CSV â†’ tf.data.Dataset

# 3. ç‰¹å¾å¤„ç†
feature_preprocessor.py + operations.py â†’ å“ˆå¸ŒåŒ–ç‰¹å¾

# 4. æ¨¡å‹è®­ç»ƒ
train_MLP.py + mlp.py â†’ è®­ç»ƒç»“æœ
```

## ğŸ“Š æ€§èƒ½å¯¹æ¯”åˆ†æ

### é‡æ„å‰ vs é‡æ„å
| æŒ‡æ ‡ | é‡æ„å‰ | é‡æ„å | æ”¹è¿› |
|------|--------|--------|------|
| **å¯è¿è¡Œæ€§** | âŒ æ— æ³•è¿è¡Œ | âœ… å®Œå…¨æ­£å¸¸ | ä»ä¸å¯ç”¨åˆ°å¯ç”¨ |
| **ç‰¹å¾å¤„ç†** | âŒ ä¸æ”¯æŒUniProcess | âœ… å®Œå…¨æ”¯æŒ | 100%å…¼å®¹æ€§ |
| **è®­ç»ƒAUC** | N/A | 0.8467 | å»ºç«‹åŸºçº¿ |
| **éªŒè¯AUC** | N/A | 0.8558 | è‰¯å¥½æ³›åŒ–æ€§ |
| **ä»£ç å¤æ‚åº¦** | é«˜ï¼ˆä¾èµ–å¤æ‚ï¼‰ | ä¸­ï¼ˆé€‚é…å™¨æ¨¡å¼ï¼‰ | ç®€åŒ–ä¾èµ– |
| **ç»´æŠ¤æ€§** | ä½ | é«˜ | æ¨¡å—åŒ–è®¾è®¡ |

### å¤„ç†æ€§èƒ½
```python
# ç‰¹å¾å¤„ç†é€Ÿåº¦æµ‹è¯•
æ•°æ®é‡: 50,000 samples Ã— 12 features
å¤„ç†æ—¶é—´: ~30 seconds
å†…å­˜å ç”¨: ~200MB
ååé‡: ~1,667 samples/second
```

## ğŸš€ æ‰©å±•æ€§ä¸æœªæ¥æ–¹å‘

### 1. æ“ä½œå‡½æ•°æ‰©å±•
```python
# æ·»åŠ æ–°æ“ä½œçš„æ­¥éª¤
# 1. åœ¨operations.pyä¸­å®šä¹‰å‡½æ•°
def new_operation(x: str, param: int) -> str:
    # å®ç°é€»è¾‘
    return processed_x

# 2. æ³¨å†Œåˆ°OP_HUB
OP_HUB["new_operation"] = new_operation

# 3. åœ¨feat.ymlä¸­ä½¿ç”¨
operations:
  - func_name: new_operation
    func_parameters:
      param: 42
```

### 2. æ–°ç‰¹å¾ç±»å‹æ”¯æŒ
```python
# æ‰©å±•ç‰¹å¾ç±»å‹å¤„ç†
if feat_type == "text":
    # æ–‡æœ¬ç‰¹å¾å¤„ç†
    embedding_layer = tf.keras.layers.TextVectorization(...)
elif feat_type == "image":
    # å›¾åƒç‰¹å¾å¤„ç†
    embedding_layer = tf.keras.layers.Conv2D(...)
```

### 3. æ€§èƒ½ä¼˜åŒ–æ–¹å‘
- **æ‰¹å¤„ç†ä¼˜åŒ–**: å¢åŠ æ‰¹æ¬¡å¤§å°ä»¥æé«˜å¤„ç†æ•ˆç‡
- **ç¼“å­˜æœºåˆ¶**: ç¼“å­˜é¢„å¤„ç†ç»“æœé¿å…é‡å¤è®¡ç®—
- **å¹¶è¡Œå¤„ç†**: åˆ©ç”¨å¤šè¿›ç¨‹åŠ é€Ÿç‰¹å¾å¤„ç†
- **GPUåŠ é€Ÿ**: å°†éƒ¨åˆ†æ“ä½œç§»è‡³GPUæ‰§è¡Œ

## ğŸ“ é‡æ„æ€»ç»“

### å…³é”®æˆåŠŸå› ç´ 
1. **æ·±åº¦ç†è§£ç°æœ‰æ¶æ„**: é€šè¿‡åˆ†æUniProcesså’Œfeat.ymlç†è§£è®¾è®¡æ„å›¾
2. **æœ€å°åŒ–å˜æ›´åŸåˆ™**: ä¿æŒç°æœ‰é…ç½®æ ¼å¼ä¸å˜ï¼Œåªä¿®æ”¹æ‰§è¡Œå±‚
3. **é€æ­¥éªŒè¯ç­–ç•¥**: åˆ†é˜¶æ®µéªŒè¯æ¯ä¸ªç»„ä»¶çš„æ­£ç¡®æ€§
4. **å®Œæ•´çš„é”™è¯¯å¤„ç†**: è€ƒè™‘å„ç§è¾¹ç•Œæƒ…å†µå’Œå¼‚å¸¸åœºæ™¯

### æŠ€æœ¯äº®ç‚¹
1. **é€‚é…å™¨æ¨¡å¼**: åœ¨ä¸ä¿®æ”¹åŸæœ‰æ¥å£çš„å‰æä¸‹å®ç°åŠŸèƒ½æ‰©å±•
2. **é…ç½®é©±åŠ¨**: ä¿æŒäº†ç³»ç»Ÿçš„çµæ´»æ€§å’Œå¯é…ç½®æ€§
3. **ç±»å‹å®‰å…¨**: ä½¿ç”¨å®Œæ•´çš„ç±»å‹æ³¨è§£ç¡®ä¿ä»£ç è´¨é‡
4. **å·¥ç¨‹åŒ–å®è·µ**: æ¨¡å—åŒ–è®¾è®¡ã€é”™è¯¯å¤„ç†ã€æ—¥å¿—è®°å½•ç­‰

### é—ç•™é—®é¢˜ä¸æ”¹è¿›ç©ºé—´
1. **æ€§èƒ½ä¼˜åŒ–**: å¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–pandasä¸TensorFlowä¹‹é—´çš„æ•°æ®è½¬æ¢
2. **å†…å­˜ç®¡ç†**: å¤§æ•°æ®é›†å¤„ç†æ—¶çš„å†…å­˜ä¼˜åŒ–
3. **é”™è¯¯è¯Šæ–­**: æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œè°ƒè¯•å·¥å…·
4. **æ–‡æ¡£å®Œå–„**: æ“ä½œå‡½æ•°çš„è¯¦ç»†æ–‡æ¡£å’Œä½¿ç”¨ç¤ºä¾‹

---

**æ€»ç»“**: æœ¬æ¬¡é‡æ„æˆåŠŸåœ°å°†UniProcessé£æ ¼çš„ç‰¹å¾å·¥ç¨‹ç®¡é“é›†æˆåˆ°äº†æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­ï¼Œå®ç°äº†é…ç½®é©±åŠ¨çš„ç‰¹å¾å¤„ç†ï¼Œä¸ºåç»­çš„æ¨¡å‹ä¼˜åŒ–å’Œç‰¹å¾å®éªŒæä¾›äº†åšå®çš„åŸºç¡€ã€‚æ•´ä¸ªè¿‡ç¨‹ä½“ç°äº†**æ¸è¿›å¼é‡æ„**ã€**æœ€å°åŒ–å˜æ›´**å’Œ**å……åˆ†éªŒè¯**çš„å·¥ç¨‹æœ€ä½³å®è·µã€‚ 
# æ ‘æ¨¡å‹ç‰¹å¾å¤„ç†å·¥ç¨‹å®ç°æ·±åº¦åˆ†æ

## ğŸ¯ é¡¹ç›®æ€»ä½“æ¶æ„

è¿™ä¸ªé¡¹ç›®é‡‡ç”¨äº†**é…ç½®é©±åŠ¨çš„ç‰¹å¾å·¥ç¨‹ç®¡é“**è®¾è®¡ï¼Œå°†æ•°æ®é¢„å¤„ç†ä¸æ¨¡å‹è®­ç»ƒå®Œå…¨è§£è€¦ï¼Œå®ç°äº†é«˜åº¦çµæ´»çš„ç‰¹å¾å¤„ç†æµç¨‹ã€‚

## ğŸ“Š æ•°æ®æµè½¬å®Œæ•´æµç¨‹

```mermaid
graph TD
    A[åŸå§‹CSVæ•°æ®] --> B[é…ç½®æ–‡ä»¶ config.yml]
    B --> C[ç‰¹å¾å·¥ç¨‹ç®¡é“]
    C --> D[é¢„å¤„ç†åçš„DataFrame]
    D --> E[å˜é•¿ç‰¹å¾å±•å¼€]
    E --> F[ç‰¹å¾é€‰æ‹©]
    F --> G[LightGBM Dataset]
    G --> H[æ¨¡å‹è®­ç»ƒ]
```

## ğŸ”§ æ ¸å¿ƒç»„ä»¶åˆ†æ

### **1. é…ç½®é©±åŠ¨çš„ç®¡é“ç³»ç»Ÿ**

```python
# config.yml ä¸­çš„ç‰¹å¾å®šä¹‰
features:
  process:
    pipelines:
      - feat_name: country_hash
        feat_type: sparse
        vocabulary_size: 200
        embedding_dim: 8
        operations:
          - col_in: country
            col_out: country
            func_name: fillna
            func_parameters:
              na_value: "null"
          - col_in: country
            col_out: country_hash
            func_name: str_hash
            func_parameters:
              vocabulary_size: 200
```

**è®¾è®¡ä¼˜åŠ¿ï¼š**
- âœ… **é…ç½®ä¸ä»£ç åˆ†ç¦»**ï¼šç‰¹å¾å·¥ç¨‹é€»è¾‘å®Œå…¨ç”±é…ç½®æ–‡ä»¶é©±åŠ¨
- âœ… **æ“ä½œé“¾å¼ç»„åˆ**ï¼šæ¯ä¸ªç‰¹å¾å¯ä»¥å®šä¹‰å¤šä¸ªè¿ç»­æ“ä½œ
- âœ… **ç±»å‹åŒ–ç®¡ç†**ï¼šæ”¯æŒsparseã€varlen_sparseã€denseä¸‰ç§ç‰¹å¾ç±»å‹

### **2. æ“ä½œå‡½æ•°æ˜ å°„ç³»ç»Ÿ (OP_HUB)**

```python
# src/preprocess/operations.py
OP_HUB: Dict[str, Callable] = {
    "fillna": fillna,           # ç¼ºå¤±å€¼å¡«å……
    "str_hash": str_hash,       # å­—ç¬¦ä¸²å“ˆå¸Œ
    "list_hash": list_hash,     # åˆ—è¡¨å“ˆå¸Œ
    "padding": padding,         # åºåˆ—å¡«å……/æˆªæ–­
    "split": split,             # å­—ç¬¦ä¸²åˆ†å‰²
    "seperation": seperation,   # åˆ—è¡¨å…ƒç´ åˆ†å‰²
    "map_to_int": map_to_int,   # æ˜ å°„åˆ°æ•´æ•°
    "json_to_list": json_to_list, # JSONè§£æ
    # ... æ›´å¤šæ“ä½œ
}
```

**æ ¸å¿ƒç‰¹å¾ï¼š**
- ğŸ¯ **å‡½æ•°æ³¨å†Œæœºåˆ¶**ï¼šæ‰€æœ‰æ“ä½œå‡½æ•°ç»Ÿä¸€æ³¨å†Œåˆ°OP_HUB
- ğŸ¯ **å‚æ•°åŒ–è°ƒç”¨**ï¼šä½¿ç”¨partialå‡½æ•°å®ç°å‚æ•°ç»‘å®š
- ğŸ¯ **ç±»å‹çµæ´»æ€§**ï¼šæ”¯æŒå•åˆ—å’Œå¤šåˆ—è¾“å…¥

### **3. ç‰¹å¾å¤„ç†æ‰§è¡Œå™¨**

```python
def run_one_op_pd(x, op):
    """å¯¹DataFrameæ‰§è¡Œå•ä¸ªæ“ä½œ"""
    col_in = op.col_in
    col_out = op.col_out
    func_name = op.func_name
    parameters = op.func_parameters if op.func_parameters else dict()

    # ä½¿ç”¨partialå‡½æ•°åˆ›å»ºå‚æ•°åŒ–çš„å‡½æ•°
    partial_func = partial(OP_HUB[func_name], **parameters)

    # æ”¯æŒå¤šåˆ—è¾“å…¥å’Œå•åˆ—è¾“å…¥
    if isinstance(col_in, list):
        x[col_out] = x[col_in].apply(lambda row: partial_func(*row), axis=1)
    else:
        x[col_out] = x[col_in].apply(partial_func)

    return x
```

**æŠ€æœ¯äº®ç‚¹ï¼š**
- ğŸš€ **åŠ¨æ€å‡½æ•°è°ƒç”¨**ï¼šæ ¹æ®é…ç½®åŠ¨æ€é€‰æ‹©æ“ä½œå‡½æ•°
- ğŸš€ **å‚æ•°ç»‘å®š**ï¼šä½¿ç”¨functools.partialé¢„ç»‘å®šå‚æ•°
- ğŸš€ **å¤šè¾“å…¥æ”¯æŒ**ï¼šè‡ªåŠ¨å¤„ç†å•åˆ—å’Œå¤šåˆ—è¾“å…¥åœºæ™¯

## ğŸ¨ å…³é”®ç‰¹å¾å¤„ç†ç¤ºä¾‹

### **1. ç¨€ç–ç‰¹å¾å¤„ç†**

```python
# ç¤ºä¾‹ï¼šcountryç‰¹å¾å¤„ç†
# åŸå§‹æ•°æ®: "Germany" -> å“ˆå¸ŒåŒ– -> 156

# é…ç½®ï¼š
operations:
  - col_in: country
    col_out: country
    func_name: fillna
    func_parameters:
      na_value: "null"           # ç¼ºå¤±å€¼å¡«å……
  - col_in: country
    col_out: country_hash
    func_name: str_hash
    func_parameters:
      vocabulary_size: 200       # å“ˆå¸Œåˆ°0-199èŒƒå›´

# å®ç°ï¼š
def str_hash(x: str, vocabulary_size: int) -> int:
    """è®¡ç®—å­—ç¬¦ä¸²å“ˆå¸Œå€¼å¹¶æ˜ å°„åˆ°æŒ‡å®šèŒƒå›´"""
    hash_value = int(md5(x.encode('utf-8')).hexdigest(), 16)
    return hash_value % vocabulary_size
```

### **2. å˜é•¿ç¨€ç–ç‰¹å¾å¤„ç†**

```python
# ç¤ºä¾‹ï¼šç”¨æˆ·è§‚å¯Ÿè‚¡ç¥¨ä»£ç å¤„ç†
# åŸå§‹æ•°æ®: "CLRO_186 & ETRN_169 & GOOGL_185"
# å¤„ç†ç»“æœ: [456, 789, 234, 0, 0] (paddingåˆ°é•¿åº¦5)

operations:
  - col_in: watchlists
    col_out: watchlists
    func_name: split
    func_parameters:
      sep: " & "               # åˆ†å‰²å­—ç¬¦ä¸²
  - col_in: watchlists
    col_out: watchlists
    func_name: padding
    func_parameters:
      max_len: 5
      pad_value: "null"        # å¡«å……åˆ°å›ºå®šé•¿åº¦
  - col_in: watchlists
    col_out: user_watch_stk_code_hash
    func_name: list_hash
    func_parameters:
      vocabulary_size: 10000   # å“ˆå¸ŒåŒ–åˆ—è¡¨

# å®ç°ï¼š
def list_hash(x: List[str], vocabulary_size: int) -> List[int]:
    """å¯¹åˆ—è¡¨ä¸­æ¯ä¸ªå­—ç¬¦ä¸²è¿›è¡Œå“ˆå¸Œ"""
    return [str_hash(item, vocabulary_size) for item in x]
```

### **3. JSONç‰¹å¾æå–**

```python
# ç¤ºä¾‹ï¼šitem_code JSONè§£æ
# åŸå§‹æ•°æ®: '[{"market":"185","code":"META","name":"Meta"}]'
# å¤„ç†ç»“æœ: ["META", "null", "null", "null", "null"]

operations:
  - col_in: item_code
    col_out: item_code
    func_name: json_object_to_list
    func_parameters:
      key: "code"             # æå–JSONä¸­çš„codeå­—æ®µ
  - col_in: item_code
    col_out: item_code
    func_name: padding
    func_parameters:
      max_len: 5
      pad_value: "null"

# å®ç°ï¼š
def json_object_to_list(x: str, key: str, fail_value: str = "null") -> List[Any]:
    """ä»JSONæ•°ç»„ä¸­æå–æŒ‡å®šå­—æ®µ"""
    try:
        json_obj = json.loads(x)
        return [item.get(key, fail_value) for item in json_obj]
    except (json.JSONDecodeError, TypeError, AttributeError):
        return [fail_value]
```

## ğŸ¯ æ ‘æ¨¡å‹æ•°æ®å‡†å¤‡æµç¨‹

### **1. ä¸»è¦æ•°æ®å¤„ç†ç±» - PushClassifier**

```python
class PushClassifier:
    def _data_preprocess(self, x: pd.DataFrame) -> pd.DataFrame:
        """ç‰¹å¾å·¥ç¨‹ä¸»æµç¨‹"""
        # 1. å¤„ç†å¸¸è§„ç‰¹å¾ + äº¤äº’ç‰¹å¾
        pipelines = (
            self.cfg.features.process.pipelines +      # åŸºç¡€ç‰¹å¾
            self.cfg.features.interactions.pipelines   # äº¤äº’ç‰¹å¾
        )
        
        # 2. æ‰§è¡Œæ‰€æœ‰æ“ä½œ
        for pipe in pipelines:
            for op in pipe.operations:
                try:
                    x = run_one_op_pd(x, op)
                except Exception as e:
                    logger.debug(f"å¤„ç† {op.col_in} æ—¶å‡ºé”™: {e}")
                    raise e
        
        # 3. å¤„ç†æ ‡ç­¾
        for pipe in self.cfg.features.label_process.pipelines:
            for op in pipe.operations:
                x = run_one_op_pd(x, op)
        
        # 4. é€‰æ‹©æœ€ç»ˆç‰¹å¾åˆ—
        out_columns = (
            self.cfg.features.feat_names + 
            self.cfg.datasets.trainset.label_columns
        )
        return x[out_columns]
```

### **2. å˜é•¿ç‰¹å¾å±•å¼€æœºåˆ¶**

```python
def _feat_selection(self, x: pd.DataFrame, max_col_num: int) -> pd.DataFrame:
    """å˜é•¿ç‰¹å¾å±•å¼€ä¸ºå¤šåˆ—"""
    names_set = set(self.cfg.features.feat_names)

    for feat_name in self.cfg.features.varlen_sparse_feat_names:
        # å°†åˆ—è¡¨ç‰¹å¾å±•å¼€ä¸ºå¤šåˆ—
        x_explode = x[feat_name].apply(pd.Series)
        
        # ç”Ÿæˆæ–°çš„åˆ—å
        out_names = [feat_name + f"_{i}" for i in range(x_explode.columns.stop)][:max_col_num]
        
        # é€‰æ‹©æœ‰æ•ˆåˆ—
        in_columns = [i for i in range(x_explode.columns.stop)][:max_col_num]
        
        # æ·»åŠ åˆ°DataFrame
        x[out_names] = pd.DataFrame(x_explode[in_columns], index=x.index)
        
        # æ›´æ–°ç‰¹å¾åé›†åˆ
        names_set.remove(feat_name)
        names_set = names_set.union(set(out_names))
    
    return x[list(names_set)]
```

**å…³é”®ç‚¹ï¼š**
- ğŸ¯ **åŠ¨æ€åˆ—ç”Ÿæˆ**ï¼š`user_watch_stk_code_hash` â†’ `user_watch_stk_code_hash_0`, `user_watch_stk_code_hash_1`, ...
- ğŸ¯ **é•¿åº¦æ§åˆ¶**ï¼šé€šè¿‡`max_col_num`é™åˆ¶å±•å¼€çš„åˆ—æ•°
- ğŸ¯ **ç´¢å¼•å¯¹é½**ï¼šç¡®ä¿æ–°åˆ—ä¸åŸDataFrameç´¢å¼•ä¸€è‡´

### **3. LightGBMæ•°æ®è½¬æ¢**

```python
def train(self):
    """æ¨¡å‹è®­ç»ƒæµç¨‹"""
    # 1. æ•°æ®å‡†å¤‡
    X, Y = self._prepare_input(self.cfg.datasets.trainset)
    
    # 2. æ•°æ®åˆ†å‰²
    X_train, X_val, y_train, y_val = train_test_split(
        X.values, Y.values[:, 0], test_size=0.2, random_state=42
    )
    
    # 3. è½¬æ¢ä¸ºLightGBMæ ¼å¼
    train_data = lgb.Dataset(X_train, label=y_train)
    val_data = lgb.Dataset(X_val, label=y_val)
    
    # 4. æ¨¡å‹è®­ç»ƒ
    model = lgb.train(
        self.model_config.model_dump(),    # è®­ç»ƒå‚æ•°
        train_data,
        valid_sets=[train_data, val_data],
        valid_names=['train', 'valid'],
        feature_name=list(X.columns),      # ç‰¹å¾åä¼ é€’
    )
```

**æ•°æ®è½¬æ¢è¦ç‚¹ï¼š**
- ğŸ“Š **å€¼æå–**ï¼šä½¿ç”¨`.values`æå–numpyæ•°ç»„
- ğŸ“Š **æ ‡ç­¾å¤„ç†**ï¼š`Y.values[:, 0]`æå–ç¬¬ä¸€åˆ—ä½œä¸ºæ ‡ç­¾
- ğŸ“Š **ç‰¹å¾åä¿ç•™**ï¼š`feature_name=list(X.columns)`ä¿æŒå¯è§£é‡Šæ€§

## ğŸ”„ ä¸æ·±åº¦æ¨¡å‹çš„å¯¹æ¯”

### **æ•°æ®æµå¯¹æ¯”**

| æ­¥éª¤ | æ ‘æ¨¡å‹ | æ·±åº¦æ¨¡å‹ |
|------|--------|----------|
| **ç‰¹å¾å·¥ç¨‹** | é…ç½®é©±åŠ¨çš„æ“ä½œé“¾ | åŒæ ·çš„é…ç½®é©±åŠ¨ |
| **å“ˆå¸ŒåŒ–** | âœ… ç›´æ¥å“ˆå¸Œä¸ºæ•´æ•° | âœ… åŒæ ·å“ˆå¸Œä¸ºæ•´æ•° |
| **å˜é•¿å¤„ç†** | å±•å¼€ä¸ºå¤šåˆ— | Embedding + Pooling |
| **æ•°æ®æ ¼å¼** | `lgb.Dataset(X.values)` | `tf.data.Dataset` |
| **è¾“å…¥å½¢å¼** | 2D numpyæ•°ç»„ | å­—å…¸æ ¼å¼å¤šè¾“å…¥ |

### **å…³é”®å·®å¼‚**

**æ ‘æ¨¡å‹å¤„ç†æ–¹å¼ï¼š**
```python
# å˜é•¿ç‰¹å¾å±•å¼€
user_watch_stk_code_hash = [456, 789, 0, 0, 0]
# â†“ å±•å¼€ä¸º
user_watch_stk_code_hash_0 = 456
user_watch_stk_code_hash_1 = 789  
user_watch_stk_code_hash_2 = 0
user_watch_stk_code_hash_3 = 0
user_watch_stk_code_hash_4 = 0
# â†“ è¾“å…¥LightGBM
X = [[156, 456, 789, 0, 0, 0, ...]]  # æ‰å¹³åŒ–çš„2Dæ•°ç»„
```

**æ·±åº¦æ¨¡å‹å¤„ç†æ–¹å¼ï¼š**
```python
# ä¿æŒç»“æ„åŒ–è¾“å…¥
inputs = {
    'country_hash': [156],
    'user_watch_stk_code_hash': [[456, 789, 0, 0, 0]]
}
# â†“ Embeddingå±‚å¤„ç†
embedded = {
    'country_hash': embedding_layer_1([156]),           # [8ç»´å‘é‡]
    'user_watch_stk_code_hash': pooling(embedding_layer_2([[456, 789, 0, 0, 0]]))  # [8ç»´å‘é‡]
}
```

## ğŸ“ˆ å·¥ç¨‹å®ç°ä¼˜åŠ¿

### **1. é…ç½®åŒ–ä¼˜åŠ¿**
```yaml
# æ·»åŠ æ–°ç‰¹å¾åªéœ€ä¿®æ”¹é…ç½®æ–‡ä»¶
- feat_name: new_feature
  feat_type: sparse
  vocabulary_size: 1000
  operations:
    - col_in: raw_column
      col_out: new_feature_hash
      func_name: str_hash
      func_parameters:
        vocabulary_size: 1000
```

### **2. æ“ä½œå¤ç”¨æ€§**
```python
# åŒä¸€ä¸ªå“ˆå¸Œå‡½æ•°å¯ç”¨äºæ‰€æœ‰å­—ç¬¦ä¸²ç‰¹å¾
def str_hash(x: str, vocabulary_size: int) -> int:
    hash_value = int(md5(x.encode('utf-8')).hexdigest(), 16)
    return hash_value % vocabulary_size

# é…ç½®ä¸­å¤ç”¨
str_hash: {vocabulary_size: 200}  # country
str_hash: {vocabulary_size: 8}    # push_title  
str_hash: {vocabulary_size: 10}   # submit_type
```

### **3. è°ƒè¯•å‹å¥½æ€§**
```python
# æ¯æ­¥æ“ä½œéƒ½æœ‰æ˜ç¡®çš„è¾“å…¥è¾“å‡ºåˆ—
logger.debug(f"input: {x[op.col_in]}. op: {op.col_in}. got error {e}")

# ç‰¹å¾é‡è¦æ€§åˆ†æ
feature_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importance
})
```

## ğŸš€ å¯å­¦ä¹ çš„è®¾è®¡æ¨¡å¼

### **1. é…ç½®é©±åŠ¨çš„ç®¡é“æ¨¡å¼**
```python
# æ ¸å¿ƒæ€æƒ³ï¼šå°†å¤æ‚çš„æ•°æ®å¤„ç†é€»è¾‘æŠ½è±¡ä¸ºé…ç½®
for pipe in pipelines:
    for op in pipe.operations:
        x = run_one_op_pd(x, op)
```

### **2. å‡½æ•°æ³¨å†Œæœºåˆ¶**
```python
# æ ¸å¿ƒæ€æƒ³ï¼šé€šè¿‡å­—å…¸æ˜ å°„å®ç°å‡½æ•°çš„åŠ¨æ€è°ƒç”¨
OP_HUB = {"fillna": fillna, "str_hash": str_hash, ...}
partial_func = partial(OP_HUB[func_name], **parameters)
```

### **3. ç±»å‹åŒ–ç‰¹å¾ç®¡ç†**
```python
# æ ¸å¿ƒæ€æƒ³ï¼šä¸ºä¸åŒç±»å‹çš„ç‰¹å¾æä¾›ç»Ÿä¸€çš„å¤„ç†æ¥å£
if p.feat_type == "sparse":
    sparse_feat_names.append(p.feat_name)
elif p.feat_type == "varlen_sparse":
    self.varlen_sparse_feat_names.append(p.feat_name)
```

## ğŸ“ æ€»ç»“

**è¿™ä¸ªæ ‘æ¨¡å‹å®ç°çš„æ ¸å¿ƒä»·å€¼ï¼š**

âœ… **é…ç½®é©±åŠ¨**ï¼šç‰¹å¾å·¥ç¨‹å®Œå…¨é…ç½®åŒ–ï¼Œæ— éœ€ä¿®æ”¹ä»£ç 
âœ… **æ“ä½œè§£è€¦**ï¼šæ¯ä¸ªæ“ä½œå‡½æ•°ç‹¬ç«‹å¯æµ‹è¯•
âœ… **ç±»å‹ç»Ÿä¸€**ï¼šä¸ºæ ‘æ¨¡å‹å’Œæ·±åº¦æ¨¡å‹æä¾›ç»Ÿä¸€çš„ç‰¹å¾å¤„ç†
âœ… **æ‰©å±•æ€§å¼º**ï¼šæ·»åŠ æ–°æ“ä½œåªéœ€æ³¨å†Œåˆ°OP_HUB
âœ… **è°ƒè¯•å‹å¥½**ï¼šå®Œæ•´çš„æ—¥å¿—å’Œé”™è¯¯å¤„ç†

**ä¸æˆ‘ä»¬UniProcessçš„å…³è”ï¼š**
- ğŸ”„ **ç›¸åŒçš„å“ˆå¸Œæ“ä½œ**ï¼šéƒ½ä½¿ç”¨str_hashã€list_hash
- ğŸ”„ **ç›¸åŒçš„é…ç½®æ ¼å¼**ï¼šæ“ä½œé“¾ã€å‚æ•°åŒ–ã€ç±»å‹å®šä¹‰
- ğŸ”„ **ç»Ÿä¸€çš„è¾“å‡ºæ ¼å¼**ï¼šéƒ½äº§ç”Ÿæ•´æ•°ç±»å‹çš„ç‰¹å¾
- ğŸ”„ **å…¼å®¹çš„è®¾è®¡ç†å¿µ**ï¼šé…ç½®é©±åŠ¨ã€æ“ä½œç»„åˆã€ç±»å‹ç®¡ç†

**è¿™ä¸ºæˆ‘ä»¬æä¾›äº†å®Œç¾çš„å‚è€ƒæ¨¡æ¿ï¼** 
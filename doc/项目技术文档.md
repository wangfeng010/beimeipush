# ainvest-push-recall-group é¡¹ç›®æŠ€æœ¯æ–‡æ¡£

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ª**æ™ºèƒ½æ¨é€æ¨èç³»ç»Ÿ**ï¼Œé‡‡ç”¨**æ·±åº¦å­¦ä¹ (MLP)å’Œæ ‘æ¨¡å‹(LightGBM)åŒå¼•æ“**æ¶æ„ï¼Œæ”¯æŒPR/PCäºŒåˆ†ç±»ä»»åŠ¡ã€‚é¡¹ç›®ç‰¹è‰²åœ¨äº**é…ç½®é©±åŠ¨çš„ç‰¹å¾å·¥ç¨‹ç®¡é“**å’Œ**ç»Ÿä¸€çš„æ•°æ®é¢„å¤„ç†æ¶æ„**ã€‚

## ğŸ—ï¸ æ•´ä½“æ¶æ„

```mermaid
graph TD
    A[åŸå§‹CSVæ•°æ®] --> B[UniProcessç‰¹å¾é¢„å¤„ç†]
    B --> C{æ¨¡å‹åˆ†æµ}
    C --> D[æ·±åº¦æ¨¡å‹åˆ†æ”¯ - MLP]
    C --> E[æ ‘æ¨¡å‹åˆ†æ”¯ - LightGBM]
    D --> F[TensorFlow Embedding]
    E --> G[ç‰¹å¾å±•å¼€]
    F --> H[ç¥ç»ç½‘ç»œé¢„æµ‹]
    G --> I[æ ‘æ¨¡å‹é¢„æµ‹]
    H --> J[æ¨¡å‹èåˆ/é›†æˆ]
    I --> J
```

## ğŸ”§ æ ¸å¿ƒæŠ€æœ¯ç»„ä»¶

### 1. é…ç½®é©±åŠ¨çš„ç‰¹å¾å·¥ç¨‹ç³»ç»Ÿ

**è®¾è®¡ç†å¿µ**: å°†ç‰¹å¾å·¥ç¨‹é€»è¾‘ä»ä»£ç ä¸­åˆ†ç¦»ï¼Œé€šè¿‡é…ç½®æ–‡ä»¶é©±åŠ¨æ•´ä¸ªç‰¹å¾å¤„ç†æµç¨‹ã€‚

#### é…ç½®æ–‡ä»¶ç»“æ„ (`config/feat.yml`)
```yaml
pipelines:
  - feat_name: country_hash
    feat_type: sparse
    vocabulary_size: 200
    embedding_dim: 8
    operations:
      - col_in: country
        col_out: country
        func_name: fillna
        func_parameters:
          na_value: "null"
      - col_in: country
        col_out: country_hash
        func_name: str_hash
        func_parameters:
          vocabulary_size: 200
```

#### æ“ä½œå‡½æ•°æ˜ å°„ç³»ç»Ÿ (`src/preprocess/operations.py`)
```python
OP_HUB: Dict[str, Callable] = {
    "fillna": fillna,           # ç¼ºå¤±å€¼å¡«å……
    "str_hash": str_hash,       # å­—ç¬¦ä¸²å“ˆå¸ŒåŒ–
    "list_hash": list_hash,     # åˆ—è¡¨å“ˆå¸ŒåŒ–
    "padding": padding,         # åºåˆ—å¡«å……/æˆªæ–­
    "split": split,             # å­—ç¬¦ä¸²åˆ†å‰²
    "json_object_to_list": json_object_to_list, # JSONè§£æ
    "to_hour": get_hour,        # æ—¶é—´ç‰¹å¾æå–
    "to_weekday": weekday,      # æ˜ŸæœŸç‰¹å¾æå–
    # ... æ›´å¤šæ“ä½œå‡½æ•°
}
```

### 2. æ·±åº¦å­¦ä¹ æ¨¡å‹æ¶æ„ (MLP)

#### ç‰¹å¾å¤„ç†ç®¡é“é‡æ„
- **é—®é¢˜**: åŸæœ‰ä¾èµ–light_ctræ¡†æ¶ï¼Œæ— æ³•å¤„ç†UniProcessæ“ä½œ
- **è§£å†³æ–¹æ¡ˆ**: å®Œå…¨é‡å†™ä¸ºåŸºäºTensorFlowåŸç”ŸEmbeddingçš„æ–¹æ¡ˆ

#### æŠ€æœ¯å®ç°
```python
class FeaturePipelineBuilder:
    """åŸºäºTensorFlowåŸç”ŸEmbeddingçš„ç‰¹å¾å¤„ç†ç®¡é“æ„å»ºå™¨"""
    
    def build_feature_pipelines(self, pipelines_config) -> List[Tuple[str, List]]:
        """ä»feat.ymlé…ç½®æ„å»ºç‰¹å¾å¤„ç†ç®¡é“"""
        
    def _create_embedding_processors(self, pipeline) -> List[tf.keras.layers.Layer]:
        """æ ¹æ®ç‰¹å¾ç±»å‹åˆ›å»ºå¯¹åº”çš„å¤„ç†å™¨"""
```

#### æ”¯æŒçš„ç‰¹å¾ç±»å‹
| ç‰¹å¾ç±»å‹ | è¾“å…¥æ ¼å¼ | å¤„ç†æµç¨‹ | è¾“å‡ºæ ¼å¼ |
|----------|----------|----------|----------|
| **sparse** | å•ä¸ªå“ˆå¸Œæ•´æ•° | Embeddingå±‚ | (batch_size, embed_dim) |
| **varlen_sparse** | å“ˆå¸Œæ•´æ•°åˆ—è¡¨ | Embedding + Pooling | (batch_size, embed_dim) |
| **dense** | æ•°å€¼ | Lambdaå±‚ï¼ˆç›´é€šï¼‰ | (batch_size, 1) |

### 3. æ•°æ®é¢„å¤„ç†é€‚é…å™¨

#### é—®é¢˜è§£å†³ - log_typeæ•°æ®æµåˆ†æ
**å…³é”®å‘ç°**: log_typeåœ¨æ•°æ®æµç¨‹ä¸­çš„åŒé‡èº«ä»½é—®é¢˜
- `dataset_utils.py`: `labels = df.pop('log_type')` - ä½œä¸ºæ ‡ç­¾ç§»é™¤
- `feat.yml`: é”™è¯¯åœ°å®šä¹‰ä¸ºç‰¹å¾

**è§£å†³æ–¹æ¡ˆ**: 
1. ä»ç‰¹å¾é…ç½®ä¸­åˆ é™¤log_typeå®šä¹‰
2. ç¡®ä¿log_typeåªä½œä¸ºæ ‡ç­¾ä½¿ç”¨
3. æœ€ç»ˆå®ç°ï¼š12ä¸ªç‰¹å¾ + log_typeæ ‡ç­¾çš„æ­£ç¡®æ¶æ„

#### ç‰¹å¾é¢„å¤„ç†é€‚é…å™¨ (`src/data/feature_preprocessor.py`)
```python
def apply_feature_preprocessing(dataset: tf.data.Dataset, 
                               feat_config_path: str = "config/feat.yml") -> tf.data.Dataset:
    """å¯¹TensorFlowæ•°æ®é›†åº”ç”¨UniProcessç‰¹å¾é¢„å¤„ç†"""
    # 1. åŠ è½½ç‰¹å¾é…ç½®
    # 2. å°†æ•°æ®é›†è½¬æ¢ä¸ºpandasè¿›è¡Œå¤„ç†
    # 3. åº”ç”¨æ“ä½œé“¾
    # 4. è½¬æ¢å›TensorFlowæ ¼å¼
```

### 4. è®­ç»ƒæµç¨‹å·¥ç¨‹åŒ–

#### æ¨¡å—åŒ–è®¾è®¡
```python
# è®­ç»ƒè„šæœ¬æ¶æ„ (src/train_MLP.py)
â”œâ”€â”€ ç¯å¢ƒè®¾ç½®æ¨¡å—          # GPUé…ç½®ã€éšæœºç§å­ã€ç›®å½•åˆ›å»º
â”œâ”€â”€ é…ç½®ç®¡ç†æ¨¡å—          # å¤šæ ¼å¼é…ç½®æ–‡ä»¶åŠ è½½
â”œâ”€â”€ æ•°æ®å¤„ç†æµæ°´çº¿        # æ•°æ®å‡†å¤‡ã€éªŒè¯ã€é¢„å¤„ç†
â”œâ”€â”€ æ¨¡å‹å‡½æ•°è¿½è¸ªæœºåˆ¶      # è§£å†³TensorFlowä¿å­˜è­¦å‘Š
â”œâ”€â”€ è®­ç»ƒä¸è¯„ä¼°æ¨¡å—        # æ¨¡å‹è®­ç»ƒã€æ€§èƒ½è¯„ä¼°
â””â”€â”€ ç‰¹å¾é‡è¦æ€§åˆ†æ        # å¯è§£é‡Šæ€§åˆ†æ
```

#### å…³é”®æŠ€æœ¯ç‰¹æ€§
- **æ¨¡å‹å‡½æ•°è¿½è¸ª**: è§£å†³TensorFlow"æœªè¿½è¸ªå‡½æ•°"è­¦å‘Š
- **è¿‡æ‹Ÿåˆæ£€æµ‹**: è‡ªåŠ¨ç›‘æ§è®­ç»ƒ/éªŒè¯AUCå·®å¼‚
- **ç‰¹å¾é‡è¦æ€§åˆ†æ**: å¯è§£é‡Šæ€§AIæ”¯æŒ
- **å®¹é”™æœºåˆ¶**: å®Œæ•´çš„å¼‚å¸¸å¤„ç†å’Œé™çº§ç­–ç•¥

## ğŸ“Š æ•°æ®æµç¨‹è¯¦è§£

### å®Œæ•´æ•°æ®æµç¨‹å›¾
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CSVæ–‡ä»¶   â”‚â”€â”€â”€â–¶â”‚  build_dataset   â”‚â”€â”€â”€â–¶â”‚ ç‰¹å¾é¢„å¤„ç†æµç¨‹  â”‚â”€â”€â”€â–¶â”‚  MLPè®­ç»ƒ    â”‚
â”‚åŒ…å«log_type â”‚    â”‚  (dataset_utils) â”‚    â”‚(feature_preproc)â”‚    â”‚   (12ç‰¹å¾)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  _process_labels â”‚
                   â”‚ labels=df.pop()  â”‚
                   â”‚ log_type â†’ æ ‡ç­¾  â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ç‰¹å¾å¤„ç†ç¤ºä¾‹

#### ç¨€ç–ç‰¹å¾å¤„ç†
```python
# åŸå§‹æ•°æ®: "Germany" -> å“ˆå¸ŒåŒ– -> 156
def str_hash(x: str, vocabulary_size: int) -> int:
    """è®¡ç®—å­—ç¬¦ä¸²å“ˆå¸Œå€¼å¹¶æ˜ å°„åˆ°æŒ‡å®šèŒƒå›´"""
    hash_value = int(md5(x.encode('utf-8')).hexdigest(), 16)
    return hash_value % vocabulary_size
```

#### å˜é•¿ç¨€ç–ç‰¹å¾å¤„ç†
```python
# åŸå§‹æ•°æ®: "CLRO_186 & ETRN_169 & GOOGL_185"
# å¤„ç†ç»“æœ: [456, 789, 234, 0, 0] (paddingåˆ°é•¿åº¦5)

operations:
  - func_name: split (åˆ†å‰²å­—ç¬¦ä¸²)
  - func_name: padding (å¡«å……åˆ°å›ºå®šé•¿åº¦)
  - func_name: list_hash (å“ˆå¸ŒåŒ–åˆ—è¡¨)
```

#### JSONç‰¹å¾æå–
```python
# åŸå§‹æ•°æ®: '[{"market":"185","code":"META","name":"Meta"}]'
# å¤„ç†ç»“æœ: ["META", "null", "null", "null", "null"]

def json_object_to_list(x: str, key: str) -> List[str]:
    """ä»JSONæ•°ç»„ä¸­æå–æŒ‡å®šå­—æ®µ"""
    json_obj = json.loads(x)
    return [item.get(key, "null") for item in json_obj]
```

## ğŸ¯ æ€§èƒ½ä¸éªŒè¯

### è®­ç»ƒç»“æœ
- âœ… **è®­ç»ƒAUC**: 0.8467
- âœ… **éªŒè¯AUC**: 0.8558
- âœ… **æ”¶æ•›ç¨³å®š**: 2è½®è®­ç»ƒè¾¾åˆ°ç¨³å®šæ€§èƒ½

### ç‰¹å¾é‡è¦æ€§åˆ†æ
1. **user_propernoun_hash**: 0.1877 (æœ€é‡è¦)
2. **country_hash**: 0.0446
3. **user_watch_stk_code_hash**: 0.0127
4. å…¶ä»–ç‰¹å¾è´¡çŒ®ç›¸å¯¹è¾ƒå°

### æ¨¡å‹å‚æ•°ç»Ÿè®¡
| æŒ‡æ ‡ | æ•°å€¼ | è¯´æ˜ |
|------|------|------|
| **æ”¯æŒç‰¹å¾æ•°** | 12ä¸ª | å¤„ç†åçš„ç‰¹å¾æ•°é‡ |
| **embeddingå‚æ•°** | ~450,000 | ä¸»è¦æ¥è‡ªå¤§è¯æ±‡è¡¨ç‰¹å¾ |
| **MLPå‚æ•°** | ~40,000 | ç¥ç»ç½‘ç»œå±‚å‚æ•° |
| **æ€»å‚æ•°** | 491,513 | å¯è®­ç»ƒå‚æ•°æ€»æ•° |

## ğŸ’¡ å…³é”®æŠ€æœ¯åˆ›æ–°

### 1. åŒå¼•æ“æ¶æ„
- **æ·±åº¦æ¨¡å‹**: å¤„ç†å¤æ‚ç‰¹å¾äº¤äº’ï¼Œæ•è·éçº¿æ€§å…³ç³»
- **æ ‘æ¨¡å‹**: å¤„ç†ç»“æ„åŒ–ç‰¹å¾ï¼Œæä¾›å¯è§£é‡Šæ€§
- **ç»Ÿä¸€é¢„å¤„ç†**: ä¸¤ä¸ªæ¨¡å‹å…±äº«åŒä¸€å¥—ç‰¹å¾å·¥ç¨‹pipeline

### 2. é…ç½®é©±åŠ¨è®¾è®¡
- **æ“ä½œé“¾ç»„åˆ**: æ¯ä¸ªç‰¹å¾å¯å®šä¹‰å¤šä¸ªè¿ç»­æ“ä½œ
- **å‚æ•°åŒ–è°ƒç”¨**: ä½¿ç”¨functools.partialå®ç°å‚æ•°ç»‘å®š
- **åŠ¨æ€å‡½æ•°è°ƒç”¨**: æ ¹æ®é…ç½®åŠ¨æ€é€‰æ‹©æ“ä½œå‡½æ•°

### 3. å·¥ç¨‹åŒ–æœ€ä½³å®è·µ
- **ç±»å‹æ³¨è§£**: å®Œæ•´çš„ç±»å‹æç¤ºå’Œæ–‡æ¡£
- **æ¨¡å—åŒ–è®¾è®¡**: é«˜åº¦è§£è€¦çš„ç»„ä»¶æ¶æ„
- **é”™è¯¯å¤„ç†**: å®Œæ•´çš„å¼‚å¸¸å¤„ç†å’Œå®¹é”™æœºåˆ¶
- **å¯é‡ç°æ€§**: éšæœºç§å­æ§åˆ¶å’Œç¯å¢ƒæ ‡å‡†åŒ–

## ğŸ”§ ä½¿ç”¨æŒ‡å—

### å¿«é€Ÿå¼€å§‹
```bash
# 1. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 2. é…ç½®æ£€æŸ¥
python -c "from src.utils.config_loader import load_data_config; print('é…ç½®æ­£å¸¸')"

# 3. è¿è¡Œè®­ç»ƒ
python src/train_MLP.py
```

### é…ç½®å®šåˆ¶
```python
# ç‰¹å¾é€‰æ‹©æ€§åŠ è½½
from src.utils.config_loader import load_feature_config

# æ’é™¤æŸäº›ç‰¹å¾è¿›è¡Œå®éªŒ
exclude_list = ["user_watch_stk_code_hash", "country_hash"]
filtered_features = load_feature_config(
    "./config/feat.yml", 
    exclude_features=exclude_list
)
```

### æ•°æ®é¢„å¤„ç†éªŒè¯
```python
# éªŒè¯ç‰¹å¾é¢„å¤„ç†
from src.data.feature_preprocessor import preprocess_features

test_data = {
    'country': ['United States', 'Germany', 'China'],
    'watchlists': ['AAPL & TSLA', 'GOOGL', 'META & AMZN & NFLX']
}

processed_data = preprocess_features(test_data)
print("å¤„ç†ç»“æœ:", processed_data)
```

## ğŸ“ å¼€å‘è§„èŒƒ

### ä»£ç ç»„ç»‡
```
src/
â”œâ”€â”€ data/                   # æ•°æ®å¤„ç†æ¨¡å—
â”‚   â”œâ”€â”€ dataset_utils.py   # æ•°æ®é›†å·¥å…·
â”‚   â”œâ”€â”€ feature_preprocessor.py  # ç‰¹å¾é¢„å¤„ç†
â”‚   â””â”€â”€ data_preparation.py     # æ•°æ®å‡†å¤‡
â”œâ”€â”€ models/                 # æ¨¡å‹æ¨¡å—
â”‚   â”œâ”€â”€ deep/              # æ·±åº¦å­¦ä¹ æ¨¡å‹
â”‚   â””â”€â”€ model_utils.py     # æ¨¡å‹å·¥å…·
â”œâ”€â”€ preprocess/            # é¢„å¤„ç†æ“ä½œ
â”‚   â””â”€â”€ operations.py      # æ“ä½œå‡½æ•°åº“
â””â”€â”€ utils/                 # å·¥å…·æ¨¡å—
    â”œâ”€â”€ config_loader.py   # é…ç½®åŠ è½½
    â”œâ”€â”€ training_utils.py  # è®­ç»ƒå·¥å…·
    â””â”€â”€ feature_analysis_utils.py  # ç‰¹å¾åˆ†æ
```

### é…ç½®æ–‡ä»¶ç»“æ„
```
config/
â”œâ”€â”€ data.yml              # æ•°æ®é…ç½®
â”œâ”€â”€ train.yml             # è®­ç»ƒé…ç½®
â””â”€â”€ feat.yml              # ç‰¹å¾é…ç½®
```

## ğŸš€ æœªæ¥æ‰©å±•æ–¹å‘

### 1. æ¨¡å‹ä¼˜åŒ–
- **æ·±åº¦æ¨¡å‹**: å°è¯•Transformerã€Wide&Deepç­‰æ¶æ„
- **ç‰¹å¾å·¥ç¨‹**: è‡ªåŠ¨åŒ–ç‰¹å¾é€‰æ‹©å’Œäº¤äº’ç‰¹å¾ç”Ÿæˆ
- **æ¨¡å‹èåˆ**: æ·±åº¦æ¨¡å‹ä¸æ ‘æ¨¡å‹çš„ensembleç­–ç•¥

### 2. å·¥ç¨‹ä¼˜åŒ–
- **åˆ†å¸ƒå¼è®­ç»ƒ**: æ”¯æŒå¤šGPUå’Œåˆ†å¸ƒå¼è®­ç»ƒ
- **æ¨¡å‹æœåŠ¡åŒ–**: æ¨¡å‹éƒ¨ç½²å’Œåœ¨çº¿æ¨ç†ä¼˜åŒ–
- **ç›‘æ§å‘Šè­¦**: æ¨¡å‹æ€§èƒ½ç›‘æ§å’Œæ¼‚ç§»æ£€æµ‹

### 3. ä¸šåŠ¡æ‰©å±•
- **å¤šä»»åŠ¡å­¦ä¹ **: æ”¯æŒå¤šç›®æ ‡ä¼˜åŒ–
- **å®æ—¶ç‰¹å¾**: æµå¼ç‰¹å¾å¤„ç†å’Œåœ¨çº¿å­¦ä¹ 
- **A/Bæµ‹è¯•æ¡†æ¶**: æ¨¡å‹æ•ˆæœå¯¹æ¯”å’Œç°åº¦å‘å¸ƒ

---

æœ¬æ–‡æ¡£æ¶µç›–äº†é¡¹ç›®çš„æ ¸å¿ƒæŠ€æœ¯æ¶æ„ã€å®ç°ç»†èŠ‚å’Œä½¿ç”¨æŒ‡å—ã€‚å¦‚æœ‰ç–‘é—®æˆ–éœ€è¦è¿›ä¸€æ­¥äº†è§£æŸä¸ªæ¨¡å—ï¼Œè¯·å‚è€ƒå¯¹åº”çš„æºç æˆ–è”ç³»å¼€å‘å›¢é˜Ÿã€‚ 
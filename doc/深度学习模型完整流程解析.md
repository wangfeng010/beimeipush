# æ·±åº¦å­¦ä¹ æ¨¡å‹å®Œæ•´æµç¨‹è§£æ

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†è§£æainvestæ¨èç³»ç»Ÿä¸­MLPæ·±åº¦å­¦ä¹ æ¨¡å‹çš„å®Œæ•´å®ç°æµç¨‹ï¼Œä»åŸå§‹CSVæ•°æ®åˆ°æœ€ç»ˆçš„ç‰¹å¾é‡è¦æ€§è¯„ä¼°ï¼ŒåŒ…å«æ¯ä¸ªç¯èŠ‚çš„ä»£ç å®ç°ã€æ•°æ®æµè½¬å’Œä¸Šä¸‹æ¸¸é€»è¾‘å…³ç³»ã€‚

## ğŸ—ï¸ å®Œæ•´æ¶æ„æµç¨‹å›¾

```mermaid
graph TD
    subgraph "1. æ•°æ®åŠ è½½é˜¶æ®µ"
        A[CSVåŸå§‹æ–‡ä»¶] --> B[data.ymlé…ç½®]
        B --> C[dataset_utils.py]
        C --> D[TensorFlow Dataset]
        D --> E[ç‰¹å¾/æ ‡ç­¾åˆ†ç¦»]
    end
    
    subgraph "2. ç‰¹å¾å¤„ç†é˜¶æ®µ"
        F[feat.ymlé…ç½®] --> G[feature_preprocessor.py]
        E --> G
        G --> H[operations.py æ“ä½œå‡½æ•°]
        H --> I[å“ˆå¸ŒåŒ–ç‰¹å¾æ•°æ®]
    end
    
    subgraph "3. æ¨¡å‹æ„å»ºé˜¶æ®µ"
        J[train.ymlé…ç½®] --> K[MLP.py æ¨¡å‹å®šä¹‰]
        K --> L[feature_pipeline.py]
        L --> M[TensorFlow Embeddingå±‚]
        I --> M
        M --> N[å®Œæ•´MLPæ¨¡å‹]
    end
    
    subgraph "4. è®­ç»ƒé˜¶æ®µ"
        N --> O[model_utils.py ç¼–è¯‘]
        O --> P[training_utils.py è®­ç»ƒ]
        P --> Q[è®­ç»ƒå†å²è®°å½•]
    end
    
    subgraph "5. è¯„ä¼°é˜¶æ®µ"
        Q --> R[æ€§èƒ½æŒ‡æ ‡è®¡ç®—]
        N --> S[feature_analysis_utils.py]
        S --> T[ç‰¹å¾é‡è¦æ€§åˆ†æ]
        T --> U[å¯è§†åŒ–è¾“å‡º]
    end
```

## ğŸ“Š ç¬¬ä¸€é˜¶æ®µï¼šæ•°æ®åŠ è½½ä¸é¢„å¤„ç†

### 1.1 é…ç½®æ–‡ä»¶è§£æ

#### æ•°æ®é…ç½® (`config/data.yml`)
```yaml
# æ•°æ®æºé…ç½®
file_pattern: "data/train/*.csv"
batch_size: 1024
validation_split: 0.2

# åˆ—å®šä¹‰
raw_data_columns:
  - user_id: string
  - create_time: string
  - country: string
  - watchlists: string
  - holdings: string
  - prefer_bid: string
  - user_propernoun: string
  - push_title: string
  - push_content: string
  - item_code: string
  - item_tags: string
  - submit_type: string
  - log_type: string  # æ ‡ç­¾åˆ—
```

#### ä»£ç å®ç°ä½ç½®
```python
# src/utils/config_loader.py
def load_data_config(config_path: str = "./config/data.yml") -> Dict[str, Any]:
    """åŠ è½½æ•°æ®é…ç½®æ–‡ä»¶"""
    return load_config_file(config_path)

def extract_config_info(data_config: Dict, dtype_mapping: Dict) -> Tuple:
    """æå–TensorFlowæ‰€éœ€çš„é…ç½®ä¿¡æ¯"""
    file_pattern = data_config.get('file_pattern', '')
    column_names = [list(col.keys())[0] for col in data_config['raw_data_columns']]
    column_defaults = _build_column_defaults(data_config, column_names, dtype_mapping)
    label_columns = data_config.get('label_columns', ['log_type'])
    return file_pattern, column_names, column_defaults, label_columns
```

### 1.2 CSVæ•°æ®åŠ è½½

#### æ ¸å¿ƒå®ç° (`src/data/dataset_utils.py`)
```python
def build_dataset(
    file_pattern: str,
    column_names: List[str], 
    column_defaults: List[Any],
    **kwargs
) -> Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset]:
    """
    æ„å»ºTensorFlowæ•°æ®é›†çš„æ ¸å¿ƒå‡½æ•°
    
    æ•°æ®æµï¼š
    CSVæ–‡ä»¶ â†’ tf.data.experimental.CsvDataset â†’ ç‰¹å¾å­—å…¸ â†’ æ ‡ç­¾å¤„ç†
    """
    
    # 1. è¯»å–CSVæ–‡ä»¶
    dataset = tf.data.experimental.CsvDataset(
        filenames=tf.data.Dataset.list_files(file_pattern),
        record_defaults=column_defaults,
        header=True,
        select_cols=None
    )
    
    # 2. è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
    def make_features(*row):
        features = dict(zip(column_names, row))
        return features
    
    dataset = dataset.map(make_features)
    
    # 3. åº”ç”¨æ ‡ç­¾å¤„ç†
    dataset = dataset.map(lambda features: (
        _process_features(features),
        _process_labels(features)
    ))
    
    return dataset
```

#### æ ‡ç­¾å¤„ç†æ ¸å¿ƒé€»è¾‘
```python
def _process_labels(features: Dict[str, tf.Tensor]) -> tf.Tensor:
    """
    æ ‡ç­¾å¤„ç†ï¼šæå–log_typeå¹¶è½¬æ¢ä¸ºæ•°å€¼
    
    å…³é”®ç‚¹ï¼šlog_typeä»ç‰¹å¾ä¸­ç§»é™¤ï¼Œä¸“é—¨ä½œä¸ºæ ‡ç­¾ä½¿ç”¨
    """
    # ä»ç‰¹å¾å­—å…¸ä¸­å¼¹å‡ºlog_type
    log_type = features.pop('log_type')
    
    # å­—ç¬¦ä¸²åˆ°æ•°å€¼çš„æ˜ å°„
    label_mapping = {'PR': 0, 'PC': 1}
    
    # åº”ç”¨æ˜ å°„
    def map_label(x):
        return tf.cond(
            tf.equal(x, 'PR'), 
            lambda: tf.constant(0, dtype=tf.int32),
            lambda: tf.cond(
                tf.equal(x, 'PC'),
                lambda: tf.constant(1, dtype=tf.int32),
                lambda: tf.constant(-1, dtype=tf.int32)  # æœªçŸ¥æ ‡ç­¾
            )
        )
    
    return tf.map_fn(map_label, log_type, dtype=tf.int32)
```

### 1.3 æ•°æ®åˆ†å‰²ä¸æ‰¹å¤„ç†

#### å®ç°ä»£ç  (`src/data/data_preparation.py`)
```python
def prepare_datasets(
    data_config: Dict[str, Any],
    train_config: Optional[Dict[str, Any]],
    dtype_mapping: Dict[str, tf.DType]
) -> Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset, List[str], Dict]:
    """
    æ•°æ®å‡†å¤‡çš„å®Œæ•´æµç¨‹
    
    è¾“å‡ºï¼š
    - full_dataset: å®Œæ•´æ•°æ®é›†
    - train_dataset: è®­ç»ƒé›† (80%)
    - validation_dataset: éªŒè¯é›† (20%)
    - column_names: åˆ—ååˆ—è¡¨
    - input_signature: è¾“å…¥ç­¾å
    """
    
    # 1. æ„å»ºåŸå§‹æ•°æ®é›†
    file_pattern, column_names, column_defaults, label_columns = extract_config_info(
        data_config, dtype_mapping
    )
    
    dataset = build_dataset(file_pattern, column_names, column_defaults)
    
    # 2. æ•°æ®åˆ†å‰²
    dataset_size = dataset.cardinality().numpy()
    train_size = int(0.8 * dataset_size)
    
    train_dataset = dataset.take(train_size)
    validation_dataset = dataset.skip(train_size)
    
    # 3. æ‰¹å¤„ç†å’Œæ€§èƒ½ä¼˜åŒ–
    batch_size = data_config.get('batch_size', 32)
    
    train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)
    validation_dataset = validation_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)
    full_dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)
    
    return full_dataset, train_dataset, validation_dataset, column_names, input_signature
```

## ğŸ”§ ç¬¬äºŒé˜¶æ®µï¼šç‰¹å¾å¤„ç†æµæ°´çº¿

### 2.1 ç‰¹å¾é…ç½®è§£æ

#### é…ç½®ç»“æ„ (`config/feat.yml`)
```yaml
pipelines:
  - feat_name: country_hash
    feat_type: sparse
    vocabulary_size: 200
    embedding_dim: 8
    operations:
      - col_in: country
        col_out: country
        func_name: fillna
        func_parameters:
          na_value: "null"
      - col_in: country
        col_out: country_hash
        func_name: str_hash
        func_parameters:
          vocabulary_size: 200
```

#### é…ç½®åŠ è½½å®ç°
```python
# src/utils/config_loader.py
def load_feature_config(
    config_path: str = "./config/feat.yml",
    exclude_features: Optional[List[str]] = None
) -> List[Dict[str, Any]]:
    """
    åŠ è½½ç‰¹å¾é…ç½®ï¼Œæ”¯æŒåŠ¨æ€ç‰¹å¾æ’é™¤
    """
    exclude_features = exclude_features or []
    
    config = load_config_file(config_path)
    return _filter_feature_pipelines(config, exclude_features)

def _filter_feature_pipelines(config: Dict, exclude_features: List[str]) -> List[Dict]:
    """è¿‡æ»¤ç‰¹å¾ç®¡é“ï¼Œæ”¯æŒA/Bæµ‹è¯•"""
    pipelines = config.get('pipelines', [])
    
    if not exclude_features:
        return pipelines
    
    return [
        pipeline for pipeline in pipelines 
        if pipeline.get('feat_name', '') not in exclude_features
    ]
```

### 2.2 æ“ä½œå‡½æ•°åº“

#### æ ¸å¿ƒæ“ä½œå‡½æ•° (`src/preprocess/operations.py`)
```python
# æ“ä½œå‡½æ•°æ³¨å†Œä¸­å¿ƒ
OP_HUB: Dict[str, Callable] = {
    # åŸºç¡€æ•°æ®å¤„ç†
    "fillna": fillna,                    # ç¼ºå¤±å€¼å¡«å……
    "split": split,                      # å­—ç¬¦ä¸²åˆ†å‰²
    "padding": padding,                  # åºåˆ—å¡«å……/æˆªæ–­
    
    # å“ˆå¸ŒåŒ–æ“ä½œ
    "str_hash": str_hash,               # å­—ç¬¦ä¸²å“ˆå¸Œ
    "list_hash": list_hash,             # åˆ—è¡¨å“ˆå¸Œ
    
    # åˆ—è¡¨æ“ä½œ
    "list_get": list_get,               # æå–åˆ—è¡¨å…ƒç´ 
    "list_len": list_len,               # è®¡ç®—åˆ—è¡¨é•¿åº¦
    "remove_items": remove_items,       # ç§»é™¤æŒ‡å®šé¡¹
    "seperation": seperation,           # åˆ—è¡¨å…ƒç´ åˆ†å‰²
    
    # JSONå¤„ç†
    "json_object_to_list": json_object_to_list,  # JSONè§£æ
    
    # æ—¶é—´ç‰¹å¾
    "to_hour": get_hour,                # æå–å°æ—¶
    "to_weekday": weekday,              # æå–æ˜ŸæœŸ
    
    # æ•°å€¼å¤„ç†
    "int_max": int_max,                 # æ•´æ•°æœ€å¤§å€¼é™åˆ¶
}

# ç¤ºä¾‹ï¼šå­—ç¬¦ä¸²å“ˆå¸Œå‡½æ•°
def str_hash(x: str, vocabulary_size: int) -> int:
    """
    å­—ç¬¦ä¸²å“ˆå¸ŒåŒ–
    
    ç®—æ³•ï¼šMD5å“ˆå¸Œ â†’ åå…­è¿›åˆ¶è½¬æ•´æ•° â†’ æ¨¡è¿ç®—
    """
    if pd.isna(x) or x == '':
        return 0
    
    hash_value = int(md5(str(x).encode('utf-8')).hexdigest(), 16)
    return hash_value % vocabulary_size

# ç¤ºä¾‹ï¼šå˜é•¿åˆ—è¡¨å¤„ç†
def list_hash(x: List[str], vocabulary_size: int) -> List[int]:
    """
    åˆ—è¡¨å“ˆå¸ŒåŒ–ï¼šå¯¹åˆ—è¡¨ä¸­æ¯ä¸ªå…ƒç´ è¿›è¡Œå“ˆå¸Œ
    """
    if not isinstance(x, list):
        return [0]
    
    return [str_hash(item, vocabulary_size) for item in x]
```

### 2.3 ç‰¹å¾é¢„å¤„ç†æ‰§è¡Œå™¨

#### æ ¸å¿ƒé€‚é…å™¨ (`src/data/feature_preprocessor.py`)
```python
def apply_feature_preprocessing(
    dataset: tf.data.Dataset,
    feat_config_path: str = "config/feat.yml",
    verbose: bool = True
) -> tf.data.Dataset:
    """
    ç‰¹å¾é¢„å¤„ç†çš„ä¸»å…¥å£å‡½æ•°
    
    æµç¨‹ï¼šTensorFlow Dataset â†’ pandaså¤„ç† â†’ TensorFlow Dataset
    """
    
    # 1. åŠ è½½ç‰¹å¾é…ç½®
    feat_configs = load_feature_config(feat_config_path)
    
    if verbose:
        print(f"ğŸ”§ åŠ è½½äº† {len(feat_configs)} ä¸ªç‰¹å¾é…ç½®")
    
    # 2. å®šä¹‰å¤„ç†å‡½æ•°
    def process_batch(features_dict, labels_tensor):
        """æ‰¹æ¬¡å¤„ç†å‡½æ•°"""
        # è½¬æ¢ä¸ºpandasä¾¿äºå¤„ç†
        batch_data = {}
        for name, tensor in features_dict.items():
            if tensor.dtype == tf.string:
                batch_data[name] = [t.decode('utf-8') for t in tensor.numpy()]
            else:
                batch_data[name] = tensor.numpy().tolist()
        
        # åº”ç”¨ç‰¹å¾å¤„ç†
        processed_data = preprocess_features(batch_data, feat_configs)
        
        # è½¬æ¢å›TensorFlowæ ¼å¼
        return processed_data, labels_tensor.numpy()
    
    # 3. åº”ç”¨åˆ°æ•°æ®é›†
    processed_dataset = dataset.map(
        lambda features, labels: tf.py_function(
            func=process_batch,
            inp=[features, labels],
            Tout=(tf.int32, tf.int32)
        ),
        num_parallel_calls=tf.data.AUTOTUNE
    )
    
    return processed_dataset

def preprocess_features(
    batch_data: Dict[str, List], 
    feat_configs: List[Dict]
) -> Dict[str, List]:
    """
    æ‰§è¡Œç‰¹å¾é¢„å¤„ç†æ“ä½œé“¾
    
    æ ¸å¿ƒç®—æ³•ï¼šéå†æ¯ä¸ªç‰¹å¾é…ç½® â†’ æ‰§è¡Œæ“ä½œé“¾ â†’ è¾“å‡ºå¤„ç†ç»“æœ
    """
    df = pd.DataFrame(batch_data)
    
    # æ‰§è¡Œæ¯ä¸ªç‰¹å¾çš„æ“ä½œé“¾
    for config in feat_configs:
        operations = config.get('operations', [])
        
        for operation in operations:
            func_name = operation['func_name']
            func_parameters = operation.get('func_parameters', {})
            col_in = operation['col_in']
            col_out = operation['col_out']
            
            # åŠ¨æ€å‡½æ•°è°ƒç”¨ + å‚æ•°ç»‘å®š
            operation_func = partial(OP_HUB[func_name], **func_parameters)
            
            # æ‰§è¡Œæ“ä½œ
            if isinstance(col_in, list):
                # å¤šåˆ—è¾“å…¥
                df[col_out] = df[col_in].apply(
                    lambda row: operation_func(*row), axis=1
                )
            else:
                # å•åˆ—è¾“å…¥
                df[col_out] = df[col_in].apply(operation_func)
    
    # æ„å»ºè¾“å‡ºç‰¹å¾å­—å…¸
    processed_features = {}
    for config in feat_configs:
        feat_name = config['feat_name']
        if feat_name in df.columns:
            feat_type = config.get('feat_type', 'sparse')
            values = df[feat_name].tolist()
            
            # æ ¹æ®ç‰¹å¾ç±»å‹å¤„ç†è¾“å‡ºæ ¼å¼
            if feat_type == 'varlen_sparse':
                # ç¡®ä¿å˜é•¿ç‰¹å¾æ ¼å¼ä¸€è‡´
                processed_features[feat_name] = ensure_consistent_length(values)
            else:
                processed_features[feat_name] = values
    
    return processed_features
```

## ğŸ§  ç¬¬ä¸‰é˜¶æ®µï¼šæ¨¡å‹æ„å»º

### 3.1 ç‰¹å¾ç®¡é“æ„å»º

#### Embeddingå±‚æ„å»ºå™¨ (`src/models/deep/feature_pipeline.py`)
```python
class FeaturePipelineBuilder:
    """
    ç‰¹å¾å¤„ç†ç®¡é“æ„å»ºå™¨
    è´Ÿè´£å°†å“ˆå¸ŒåŒ–ç‰¹å¾è½¬æ¢ä¸ºembeddingå‘é‡
    """
    
    def __init__(self, verbose: bool = False):
        self.embedding_layers = {}
        self.pooling_layers = {}
        self.verbose = verbose
    
    def build_feature_pipelines(
        self, 
        pipelines_config: List[Dict[str, Any]]
    ) -> List[Tuple[str, List]]:
        """
        æ„å»ºç‰¹å¾å¤„ç†ç®¡é“
        
        è¾“å…¥ï¼šfeat.ymlé…ç½®
        è¾“å‡ºï¼š(ç‰¹å¾å, [TensorFlowå±‚åˆ—è¡¨]) çš„åˆ—è¡¨
        """
        pipelines = []
        
        for pipeline_config in pipelines_config:
            feat_name = pipeline_config['feat_name']
            
            # åˆ›å»ºembeddingå¤„ç†å™¨
            processors = self._create_embedding_processors(pipeline_config)
            
            pipelines.append((feat_name, processors))
            
            if self.verbose:
                print(f"âœ… æ„å»ºç‰¹å¾ç®¡é“: {feat_name} ({len(processors)} ä¸ªå¤„ç†å™¨)")
        
        return pipelines
    
    def _create_embedding_processors(
        self, 
        pipeline: Dict[str, Any]
    ) -> List[tf.keras.layers.Layer]:
        """
        æ ¹æ®ç‰¹å¾ç±»å‹åˆ›å»ºå¯¹åº”çš„å¤„ç†å™¨
        """
        feat_type = pipeline.get('feat_type', 'sparse')
        vocabulary_size = pipeline.get('vocabulary_size', 1000)
        embedding_dim = pipeline.get('embedding_dim', 8)
        feat_name = pipeline['feat_name']
        
        processors = []
        
        if feat_type == 'sparse':
            # ç¨€ç–ç‰¹å¾ï¼šç›´æ¥embedding
            embedding_layer = tf.keras.layers.Embedding(
                input_dim=vocabulary_size,
                output_dim=embedding_dim,
                mask_zero=False,
                name=f"{feat_name}_embedding"
            )
            processors.append(embedding_layer)
            
        elif feat_type == 'varlen_sparse':
            # å˜é•¿ç¨€ç–ç‰¹å¾ï¼šembedding + pooling
            embedding_layer = tf.keras.layers.Embedding(
                input_dim=vocabulary_size,
                output_dim=embedding_dim,
                mask_zero=True,  # æ”¯æŒpaddingæ©ç 
                name=f"{feat_name}_embedding"
            )
            
            pooling_layer = tf.keras.layers.GlobalAveragePooling1D(
                name=f"{feat_name}_pooling"
            )
            
            processors.extend([embedding_layer, pooling_layer])
            
        elif feat_type == 'dense':
            # æ•°å€¼ç‰¹å¾ï¼šç›´æ¥é€šè¿‡
            identity_layer = tf.keras.layers.Lambda(
                lambda x: tf.cast(x, tf.float32),
                name=f"{feat_name}_identity"
            )
            processors.append(identity_layer)
        
        return processors
```

### 3.2 MLPæ¨¡å‹å®šä¹‰

#### æ ¸å¿ƒæ¨¡å‹æ¶æ„ (`src/models/deep/mlp.py`)
```python
class MLP(tf.keras.Model):
    """
    å¤šå±‚æ„ŸçŸ¥æœºæ¨¡å‹
    
    æ¶æ„ï¼š
    è¾“å…¥ â†’ ç‰¹å¾Embedding â†’ æ‹¼æ¥ â†’ BatchNorm â†’ Denseå±‚ â†’ Dropout â†’ è¾“å‡º
    """
    
    def __init__(self, 
                 feature_pipelines: List[Tuple[str, List]],
                 hidden_units: List[int] = [512, 256, 128],
                 dropout_rate: float = 0.3,
                 activation: str = 'relu',
                 **kwargs):
        super().__init__(**kwargs)
        
        self.feature_pipelines = feature_pipelines
        
        # æ„å»ºç½‘ç»œå±‚
        self.dense_layers = []
        self.batch_norm_layers = []
        self.dropout_layers = []
        
        for i, units in enumerate(hidden_units):
            # æ‰¹æ ‡å‡†åŒ–
            self.batch_norm_layers.append(
                tf.keras.layers.BatchNormalization(name=f'batch_norm_{i}')
            )
            
            # å…¨è¿æ¥å±‚
            self.dense_layers.append(
                tf.keras.layers.Dense(
                    units, 
                    activation=activation,
                    name=f'dense_{i}'
                )
            )
            
            # Dropoutæ­£åˆ™åŒ–
            self.dropout_layers.append(
                tf.keras.layers.Dropout(dropout_rate, name=f'dropout_{i}')
            )
        
        # è¾“å‡ºå±‚
        self.output_layer = tf.keras.layers.Dense(
            1, 
            activation='sigmoid',  # äºŒåˆ†ç±»
            name='output'
        )
    
    def call(self, inputs, training=None):
        """
        å‰å‘ä¼ æ’­
        
        æ•°æ®æµï¼š
        è¾“å…¥ç‰¹å¾å­—å…¸ â†’ å„ç‰¹å¾embedding â†’ æ‹¼æ¥ â†’ MLPç½‘ç»œ â†’ è¾“å‡ºæ¦‚ç‡
        """
        
        # 1. ç‰¹å¾å¤„ç†å’Œembedding
        feature_embeddings = []
        
        for feat_name, processors in self.feature_pipelines:
            if feat_name in inputs:
                x = inputs[feat_name]
                
                # ä¾æ¬¡åº”ç”¨å¤„ç†å™¨
                for processor in processors:
                    x = processor(x)
                
                feature_embeddings.append(x)
        
        # 2. ç‰¹å¾æ‹¼æ¥
        if len(feature_embeddings) > 1:
            combined_features = tf.keras.layers.Concatenate()(feature_embeddings)
        else:
            combined_features = feature_embeddings[0]
        
        # 3. MLPç½‘ç»œå‰å‘ä¼ æ’­
        x = combined_features
        
        for i in range(len(self.dense_layers)):
            x = self.batch_norm_layers[i](x, training=training)
            x = self.dense_layers[i](x)
            x = self.dropout_layers[i](x, training=training)
        
        # 4. è¾“å‡ºå±‚
        output = self.output_layer(x)
        
        return output
```

### 3.3 æ¨¡å‹ç¼–è¯‘

#### æ¨¡å‹åˆ›å»ºå’Œç¼–è¯‘ (`src/models/model_utils.py`)
```python
def create_and_compile_model(
    model_class,
    train_config: Optional[Dict[str, Any]] = None
) -> tf.keras.Model:
    """
    åˆ›å»ºå¹¶ç¼–è¯‘æ¨¡å‹
    """
    
    # 1. åŠ è½½ç‰¹å¾é…ç½®
    feat_configs = load_feature_config("config/feat.yml")
    
    # 2. æ„å»ºç‰¹å¾ç®¡é“
    pipeline_builder = FeaturePipelineBuilder(verbose=True)
    feature_pipelines = pipeline_builder.build_feature_pipelines(feat_configs)
    
    # 3. åˆ›å»ºæ¨¡å‹å®ä¾‹
    model_params = train_config.get('model_params', {}) if train_config else {}
    
    model = model_class(
        feature_pipelines=feature_pipelines,
        hidden_units=model_params.get('hidden_units', [512, 256, 128]),
        dropout_rate=model_params.get('dropout_rate', 0.3),
        activation=model_params.get('activation', 'relu')
    )
    
    # 4. ç¼–è¯‘æ¨¡å‹
    optimizer = tf.keras.optimizers.Adam(
        learning_rate=train_config.get('learning_rate', 0.001) if train_config else 0.001
    )
    
    model.compile(
        optimizer=optimizer,
        loss='binary_crossentropy',  # äºŒåˆ†ç±»æŸå¤±
        metrics=[
            'accuracy',
            tf.keras.metrics.AUC(name='auc'),
            tf.keras.metrics.Precision(name='precision'),
            tf.keras.metrics.Recall(name='recall')
        ]
    )
    
    return model
```

## ğŸ¯ ç¬¬å››é˜¶æ®µï¼šæ¨¡å‹è®­ç»ƒ

### 4.1 è®­ç»ƒé…ç½®

#### è®­ç»ƒå‚æ•°é…ç½® (`config/train.yml`)
```yaml
# ä¼˜åŒ–å™¨é…ç½®
learning_rate: 0.001
optimizer: "Adam"

# è®­ç»ƒå‚æ•°
epochs: 10
batch_size: 1024
validation_split: 0.2

# æ¨¡å‹å‚æ•°
model_params:
  hidden_units: [512, 256, 128]
  dropout_rate: 0.3
  activation: "relu"

# å›è°ƒå‡½æ•°
callbacks:
  early_stopping:
    monitor: "val_auc"
    patience: 3
    restore_best_weights: true
  
  model_checkpoint:
    filepath: "./models/best_model.h5"
    monitor: "val_auc"
    save_best_only: true
```

### 4.2 è®­ç»ƒæ‰§è¡Œ

#### è®­ç»ƒå·¥å…·å‡½æ•° (`src/utils/training_utils.py`)
```python
def train_model(
    model: tf.keras.Model,
    full_dataset: tf.data.Dataset,
    train_dataset: tf.data.Dataset,
    validation_dataset: tf.data.Dataset,
    train_config: Optional[Dict[str, Any]] = None
) -> tf.keras.callbacks.History:
    """
    æ¨¡å‹è®­ç»ƒä¸»å‡½æ•°
    
    åŒ…å«ï¼šå›è°ƒå‡½æ•°è®¾ç½®ã€è®­ç»ƒæ‰§è¡Œã€ç»“æœè®°å½•
    """
    
    # 1. è®­ç»ƒå‚æ•°
    epochs = train_config.get('epochs', 10) if train_config else 10
    
    # 2. å›è°ƒå‡½æ•°è®¾ç½®
    callbacks = []
    
    # æ—©åœå›è°ƒ
    if train_config and 'early_stopping' in train_config.get('callbacks', {}):
        early_stopping_config = train_config['callbacks']['early_stopping']
        callbacks.append(
            tf.keras.callbacks.EarlyStopping(
                monitor=early_stopping_config.get('monitor', 'val_auc'),
                patience=early_stopping_config.get('patience', 3),
                restore_best_weights=early_stopping_config.get('restore_best_weights', True),
                verbose=1
            )
        )
    
    # æ¨¡å‹æ£€æŸ¥ç‚¹
    if train_config and 'model_checkpoint' in train_config.get('callbacks', {}):
        checkpoint_config = train_config['callbacks']['model_checkpoint']
        callbacks.append(
            tf.keras.callbacks.ModelCheckpoint(
                filepath=checkpoint_config.get('filepath', './models/best_model.h5'),
                monitor=checkpoint_config.get('monitor', 'val_auc'),
                save_best_only=checkpoint_config.get('save_best_only', True),
                verbose=1
            )
        )
    
    # å­¦ä¹ ç‡è°ƒåº¦
    callbacks.append(
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor='val_auc',
            factor=0.5,
            patience=2,
            min_lr=1e-6,
            verbose=1
        )
    )
    
    # 3. å¼€å§‹è®­ç»ƒ
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹ (epochs={epochs})")
    
    history = model.fit(
        train_dataset,
        validation_data=validation_dataset,
        epochs=epochs,
        callbacks=callbacks,
        verbose=1
    )
    
    # 4. è®­ç»ƒç»“æœæ€»ç»“
    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼")
    print(f"æœ€ä½³è®­ç»ƒAUC: {max(history.history['auc']):.4f}")
    print(f"æœ€ä½³éªŒè¯AUC: {max(history.history['val_auc']):.4f}")
    
    return history
```

### 4.3 è®­ç»ƒæµç¨‹æ§åˆ¶

#### ä¸»è®­ç»ƒè„šæœ¬ (`src/train_MLP.py`)
```python
def main() -> None:
    """
    ä¸»è®­ç»ƒæµç¨‹
    
    å®Œæ•´æµç¨‹ï¼šç¯å¢ƒè®¾ç½® â†’ æ•°æ®å‡†å¤‡ â†’ æ¨¡å‹æ„å»º â†’ è®­ç»ƒæ‰§è¡Œ â†’ è¯„ä¼°åˆ†æ
    """
    
    # 1. ç¯å¢ƒè®¾ç½®
    setup_environment_for_training()
    
    # 2. æ•°æ®å‡†å¤‡
    print("ğŸ“‚ å¼€å§‹æ•°æ®å‡†å¤‡...")
    model, full_dataset, train_dataset, validation_dataset = prepare_model_and_data()
    
    # 3. é…ç½®åŠ è½½
    _, train_config = load_configurations()
    
    # 4. æ¨¡å‹è®­ç»ƒ
    print("ğŸ¯ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
    train_and_evaluate_model(
        model, full_dataset, train_dataset, validation_dataset,
        train_config=train_config
    )
    
    print("\nğŸ‰ è®­ç»ƒæµç¨‹å®Œæˆï¼")

def prepare_model_and_data():
    """å‡†å¤‡æ¨¡å‹å’Œæ•°æ®çš„å®Œæ•´æµç¨‹"""
    
    # 1. åŠ è½½é…ç½®
    data_config, train_config = load_configurations()
    
    # 2. åŸå§‹æ•°æ®é›†å‡†å¤‡
    datasets = prepare_dataset_from_config(data_config, train_config)
    full_dataset, train_dataset, validation_dataset = datasets[:3]
    
    # 3. ç‰¹å¾é¢„å¤„ç†
    print("ğŸ”§ åº”ç”¨ç‰¹å¾é¢„å¤„ç†...")
    
    processed_full_dataset = apply_feature_preprocessing(
        full_dataset, feat_config_path="config/feat.yml", verbose=True
    )
    processed_train_dataset = apply_feature_preprocessing(
        train_dataset, feat_config_path="config/feat.yml", verbose=False
    )
    processed_validation_dataset = apply_feature_preprocessing(
        validation_dataset, feat_config_path="config/feat.yml", verbose=False
    )
    
    # 4. æ•°æ®é›†æ£€æŸ¥
    inspect_datasets(processed_full_dataset, processed_train_dataset, processed_validation_dataset)
    
    # 5. æ¨¡å‹åˆ›å»º
    model = create_and_compile_model(MLP, train_config)
    
    # 6. æ¨¡å‹å‡½æ•°è¿½è¸ªï¼ˆä¼˜åŒ–ä¿å­˜æ€§èƒ½ï¼‰
    trace_model(model, processed_full_dataset)
    
    return model, processed_full_dataset, processed_train_dataset, processed_validation_dataset
```

## ğŸ“Š ç¬¬äº”é˜¶æ®µï¼šæ¨¡å‹è¯„ä¼°ä¸åˆ†æ

### 5.1 æ€§èƒ½æŒ‡æ ‡è®¡ç®—

#### è¯„ä¼°å‡½æ•° (`src/utils/training_utils.py`)
```python
def evaluate_model_performance(
    model: tf.keras.Model,
    validation_dataset: tf.data.Dataset,
    history: tf.keras.callbacks.History
) -> Dict[str, float]:
    """
    æ¨¡å‹æ€§èƒ½è¯„ä¼°
    
    è®¡ç®—ï¼šAUCã€å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°
    """
    
    # 1. åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
    val_results = model.evaluate(validation_dataset, verbose=0)
    val_metrics = dict(zip(model.metrics_names, val_results))
    
    # 2. è®­ç»ƒå†å²åˆ†æ
    train_auc = history.history['auc'][-1]
    val_auc = history.history['val_auc'][-1]
    
    # 3. è¿‡æ‹Ÿåˆæ£€æµ‹
    overfitting_score = train_auc - val_auc
    
    performance_metrics = {
        'train_auc': train_auc,
        'val_auc': val_auc,
        'val_accuracy': val_metrics['accuracy'],
        'val_precision': val_metrics['precision'],
        'val_recall': val_metrics['recall'],
        'overfitting_score': overfitting_score
    }
    
    # 4. æ€§èƒ½è¯Šæ–­
    if overfitting_score > 0.05:
        print(f"âš ï¸  æ£€æµ‹åˆ°è¿‡æ‹Ÿåˆç°è±¡ (å·®å¼‚: {overfitting_score:.4f})")
    else:
        print(f"âœ… æ¨¡å‹æ³›åŒ–æ€§èƒ½è‰¯å¥½ (å·®å¼‚: {overfitting_score:.4f})")
    
    return performance_metrics
```

### 5.2 ç‰¹å¾é‡è¦æ€§åˆ†æ

#### é‡è¦æ€§è®¡ç®— (`src/utils/feature_analysis_utils.py`)
```python
def check_feature_importance(
    model: tf.keras.Model,
    validation_dataset: tf.data.Dataset,
    train_config: Optional[Dict[str, Any]] = None
) -> Dict[str, float]:
    """
    è®¡ç®—ç‰¹å¾é‡è¦æ€§
    
    æ–¹æ³•ï¼šPermutation Importance
    åŸç†ï¼šéšæœºæ‰“ä¹±æŸä¸ªç‰¹å¾ï¼Œè§‚å¯Ÿæ¨¡å‹æ€§èƒ½ä¸‹é™ç¨‹åº¦
    """
    
    print("ğŸ” å¼€å§‹ç‰¹å¾é‡è¦æ€§åˆ†æ...")
    
    # 1. åŸºçº¿æ€§èƒ½
    baseline_results = model.evaluate(validation_dataset, verbose=0)
    baseline_auc = baseline_results[model.metrics_names.index('auc')]
    
    print(f"åŸºçº¿AUC: {baseline_auc:.4f}")
    
    # 2. è·å–ç‰¹å¾åˆ—è¡¨
    feature_names = [name for name, _ in model.feature_pipelines]
    feature_importance = {}
    
    # 3. é€ä¸ªç‰¹å¾è¿›è¡Œç½®æ¢æµ‹è¯•
    for feat_name in feature_names:
        print(f"   æµ‹è¯•ç‰¹å¾: {feat_name}")
        
        # åˆ›å»ºç½®æ¢æ•°æ®é›†
        permuted_dataset = validation_dataset.map(
            lambda features, labels: (
                _permute_feature(features, feat_name),
                labels
            )
        )
        
        # è¯„ä¼°ç½®æ¢åçš„æ€§èƒ½
        permuted_results = model.evaluate(permuted_dataset, verbose=0)
        permuted_auc = permuted_results[model.metrics_names.index('auc')]
        
        # è®¡ç®—é‡è¦æ€§å¾—åˆ†ï¼ˆæ€§èƒ½ä¸‹é™ç¨‹åº¦ï¼‰
        importance_score = baseline_auc - permuted_auc
        feature_importance[feat_name] = max(0, importance_score)  # ç¡®ä¿éè´Ÿ
        
        print(f"      é‡è¦æ€§å¾—åˆ†: {importance_score:.4f}")
    
    # 4. å½’ä¸€åŒ–é‡è¦æ€§å¾—åˆ†
    total_importance = sum(feature_importance.values())
    if total_importance > 0:
        feature_importance = {
            name: score / total_importance 
            for name, score in feature_importance.items()
        }
    
    # 5. æŒ‰é‡è¦æ€§æ’åº
    sorted_importance = dict(
        sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
    )
    
    return sorted_importance

def _permute_feature(features: Dict[str, tf.Tensor], feat_name: str) -> Dict[str, tf.Tensor]:
    """
    ç½®æ¢æŒ‡å®šç‰¹å¾çš„å€¼
    """
    permuted_features = features.copy()
    
    if feat_name in permuted_features:
        # éšæœºæ‰“ä¹±ç‰¹å¾å€¼
        original_tensor = permuted_features[feat_name]
        shuffled_indices = tf.random.shuffle(tf.range(tf.shape(original_tensor)[0]))
        permuted_features[feat_name] = tf.gather(original_tensor, shuffled_indices)
    
    return permuted_features
```

### 5.3 ç»“æœå¯è§†åŒ–

#### å¯è§†åŒ–å·¥å…· (`src/utils/feature_analysis_utils.py`)
```python
def plot_feature_importance(
    feature_importance: Dict[str, float],
    top_k: int = 10,
    save_path: str = "./results/feature_importance.png"
) -> None:
    """
    ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾è¡¨
    """
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    # 1. æ•°æ®å‡†å¤‡
    sorted_features = dict(
        sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:top_k]
    )
    
    features = list(sorted_features.keys())
    importance_scores = list(sorted_features.values())
    
    # 2. ç»˜å›¾è®¾ç½®
    plt.figure(figsize=(12, 8))
    sns.set_style("whitegrid")
    
    # 3. åˆ›å»ºæ¡å½¢å›¾
    bars = plt.barh(range(len(features)), importance_scores, 
                   color=sns.color_palette("viridis", len(features)))
    
    # 4. å›¾è¡¨ç¾åŒ–
    plt.yticks(range(len(features)), features)
    plt.xlabel('ç‰¹å¾é‡è¦æ€§å¾—åˆ†', fontsize=12)
    plt.title(f'Top {top_k} ç‰¹å¾é‡è¦æ€§åˆ†æ', fontsize=14, fontweight='bold')
    
    # 5. æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (bar, score) in enumerate(zip(bars, importance_scores)):
        plt.text(bar.get_width() + 0.001, bar.get_y() + bar.get_height()/2, 
                f'{score:.4f}', ha='left', va='center', fontsize=10)
    
    # 6. ä¿å­˜å›¾ç‰‡
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"ğŸ“Š ç‰¹å¾é‡è¦æ€§å›¾è¡¨å·²ä¿å­˜: {save_path}")

def plot_training_history(history: tf.keras.callbacks.History) -> None:
    """
    ç»˜åˆ¶è®­ç»ƒå†å²æ›²çº¿
    """
    import matplotlib.pyplot as plt
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # AUCæ›²çº¿
    axes[0, 0].plot(history.history['auc'], label='Train AUC')
    axes[0, 0].plot(history.history['val_auc'], label='Validation AUC')
    axes[0, 0].set_title('Model AUC')
    axes[0, 0].legend()
    
    # æŸå¤±æ›²çº¿
    axes[0, 1].plot(history.history['loss'], label='Train Loss')
    axes[0, 1].plot(history.history['val_loss'], label='Validation Loss')
    axes[0, 1].set_title('Model Loss')
    axes[0, 1].legend()
    
    # å‡†ç¡®ç‡æ›²çº¿
    axes[1, 0].plot(history.history['accuracy'], label='Train Accuracy')
    axes[1, 0].plot(history.history['val_accuracy'], label='Validation Accuracy')
    axes[1, 0].set_title('Model Accuracy')
    axes[1, 0].legend()
    
    # ç²¾ç¡®ç‡å’Œå¬å›ç‡
    axes[1, 1].plot(history.history['precision'], label='Train Precision')
    axes[1, 1].plot(history.history['recall'], label='Train Recall')
    axes[1, 1].plot(history.history['val_precision'], label='Val Precision')
    axes[1, 1].plot(history.history['val_recall'], label='Val Recall')
    axes[1, 1].set_title('Precision & Recall')
    axes[1, 1].legend()
    
    plt.tight_layout()
    plt.savefig('./results/training_history.png', dpi=300)
    plt.show()
```

## ğŸ”— å®Œæ•´ä»£ç è°ƒç”¨é“¾

### ä¸»è¦å‡½æ•°è°ƒç”¨å…³ç³»

```python
# 1. ä¸»å…¥å£
main()                                          # src/train_MLP.py
â””â”€â”€ setup_environment_for_training()           # ç¯å¢ƒè®¾ç½®
â””â”€â”€ prepare_model_and_data()                   # æ•°æ®å’Œæ¨¡å‹å‡†å¤‡
    â”œâ”€â”€ load_configurations()                  # src/utils/config_loader.py
    â”œâ”€â”€ prepare_dataset_from_config()          # src/data/data_preparation.py
    â”‚   â”œâ”€â”€ extract_config_info()              # src/utils/config_loader.py
    â”‚   â””â”€â”€ build_dataset()                    # src/data/dataset_utils.py
    â”‚       â”œâ”€â”€ _process_features()            # ç‰¹å¾å¤„ç†
    â”‚       â””â”€â”€ _process_labels()              # æ ‡ç­¾å¤„ç†
    â”œâ”€â”€ apply_feature_preprocessing()          # src/data/feature_preprocessor.py
    â”‚   â”œâ”€â”€ load_feature_config()              # src/utils/config_loader.py
    â”‚   â””â”€â”€ preprocess_features()              # ç‰¹å¾æ“ä½œé“¾æ‰§è¡Œ
    â”‚       â””â”€â”€ OP_HUB[func_name]()            # src/preprocess/operations.py
    â”œâ”€â”€ create_and_compile_model()             # src/models/model_utils.py
    â”‚   â”œâ”€â”€ FeaturePipelineBuilder()           # src/models/deep/feature_pipeline.py
    â”‚   â””â”€â”€ MLP()                              # src/models/deep/mlp.py
    â””â”€â”€ inspect_datasets()                     # src/data/dataset_utils.py

â””â”€â”€ train_and_evaluate_model()                # è®­ç»ƒå’Œè¯„ä¼°
    â”œâ”€â”€ test_model_on_batch()                 # src/models/model_utils.py
    â”œâ”€â”€ train_model()                         # src/utils/training_utils.py
    â”œâ”€â”€ check_feature_importance()            # src/utils/feature_analysis_utils.py
    â””â”€â”€ plot_feature_importance()             # å¯è§†åŒ–è¾“å‡º
```

### æ•°æ®æµè½¬è¯¦è§£

```
1. CSVæ–‡ä»¶ (data/train/*.csv)
   â†“ [dataset_utils.py::build_dataset()]
   
2. TensorFlow Dataset {features: {...}, labels: tensor}
   â†“ [feature_preprocessor.py::apply_feature_preprocessing()]
   
3. å¤„ç†åçš„Dataset {processed_features: {...}, labels: tensor}
   â†“ [model_utils.py::create_and_compile_model()]
   
4. ç¼–è¯‘å¥½çš„MLPæ¨¡å‹
   â†“ [training_utils.py::train_model()]
   
5. è®­ç»ƒå†å²å’Œæ¨¡å‹æƒé‡
   â†“ [feature_analysis_utils.py::check_feature_importance()]
   
6. ç‰¹å¾é‡è¦æ€§åˆ†æç»“æœå’Œå¯è§†åŒ–å›¾è¡¨
```

## ğŸ“ˆ æ€§èƒ½åŸºå‡†å’Œä¼˜åŒ–

### å½“å‰æ€§èƒ½æŒ‡æ ‡
```
æ•°æ®é‡: 50,000 æ ·æœ¬
ç‰¹å¾æ•°: 12 ä¸ªå¤„ç†åç‰¹å¾
è®­ç»ƒæ—¶é—´: ~5 åˆ†é’Ÿ
å†…å­˜å ç”¨: ~2GB
æ¨¡å‹å¤§å°: ~2MB

æ€§èƒ½æŒ‡æ ‡:
- è®­ç»ƒAUC: 0.8467
- éªŒè¯AUC: 0.8558
- å‡†ç¡®ç‡: ~78%
- ç²¾ç¡®ç‡: ~76%
- å¬å›ç‡: ~82%
```

### å…³é”®æ€§èƒ½ä¼˜åŒ–ç‚¹
1. **æ‰¹å¤„ç†ä¼˜åŒ–**: ä½¿ç”¨åˆé€‚çš„batch_sizeå’Œprefetch
2. **ç‰¹å¾ç¼“å­˜**: é¢„å¤„ç†ç»“æœç¼“å­˜é¿å…é‡å¤è®¡ç®—
3. **æ¨¡å‹æ¶æ„**: é€šè¿‡è¶…å‚æ•°è°ƒä¼˜ä¼˜åŒ–ç½‘ç»œç»“æ„
4. **æ­£åˆ™åŒ–**: Dropoutå’ŒBatchNormé˜²æ­¢è¿‡æ‹Ÿåˆ

---

## ğŸ“ æ€»ç»“

æœ¬æ–‡æ¡£è¯¦ç»†è§£æäº†ainvestæ¨èç³»ç»Ÿæ·±åº¦å­¦ä¹ æ¨¡å‹çš„å®Œæ•´å®ç°æµç¨‹ï¼Œä»CSVåŸå§‹æ•°æ®åˆ°æœ€ç»ˆçš„ç‰¹å¾é‡è¦æ€§è¯„ä¼°ï¼Œæ¶µç›–äº†ï¼š

1. **æ•°æ®åŠ è½½**: é…ç½®é©±åŠ¨çš„CSVæ•°æ®è¯»å–å’Œé¢„å¤„ç†
2. **ç‰¹å¾å·¥ç¨‹**: UniProcessæ“ä½œé“¾çš„æ‰§è¡Œå’Œå“ˆå¸ŒåŒ–å¤„ç†
3. **æ¨¡å‹æ„å»º**: åŸºäºTensorFlowçš„MLPæ¶æ„å’ŒEmbeddingå±‚
4. **è®­ç»ƒæµç¨‹**: å®Œæ•´çš„è®­ç»ƒã€éªŒè¯å’Œå›è°ƒæœºåˆ¶
5. **æ¨¡å‹è¯„ä¼°**: æ€§èƒ½æŒ‡æ ‡è®¡ç®—å’Œç‰¹å¾é‡è¦æ€§åˆ†æ

æ•´ä¸ªç³»ç»Ÿé‡‡ç”¨äº†**é…ç½®é©±åŠ¨**ã€**æ¨¡å—åŒ–è®¾è®¡**å’Œ**å·¥ç¨‹åŒ–æœ€ä½³å®è·µ**ï¼Œä¸ºAIæ¨èç³»ç»Ÿæä¾›äº†é«˜è´¨é‡çš„æŠ€æœ¯å®ç°æ–¹æ¡ˆã€‚ 
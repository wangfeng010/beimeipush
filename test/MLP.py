import tensorflow as tf
from keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout, BatchNormalization, Concatenate
from keras.models import Model
from keras.regularizers import l2
import os
import sys
import json
from functools import partial
from datetime import datetime
from hashlib import md5
from typing import Any, Dict, List, Union
import pandas as pd
import yaml
import numpy as np
from pprint import pprint
from glob import glob  # æ·»åŠ globæ¨¡å—å¯¼å…¥
import time
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

# 02 æ ¹æ®é…ç½®æ–‡ä»¶åŠ è½½åŸå§‹æ•°æ®
# å’Œæ ‘æ¨¡å‹éƒ¨åˆ†ä¸€æ ·
def load_raw_data_from_config():
    """æ ¹æ®data.ymlé…ç½®æ–‡ä»¶åŠ è½½åŸå§‹æ•°æ®ï¼Œæ”¯æŒæœ¬åœ°å’Œçº¿ä¸Šç¯å¢ƒ"""
    
    # åŠ è½½æ•°æ®é…ç½®
    with open('config/data.yml', 'r', encoding='utf-8') as f:
        data_config = yaml.safe_load(f)
    
    # æ£€æµ‹æ–‡ä»¶ç±»å‹å’Œç¯å¢ƒ
    train_dir = data_config['train_dir']
    
    # æ£€æŸ¥CSVå’ŒTXTæ–‡ä»¶
    csv_files = glob(os.path.join(train_dir, '*.csv'))
    txt_files = glob(os.path.join(train_dir, '*.txt'))
    
    if csv_files:
        # æœ¬åœ°ç¯å¢ƒ - ä½¿ç”¨CSVæ ¼å¼
        print("æ£€æµ‹åˆ°CSVæ–‡ä»¶ï¼Œä½¿ç”¨æœ¬åœ°ç¯å¢ƒé…ç½®")
        csv_config = data_config['csv_format']
        separator, header = csv_config['separator'], csv_config['header']
        
        print(f"åˆ†éš”ç¬¦: '{separator}', è¡¨å¤´è¡Œ: {header}, æ–‡ä»¶æ•°é‡: {len(csv_files)}")
        
        # è¯»å–CSVæ–‡ä»¶
        dfs = [pd.read_csv(f, sep=separator, header=header) for f in csv_files]
        
    elif txt_files:
        # çº¿ä¸Šç¯å¢ƒ - ä½¿ç”¨TXTæ ¼å¼
        print("æ£€æµ‹åˆ°TXTæ–‡ä»¶ï¼Œä½¿ç”¨çº¿ä¸Šç¯å¢ƒé…ç½®")
        txt_config = data_config.get('txt_format', {'separator': '\t', 'header': None})
        separator, header = txt_config['separator'], txt_config['header']
        
        # ä»åˆ—è¡¨ä¸­æå–åˆ—å
        raw_columns = [list(item.keys())[0] for item in data_config['raw_data_columns']]
        
        print(f"åˆ†éš”ç¬¦: '{separator}', è¡¨å¤´è¡Œ: {header}, æ–‡ä»¶æ•°é‡: {len(txt_files)}")
        print(f"é¢„å®šä¹‰åˆ—å: {raw_columns}")
        
        # è¯»å–TXTæ–‡ä»¶
        dfs = [pd.read_csv(f, sep=separator, header=header, names=raw_columns) for f in txt_files]
    else:
        raise ValueError(f"åœ¨ç›®å½• {train_dir} ä¸­æœªæ‰¾åˆ°CSVæˆ–TXTæ–‡ä»¶")
    
    df_raw = pd.concat(dfs, ignore_index=True)
    print(f"å½¢çŠ¶{df_raw.shape}, åˆ—å: {list(df_raw.columns)}")
    
    return df_raw

# åŠ è½½åŸå§‹æ•°æ®
df_raw = load_raw_data_from_config()

# æ˜¾ç¤ºæ•°æ®æ ·ä¾‹ï¼ˆç®€åŒ–ç‰ˆï¼‰
print(df_raw.iloc[0].to_dict())

# åŠ è½½æ·±åº¦æ¨¡å‹çš„è®­ç»ƒå’Œæ¨¡å‹é…ç½®ï¼ˆtrain.ymlï¼‰
with open('config/train.yml', 'r', encoding='utf-8') as f:
    train_config = yaml.safe_load(f)

training_config = train_config.get('training', {})
model_config = train_config.get('model', {})

# åŠ è½½æ·±åº¦æ¨¡å‹çš„ç‰¹å¾é…ç½® (feat.yml)
with open('config/feat.yml', 'r', encoding='utf-8') as f:
    deep_feat_config = yaml.safe_load(f)

print(training_config)
print(model_config)
print(deep_feat_config)

MISSING_VALUE = [None, '', 'null', 'NULL', 'None', np.nan]

def fillna(x: Union[float, int, str], na_value: Union[float, int, str]) -> Union[float, int, str]:
    """å¡«å……ç¼ºå¤±å€¼"""
    if x in MISSING_VALUE or (isinstance(x, float) and pd.isna(x)):
        return na_value
    return x

def split(x: str, sep: str) -> List[str]:
    """å­—ç¬¦ä¸²åˆ†å‰²"""
    return str(x).split(sep)

def seperation(x: List[str], sep: str) -> List[List[str]]:
    """åˆ—è¡¨å…ƒç´ äºŒæ¬¡åˆ†å‰²"""
    if not isinstance(x, list):
        return []
    return [item.split(sep) for item in x]

def list_get(x: List[List[Any]], item_index: int) -> List[Any]:
    """è·å–åµŒå¥—åˆ—è¡¨ä¸­æŒ‡å®šä½ç½®çš„å…ƒç´ """
    if not isinstance(x, list):
        return []
    result = []
    for sublist in x:
        if isinstance(sublist, list) and len(sublist) > item_index:
            result.append(sublist[item_index])
        else:
            result.append('null')
    return result

def remove_items(x: List[str], target_values: List[str]) -> List[str]:
    """ç§»é™¤åˆ—è¡¨ä¸­çš„æŒ‡å®šå…ƒç´ """
    if not isinstance(x, list):
        return []
    return [item for item in x if item not in target_values]

def padding(x: List[Any], pad_value: Union[str, float, int], max_len: int) -> List[Any]:
    """åˆ—è¡¨å¡«å……åˆ°æŒ‡å®šé•¿åº¦"""
    if not isinstance(x, list):
        x = []
    if len(x) >= max_len:
        return x[:max_len]
    else:
        return x + [pad_value] * (max_len - len(x))

def list_hash(x: List[str], vocabulary_size: int) -> List[int]:
    """å¯¹åˆ—è¡¨ä¸­æ¯ä¸ªå…ƒç´ è¿›è¡Œå“ˆå¸Œ"""
    if not isinstance(x, list):
        return []
    result = []
    for item in x:
        hash_val = int(md5(str(item).encode()).hexdigest(), 16) % vocabulary_size
        result.append(hash_val)
    return result

def str_hash(x: str, vocabulary_size: int) -> int:
    """å­—ç¬¦ä¸²å“ˆå¸Œ"""
    return int(md5(str(x).encode()).hexdigest(), 16) % vocabulary_size

def to_hour(x: str) -> int:
    """æå–æ—¶é—´ä¸­çš„å°æ—¶"""
    try:
        dt = pd.to_datetime(x)
        return dt.hour
    except:
        return 0

def to_weekday(x: str) -> int:
    """æå–æ—¶é—´ä¸­çš„æ˜ŸæœŸ"""
    try:
        dt = pd.to_datetime(x)
        return dt.weekday()
    except:
        return 0

def list_len(x: List) -> int:
    """åˆ—è¡¨é•¿åº¦"""
    if isinstance(x, list):
        return len(x)
    return 0

def int_max(x: int, max_value: int) -> int:
    """é™åˆ¶æ•´æ•°æœ€å¤§å€¼"""
    return min(int(x), max_value)

def json_object_to_list(x: str, key: str) -> List[str]:
    """ä»JSONå¯¹è±¡åˆ—è¡¨ä¸­æå–æŒ‡å®šé”®çš„å€¼"""
    try:
        data = json.loads(x)
        if isinstance(data, list):
            return [item.get(key, 'null') for item in data if isinstance(item, dict)]
        return ['null']
    except:
        return ['null']

def map_to_int(x: Union[str, List], map_dict: Dict[str, int], default_code: int = 0) -> Union[List[int], int]:
    """æ˜ å°„åˆ°æ•´æ•°"""
    if isinstance(x, list):
        return [map_dict.get(item, default_code) for item in x]
    else:
        return map_dict.get(str(x), default_code)

# æ„å»ºæ“ä½œä¸­å¿ƒ (OP_HUB)
OP_HUB = {
    'fillna': fillna,
    'split': split,
    'seperation': seperation,
    'list_get': list_get,
    'remove_items': remove_items,
    'padding': padding,
    'list_hash': list_hash,
    'str_hash': str_hash,
    'to_hour': to_hour,
    'to_weekday': to_weekday,
    'list_len': list_len,
    'int_max': int_max,
    'json_object_to_list': json_object_to_list,
    'map_to_int': map_to_int
}

print(f"OP_HUB æ„å»ºå®Œæˆï¼ŒåŒ…å« {len(OP_HUB)} ä¸ªæ“ä½œå‡½æ•°")
print(f"å¯ç”¨å‡½æ•°: {list(OP_HUB.keys())}")

def run_one_op(df: pd.DataFrame, operation: dict) -> pd.DataFrame:
    """æ‰§è¡Œå•ä¸ªç‰¹å¾æ“ä½œ"""
    # è·å–æ“ä½œé…ç½®
    col_in = operation['col_in']
    col_out = operation['col_out']
    func_name = operation['func_name']
    parameters = operation.get('func_parameters', {})
    
    # æ£€æŸ¥å‡½æ•°æ˜¯å¦å­˜åœ¨
    if func_name not in OP_HUB:
        return df
    
    # æ£€æŸ¥è¾“å…¥åˆ—æ˜¯å¦å­˜åœ¨
    input_cols = [col_in] if isinstance(col_in, str) else col_in
    if not all(col in df.columns for col in input_cols):
        return df
    
    # å‡†å¤‡ç‰¹å¾è½¬æ¢å‡½æ•°
    transform_func = partial(OP_HUB[func_name], **parameters)
    
    # æ‰§è¡Œç‰¹å¾è½¬æ¢
    if isinstance(col_in, list):
        df[col_out] = df[col_in].apply(lambda row: transform_func(*row), axis=1)
    else:
        df[col_out] = df[col_in].apply(transform_func)
    
    return df

def process_feature_pipelines(df_raw: pd.DataFrame, feat_config: dict) -> tuple[pd.DataFrame, list]:
    """æ‰§è¡Œç‰¹å¾å·¥ç¨‹æµæ°´çº¿ - é€‚é…ä¸åŒçš„é…ç½®æ–‡ä»¶æ ¼å¼"""
    # åˆ›å»ºæ•°æ®å‰¯æœ¬
    df = df_raw.copy()
    
    # è·å–éœ€è¦å¤„ç†çš„æµæ°´çº¿ - é€‚é…ä¸åŒæ ¼å¼
    if 'pipelines' in feat_config:
        # feat.yml æ ¼å¼: { pipelines: [...] }
        pipelines = feat_config['pipelines']
    elif 'process' in feat_config and 'pipelines' in feat_config['process']:
        # config.yml æ ¼å¼: { process: { pipelines: [...] } }
        pipelines = feat_config['process']['pipelines']
    else:
        print("æ— æ³•æ‰¾åˆ°ç‰¹å¾é…ç½®ä¸­çš„ pipelines")
        return df, []

    # è®°å½•æˆåŠŸå¤„ç†çš„ç‰¹å¾
    processed_features = []
    
    # æ‰§è¡Œæ¯ä¸ªç‰¹å¾å¤„ç†æµæ°´çº¿
    for pipeline in pipelines:
        feat_name = pipeline['feat_name']
        operations = pipeline['operations']
        
        # æ‰§è¡Œæµæ°´çº¿ä¸­çš„æ¯ä¸ªæ“ä½œ
        for operation in operations:
            df = run_one_op(df, operation)

        # è®°å½•å¤„ç†æˆåŠŸçš„ç‰¹å¾
        processed_features.append(feat_name)
    
    return df, processed_features

# labelçš„å¤„ç†
if 'label' not in df_raw.columns:
    df_raw['label'] = df_raw['log_type'].apply(lambda x: 1 if x == 'PC' else 0)

# 2.4èŠ‚ åªæœ‰è¿™é‡Œæ˜¯å’Œ1.4èŠ‚ä¸ä¸€æ ·çš„
df_deep_processed, processed_features = process_feature_pipelines(df_raw, deep_feat_config)

print("æ•°æ®ç»“æ„å¯¹æ¯”:")
print(f"åŸå§‹åˆ—æ•°: {len(df_raw.columns)}")
print(f"å¤„ç†ååˆ—æ•°: {len(df_deep_processed.columns)}")
print(f"æ–°å¢åˆ—æ•°: {len(df_deep_processed.columns) - len(df_raw.columns)}")

print("\nåŸå§‹åˆ—å:")
print(list(df_raw.columns))

print("\næ–°å¢åˆ—å:")
new_columns = [col for col in df_deep_processed.columns if col not in df_raw.columns]
print(new_columns)

# æŸ¥çœ‹æˆåŠŸç”Ÿæˆçš„ç‰¹å¾
print("æˆåŠŸç”Ÿæˆçš„ç‰¹å¾è¯¦æƒ…:")
for feat_name in processed_features:
    if feat_name in df_deep_processed.columns:
        sample_data = df_deep_processed[feat_name].iloc[0]
        data_type = type(sample_data).__name__
        print(f"  {feat_name}: {data_type} = {sample_data}")

print("æœ€ç»ˆå¤„ç†ç»“æœé¢„è§ˆ:")
display_cols = ['user_id', 'log_type'] + processed_features
display_cols = [col for col in display_cols if col in df_deep_processed.columns]

print(df_deep_processed[display_cols].head())

def prepare_tf_dataset_for_deep_model(df: pd.DataFrame, feat_config: dict, batch_size: int = 512) -> tf.data.Dataset:
    """å‡†å¤‡TensorFlowæ•°æ®é›†ç”¨äºæ·±åº¦æ¨¡å‹è®­ç»ƒ"""
    
    # è·å–éœ€è¦çš„ç‰¹å¾ - é€‚é…ä¸åŒçš„é…ç½®æ–‡ä»¶æ ¼å¼
    if 'pipelines' in feat_config:
        # feat.yml æ ¼å¼
        pipelines = feat_config['pipelines']
    elif 'process' in feat_config and 'pipelines' in feat_config['process']:
        # config.yml æ ¼å¼
        pipelines = feat_config['process']['pipelines']
    else:
        raise ValueError("æ— æ³•æ‰¾åˆ°ç‰¹å¾é…ç½®ä¸­çš„ pipelines")
    
    pipeline_feats = {p['feat_name']: p for p in pipelines}
    features_dict = {}
    
    for feat_name, config in pipeline_feats.items():
        if feat_name not in df.columns:
            continue
        
        feat_type = config.get('feat_type', 'sparse')
        
        if feat_type == 'sparse':
            # å•å€¼ç‰¹å¾ï¼Œshapeä¸º (batch_size,) - è¿™æ˜¯å…³é”®ï¼
            values = df[feat_name].values.astype(np.int32)
            features_dict[feat_name] = values
            
        elif feat_type == 'varlen_sparse':
            # å˜é•¿ç‰¹å¾ï¼Œéœ€è¦padding
            sequences = df[feat_name].tolist()
            max_len = max(len(seq) if isinstance(seq, list) else 1 for seq in sequences)
            
            padded_sequences = []
            for seq in sequences:
                if isinstance(seq, list):
                    padded = seq + [0] * (max_len - len(seq))
                else:
                    padded = [int(seq)] + [0] * (max_len - 1)
                padded_sequences.append(padded[:max_len])
            
            features_dict[feat_name] = np.array(padded_sequences, dtype=np.int32)
            
        elif feat_type == 'dense':
            # å¯†é›†ç‰¹å¾
            values = df[feat_name].values.astype(np.float32).reshape(-1, 1)
            features_dict[feat_name] = values
    
    # å‡†å¤‡æ ‡ç­¾
    labels = df['label'].values.astype(np.int32)
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = tf.data.Dataset.from_tensor_slices((features_dict, labels))
    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)
    
    return dataset

batch_size = train_config.get('training', {}).get('batch_size', 256)  # ä½¿ç”¨train.ymlçš„é»˜è®¤å€¼
full_dataset = prepare_tf_dataset_for_deep_model(df_deep_processed, deep_feat_config, batch_size)

# æŸ¥çœ‹dfæ•°æ®é›†
for features, labels in full_dataset.take(1):
    print("æ•°æ®é›†æ ¼å¼éªŒè¯:")
    print(f"  ç‰¹å¾æ•°é‡: {len(features)}")
    for name, tensor in features.items():
        print(f"  - {name}: shape={tensor.shape}, dtype={tensor.dtype}")
    print(f"  æ ‡ç­¾: shape={labels.shape}, dtype={labels.dtype}")
    break

class FeaturePipelineBuilder:
    """ç‰¹å¾å¤„ç†ç®¡é“æ„å»ºå™¨ - åŸºäºåŸé¡¹ç›®å®ç°"""
    
    def __init__(self, feat_configs: list, verbose: bool = True):
        self.feat_configs = feat_configs
        self.verbose = verbose
        self.embedding_layers = {}
        self.pooling_layers = {}
    
    def build_feature_pipelines(self) -> list:
        """æ„å»ºç‰¹å¾å¤„ç†ç®¡é“"""
        pipelines = []
        
        for config in self.feat_configs:
            feat_name = config.get('feat_name')
            feat_type = config.get('feat_type')
            
            if not feat_name or not feat_type:
                continue
                
            # åˆ›å»ºå¤„ç†å™¨åºåˆ—
            processors = self._create_processors(config)
            if processors:
                pipelines.append((feat_name, processors))
        
        if self.verbose:
            print(f"æˆåŠŸæ„å»º {len(pipelines)} ä¸ªç‰¹å¾å¤„ç†ç®¡é“")
            for feat_name, processors in pipelines:
                processor_names = [p.__class__.__name__ for p in processors]
                print(f"  {feat_name}: {' -> '.join(processor_names)}")
        
        return pipelines
    
    def _create_processors(self, config: dict) -> list:
        """æ ¹æ®ç‰¹å¾ç±»å‹åˆ›å»ºå¤„ç†å™¨"""
        feat_name = config['feat_name']
        feat_type = config['feat_type']
        vocab_size = config.get('vocabulary_size', 1000)
        embed_dim = config.get('embedding_dim', 8)
        
        if feat_type == 'sparse':
            embedding = Embedding(
                input_dim=vocab_size,
                output_dim=embed_dim,
                name=f'{feat_name}_embedding',
                mask_zero=False
            )
            return [embedding]
        
        elif feat_type == 'varlen_sparse':
            embedding = Embedding(
                input_dim=vocab_size,
                output_dim=embed_dim,
                name=f'{feat_name}_embedding',
                mask_zero=True  # å˜é•¿ç‰¹å¾éœ€è¦masking
            )
            pooling = GlobalAveragePooling1D(name=f'{feat_name}_pooling')
            return [embedding, pooling]
        
        elif feat_type == 'dense':
            # å¯†é›†ç‰¹å¾ç›´æ¥é€šè¿‡ï¼Œå¯ä»¥åŠ BN
            identity = tf.keras.layers.Lambda(lambda x: x, name=f'{feat_name}_identity')
            return [identity]
        
        return []

def process_feature_batch(features_dict: dict, pipelines: list) -> list:
    """å¤„ç†ç‰¹å¾æ‰¹æ¬¡æ•°æ®"""
    outputs = []
    
    for feat_name, processors in pipelines:
        if feat_name not in features_dict:
            continue
        
        # ä¾æ¬¡åº”ç”¨å¤„ç†å™¨
        feature_input = features_dict[feat_name]
        for processor in processors:
            feature_input = processor(feature_input)
        
        outputs.append(feature_input)
    
    return outputs
    """å‡†å¤‡TensorFlowæ•°æ®é›†ç”¨äºæ·±åº¦æ¨¡å‹è®­ç»ƒ"""
    
    # è·å–éœ€è¦çš„ç‰¹å¾ - é€‚é…ä¸åŒçš„é…ç½®æ–‡ä»¶æ ¼å¼
    if 'pipelines' in feat_config:
        # feat.yml æ ¼å¼
        pipelines = feat_config['pipelines']
    elif 'process' in feat_config and 'pipelines' in feat_config['process']:
        # config.yml æ ¼å¼
        pipelines = feat_config['process']['pipelines']
    else:
        raise ValueError("æ— æ³•æ‰¾åˆ°ç‰¹å¾é…ç½®ä¸­çš„ pipelines")
    
    pipeline_feats = {p['feat_name']: p for p in pipelines}
    features_dict = {}
    
    for feat_name, config in pipeline_feats.items():
        if feat_name not in df.columns:
            continue
        
        feat_type = config.get('feat_type', 'sparse')
        
        if feat_type == 'sparse':
            # å•å€¼ç‰¹å¾ï¼Œshapeä¸º (batch_size,) - è¿™æ˜¯å…³é”®ï¼
            values = df[feat_name].values.astype(np.int32)
            features_dict[feat_name] = values
            
        elif feat_type == 'varlen_sparse':
            # å˜é•¿ç‰¹å¾ï¼Œéœ€è¦padding
            sequences = df[feat_name].tolist()
            max_len = max(len(seq) if isinstance(seq, list) else 1 for seq in sequences)
            
            padded_sequences = []
            for seq in sequences:
                if isinstance(seq, list):
                    padded = seq + [0] * (max_len - len(seq))
                else:
                    padded = [int(seq)] + [0] * (max_len - 1)
                padded_sequences.append(padded[:max_len])
            
            features_dict[feat_name] = np.array(padded_sequences, dtype=np.int32)
            
        elif feat_type == 'dense':
            # å¯†é›†ç‰¹å¾
            values = df[feat_name].values.astype(np.float32).reshape(-1, 1)
            features_dict[feat_name] = values
    
    # å‡†å¤‡æ ‡ç­¾
    labels = df['label'].values.astype(np.int32)
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = tf.data.Dataset.from_tensor_slices((features_dict, labels))
    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)
    
    return dataset

if 'pipelines' in deep_feat_config:
    deep_pipelines = deep_feat_config['pipelines']
elif 'process' in deep_feat_config and 'pipelines' in deep_feat_config['process']:
    deep_pipelines = deep_feat_config['process']['pipelines']
else:
    raise ValueError("æ— æ³•æ‰¾åˆ°æ·±åº¦æ¨¡å‹ç‰¹å¾é…ç½®ä¸­çš„ pipelines")

class DeepMLP(tf.keras.Model):
    """æ·±åº¦MLPæ¨¡å‹ - åŸºäºåŸé¡¹ç›®å®ç°"""
    
    def __init__(self, feat_configs: list, train_config: dict = None, verbose: bool = True):
        super(DeepMLP, self).__init__()
        
        # æ„å»ºç‰¹å¾å¤„ç†ç®¡é“
        pipeline_builder = FeaturePipelineBuilder(feat_configs, verbose=verbose)
        self.feature_pipelines = pipeline_builder.build_feature_pipelines()
        
        # ç‰¹å¾è¿æ¥å±‚
        self.concat_layer = Concatenate(axis=1)
        
        # è·å–æ¨¡å‹å‚æ•° - ä»train.ymlåŠ¨æ€è¯»å–
        model_params = (train_config or {}).get('model', {})
        hidden_layers = model_params.get('layers', [128, 64, 32])  # ä½¿ç”¨train.ymlçš„é»˜è®¤å€¼
        dropout_rates = model_params.get('dropout_rates', [0.3, 0.3, 0.2])  # ä½¿ç”¨train.ymlçš„é»˜è®¤å€¼
        l2_reg = model_params.get('l2_regularization', 0.001)
        
        # æ„å»ºåˆ†ç±»å™¨
        self.classifier = self._build_classifier(hidden_layers, dropout_rates, l2_reg)
    
    def _build_classifier(self, hidden_layers: list, dropout_rates: list, l2_reg: float):
        """æ„å»ºåˆ†ç±»å™¨ç½‘ç»œ"""
        layers = []
        
        # æ·»åŠ BatchNorm
        layers.append(BatchNormalization())
        
        # æ·»åŠ éšè—å±‚
        for i, units in enumerate(hidden_layers):
            layers.append(Dense(
                units, 
                activation='relu',
                kernel_regularizer=l2(l2_reg)
            ))
            if i < len(dropout_rates):
                layers.append(Dropout(dropout_rates[i]))
        
        # è¾“å‡ºå±‚
        layers.append(Dense(1, activation='sigmoid', kernel_regularizer=l2(l2_reg)))
        
        return tf.keras.Sequential(layers)
    
    def call(self, features, training=None):
        """å‰å‘ä¼ æ’­"""
        # å¤„ç†æ‰€æœ‰ç‰¹å¾
        processed_outputs = process_feature_batch(features, self.feature_pipelines)
        
        if not processed_outputs:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„ç‰¹å¾è¾“å‡º")
        
        # åˆå¹¶ç‰¹å¾
        if len(processed_outputs) > 1:
            concat_features = self.concat_layer(processed_outputs)
        else:
            concat_features = processed_outputs[0]
        
        # åº”ç”¨åˆ†ç±»å™¨
        predictions = self.classifier(concat_features, training=training)
        return predictions

deep_model = DeepMLP(deep_pipelines, train_config, verbose=True)

# 3. ç¼–è¯‘æ¨¡å‹
learning_rate = train_config.get('training', {}).get('lr', 0.0005)  # ä½¿ç”¨train.ymlçš„é»˜è®¤å€¼
deep_model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
    loss='binary_crossentropy',
    metrics=[
        tf.keras.metrics.AUC(name='auc'),
        tf.keras.metrics.BinaryAccuracy(name='accuracy')
    ]
)

validation_split = train_config.get('training', {}).get('validation_split', 0.2)
train_size = int((1 - validation_split) * len(df_deep_processed))

print(f"\nğŸ“Š æ•°æ®ä½¿ç”¨æƒ…å†µ:")
print(f"  æ€»æ•°æ®é‡: {len(df_deep_processed):,}")
print(f"  è®­ç»ƒé›†: {train_size:,} ({(1-validation_split)*100:.1f}%)")
print(f"  éªŒè¯é›†: {len(df_deep_processed)-train_size:,} ({validation_split*100:.1f}%)")
print(f"  è®­ç»ƒæ‰¹æ¬¡æ•°: {train_size // batch_size}")
print(f"  éªŒè¯æ‰¹æ¬¡æ•°: {(len(df_deep_processed)-train_size) // batch_size}")

train_dataset = full_dataset.take(train_size // batch_size)
val_dataset = full_dataset.skip(train_size // batch_size)

epochs = train_config.get('training', {}).get('epochs', 2)  # ä½¿ç”¨train.ymlçš„é»˜è®¤å€¼
print(f"\nå¼€å§‹è®­ç»ƒæ·±åº¦æ¨¡å‹... (epochs={epochs})")

history = deep_model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=epochs,
    verbose=1
)

# 6. è¾“å‡ºç»“æœ
print("\n--- æ·±åº¦æ¨¡å‹è®­ç»ƒå®Œæˆ ---")
if 'val_auc' in history.history:
    final_train_auc = history.history['auc'][-1]
    final_val_auc = history.history['val_auc'][-1]
    print(f"æœ€ç»ˆè®­ç»ƒé›† AUC: {final_train_auc:.4f}")
    print(f"æœ€ç»ˆéªŒè¯é›† AUC: {final_val_auc:.4f}")
    print(f"AUCå·®å¼‚: {final_train_auc - final_val_auc:.4f}")
else:
    final_auc = history.history['auc'][-1]
    print(f"æœ€ç»ˆ AUC: {final_auc:.4f}")

print(f"æ·±åº¦æ¨¡å‹æ¶æ„æ€»ç»“:")
print(f"ç‰¹å¾å¤„ç†ç®¡é“: {len(deep_model.feature_pipelines)} ä¸ª")
print(f"åˆ†ç±»å™¨å±‚æ•°: {len([l for l in deep_model.classifier.layers if isinstance(l, Dense)])}")
print(f"æ€»å‚æ•°é‡: {deep_model.count_params():,}")

# æ˜¾ç¤ºæ·±åº¦æ¨¡å‹è®­ç»ƒç»“æœ
print(f"æ·±åº¦æ¨¡å‹è®­ç»ƒç»“æœ:")
if 'val_auc' in history.history:
    print(f"éªŒè¯é›†AUC: {final_val_auc:.4f}")
    print(f"è®­ç»ƒé›†AUC: {history.history['auc'][-1]:.4f}")
else:
    print(f"AUC: {final_auc:.4f}")


#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
MLPæ¨é€äºŒåˆ†ç±»æ¨¡å‹è®­ç»ƒè„šæœ¬
"""

import os
import sys
from typing import Dict, List, Tuple, Any, Optional, Union

import numpy as np
import tensorflow as tf

# è·å–é¡¹ç›®æ ¹ç›®å½•å’Œenvç›®å½•
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))  
ENV_DIR = os.path.join(PROJECT_ROOT, 'env')

# å°†å¿…è¦çš„ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)
if ENV_DIR not in sys.path:
    sys.path.insert(0, ENV_DIR)

# TensorFlowæ•°æ®ç±»å‹æ˜ å°„(å»æ‰light_cträ¾èµ–)
TF_DTYPE_MAPPING = {
    "float16": tf.float16,
    "float32": tf.float32,
    "float64": tf.float64,
    "int8": tf.int8,
    "int16": tf.int16,
    "int32": tf.int32,
    "int64": tf.int64,
    "uint8": tf.uint8,
    "uint16": tf.uint16,
    "uint32": tf.uint32,
    "uint64": tf.uint64,
    "bool": tf.bool,
    "string": tf.string,
    "complex64": tf.complex64,
    "complex128": tf.complex128,
    "qint8": tf.qint8,
    "qint16": tf.qint16,
    "qint32": tf.qint32,
    "quint8": tf.quint8,
    "quint16": tf.quint16,
}

# å¯¼å…¥æ‰€éœ€çš„å·¥å…·å‡½æ•°
from src.utils.environment_utils import setup_environment
from src.utils.training_utils import train_model
from src.utils.feature_analysis_utils import check_feature_importance, plot_feature_importance
from src.utils.config_loader import load_data_config, load_train_config
from src.data.dataset_utils import inspect_datasets
from src.data.data_preparation import prepare_datasets
from src.utils.gpu_utils import setup_gpu
from src.models.model_utils import create_and_compile_model, test_model_on_batch
# ä»æ·±åº¦æ¨¡å‹åŒ…å¯¼å…¥MLPæ¨¡å‹
from src.models.deep import MLP
from src.data.feature_preprocessor import apply_feature_preprocessing
# å¯¼å…¥æ–°çš„GAUCå·¥å…·
from src.utils.gauc_utils import calculate_gauc, calculate_gauc_with_original_data, save_gauc_results, compare_auc_gauc, validate_gauc_calculation


def set_random_seeds(seed: int = 42) -> None:
    """
    è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯å¤ç°
    
    å‚æ•°:
        seed: éšæœºç§å­å€¼
    """
    tf.random.set_seed(seed)
    np.random.seed(seed)
    print(f"éšæœºç§å­å·²è®¾ç½®ä¸º: {seed}")


def setup_directories() -> None:
    """
    åˆ›å»ºå¿…è¦çš„æ—¥å¿—å’Œæ¨¡å‹ç›®å½•
    """
    os.makedirs("./logs", exist_ok=True)
    os.makedirs("./models", exist_ok=True)
    print("å·²åˆ›å»ºæ—¥å¿—å’Œæ¨¡å‹ç›®å½•")


def load_configurations() -> Tuple[Dict[str, Any], Optional[Dict[str, Any]]]:
    """
    åŠ è½½æ‰€æœ‰å¿…è¦çš„é…ç½®æ–‡ä»¶
    
    è¿”å›:
        data_config: æ•°æ®é…ç½®
        train_config: è®­ç»ƒé…ç½®
    """
    # åŠ è½½æ•°æ®é…ç½®
    data_config = load_data_config()
    print("æ•°æ®é…ç½®å·²åŠ è½½")
    
    # åŠ è½½è®­ç»ƒé…ç½®
    try:
        train_config = load_train_config()
        print("è®­ç»ƒé…ç½®å·²åŠ è½½")
    except Exception as e:
        print(f"åŠ è½½è®­ç»ƒé…ç½®å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼")
        train_config = None
        
    return data_config, train_config


def trace_model(model: tf.keras.Model, dataset: tf.data.Dataset) -> None:
    """
    é€šè¿‡æ‰§è¡Œä¸€æ¬¡å‰å‘ä¼ é€’æ¥è¿½è¸ªæ¨¡å‹çš„æ‰€æœ‰å‡½æ•°ï¼Œè§£å†³æœªè¿½è¸ªå‡½æ•°çš„è­¦å‘Šé—®é¢˜
    
    å‚æ•°:
        model: éœ€è¦è¿½è¸ªçš„æ¨¡å‹
        dataset: ç”¨äºæ‰§è¡Œå‰å‘ä¼ é€’çš„æ•°æ®é›†
    """
    # è·å–ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®
    for batch in dataset.take(1):
        # æå–ç‰¹å¾å’Œæ ‡ç­¾
        features, labels = batch
        
        # æ£€æŸ¥featuresæ˜¯å¦ä¸ºå­—å…¸ç±»å‹ï¼Œè¿™æ˜¯å¤šè¾“å…¥ç‰¹å¾çš„å¸¸è§æƒ…å†µ
        if isinstance(features, dict):
            # ä½¿ç”¨tf.functionè£…é¥°å™¨ï¼Œä¸ºå­—å…¸ç±»å‹çš„è¾“å…¥åˆ›å»ºè¾“å…¥ç­¾å
            @tf.function
            def trace_forward(inputs):
                return model(inputs, training=False)
            
            # æ‰§è¡Œå‰å‘ä¼ é€’æ¥è¿½è¸ªæ‰€æœ‰å‡½æ•°
            _ = trace_forward(features)
        else:
            # å•ä¸€è¾“å…¥ç‰¹å¾çš„æƒ…å†µï¼ˆä¸å¤ªå¯èƒ½å‡ºç°åœ¨æ­¤æ¨¡å‹ä¸­ï¼‰
            @tf.function(input_signature=[tf.TensorSpec(shape=features.shape, dtype=features.dtype)])
            def trace_forward(inputs):
                return model(inputs, training=False)
            
            # æ‰§è¡Œå‰å‘ä¼ é€’æ¥è¿½è¸ªæ‰€æœ‰å‡½æ•°
            _ = trace_forward(features)
        
        print("æ¨¡å‹å‡½æ•°è¿½è¸ªå®Œæˆï¼Œè¿™å°†å‡å°‘ä¿å­˜æ¨¡å‹æ—¶çš„æœªè¿½è¸ªå‡½æ•°è­¦å‘Š")
        break


def prepare_model_and_data() -> Tuple[
    tf.keras.Model,
    tf.data.Dataset,
    tf.data.Dataset,
    tf.data.Dataset
]:
    """
    å‡†å¤‡æ¨¡å‹å’Œæ•°æ®
    
    è¿”å›:
        model: ç¼–è¯‘å¥½çš„æ¨¡å‹
        full_dataset: å®Œæ•´æ•°æ®é›†
        train_dataset: è®­ç»ƒæ•°æ®é›†
        validation_dataset: éªŒè¯æ•°æ®é›†
    """
    # 1. åŠ è½½é…ç½®
    data_config, train_config = load_configurations()
    
    # 2. æ•°æ®å‡†å¤‡
    datasets = prepare_dataset_from_config(data_config, train_config)
    full_dataset, train_dataset, validation_dataset = datasets[:3]
    
    # 3. æ£€æŸ¥æ•°æ®é›†
    inspect_datasets(full_dataset, train_dataset, validation_dataset)
    
    # 4. åˆ›å»ºå¹¶ç¼–è¯‘æ¨¡å‹
    model = create_and_compile_model(MLP, train_config)
    
    # 5. é€šè¿‡æ‰§è¡Œä¸€æ¬¡å‰å‘ä¼ é€’æ¥è¿½è¸ªæ¨¡å‹å‡½æ•°ï¼Œå‡å°‘ä¿å­˜æ—¶çš„è­¦å‘Š
    trace_model(model, full_dataset)
    
    return model, full_dataset, train_dataset, validation_dataset


def prepare_dataset_from_config(
    data_config: Dict[str, Any], 
    train_config: Optional[Dict[str, Any]]
) -> Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset, List[str], Dict[str, tf.TensorShape]]:
    """
    ä»é…ç½®ä¸­å‡†å¤‡æ•°æ®é›†
    
    å‚æ•°:
        data_config: æ•°æ®é…ç½®
        train_config: è®­ç»ƒé…ç½®
        
    è¿”å›:
        processed_full_dataset: å¤„ç†åçš„å®Œæ•´æ•°æ®é›†
        processed_train_dataset: å¤„ç†åçš„è®­ç»ƒæ•°æ®é›†  
        processed_validation_dataset: å¤„ç†åçš„éªŒè¯æ•°æ®é›†
        column_names: åŸå§‹åˆ—ååˆ—è¡¨
        input_signature: è¾“å…¥ç­¾å
    """
    print("\n" + "="*50)
    print("å¼€å§‹æ•°æ®é›†å‡†å¤‡å’Œç‰¹å¾å¤„ç†")
    print("="*50)
    
    # 1. åŠ è½½åŸå§‹æ•°æ®é›†
    print("\nğŸ“‚ æ­¥éª¤1: åŠ è½½åŸå§‹CSVæ•°æ®...")
    full_dataset, train_dataset, validation_dataset, column_names, input_signature = prepare_datasets(
        data_config, train_config, TF_DTYPE_MAPPING
    )
    
    print(f"âœ… åŸå§‹æ•°æ®åŠ è½½å®Œæˆ")
    print(f"   åŸå§‹ç‰¹å¾åˆ—: {column_names}")
    
    # 2. åº”ç”¨ç‰¹å¾å¤„ç†
    print("\nğŸ”§ æ­¥éª¤2: åº”ç”¨UniProcessç‰¹å¾å¤„ç†...")
    try:
        # å¤„ç†å®Œæ•´æ•°æ®é›†
        print("   å¤„ç†å®Œæ•´æ•°æ®é›†...")
        processed_full_dataset = apply_feature_preprocessing(
            full_dataset, 
            feat_config_path="config/feat.yml",
            verbose=True
        )
        
        # å¤„ç†è®­ç»ƒæ•°æ®é›†
        print("   å¤„ç†è®­ç»ƒæ•°æ®é›†...")
        processed_train_dataset = apply_feature_preprocessing(
            train_dataset,
            feat_config_path="config/feat.yml", 
            verbose=False  # é¿å…é‡å¤æ—¥å¿—
        )
        
        # å¤„ç†éªŒè¯æ•°æ®é›†
        print("   å¤„ç†éªŒè¯æ•°æ®é›†...")
        processed_validation_dataset = apply_feature_preprocessing(
            validation_dataset,
            feat_config_path="config/feat.yml",
            verbose=False  # é¿å…é‡å¤æ—¥å¿—
        )
        
        print("âœ… ç‰¹å¾å¤„ç†å®Œæˆ")
        
        # 3. éªŒè¯å¤„ç†åçš„æ•°æ®é›†
        print("\nğŸ” æ­¥éª¤3: éªŒè¯å¤„ç†åçš„æ•°æ®é›†...")
        _validate_processed_datasets(
            processed_full_dataset, 
            processed_train_dataset, 
            processed_validation_dataset
        )
        
        return (processed_full_dataset, processed_train_dataset, 
                processed_validation_dataset, column_names, input_signature)
        
    except Exception as e:
        print(f"âŒ ç‰¹å¾å¤„ç†å¤±è´¥: {e}")
        print("ğŸ”„ å›é€€åˆ°åŸå§‹æ•°æ®é›†...")
        return full_dataset, train_dataset, validation_dataset, column_names, input_signature


def _validate_processed_datasets(full_dataset: tf.data.Dataset,
                                train_dataset: tf.data.Dataset, 
                                validation_dataset: tf.data.Dataset) -> None:
    """éªŒè¯å¤„ç†åçš„æ•°æ®é›†
    
    Args:
        full_dataset: å¤„ç†åçš„å®Œæ•´æ•°æ®é›†
        train_dataset: å¤„ç†åçš„è®­ç»ƒæ•°æ®é›†
        validation_dataset: å¤„ç†åçš„éªŒè¯æ•°æ®é›†
    """
    try:
        # æ£€æŸ¥å¤„ç†åçš„ç‰¹å¾åç§°å’Œæ•°æ®ç±»å‹
        for batch_features, batch_labels in full_dataset.take(1):
            print(f"   å¤„ç†åç‰¹å¾æ•°é‡: {len(batch_features)}")
            print(f"   ç‰¹å¾åç§°: {list(batch_features.keys())}")
            
            # æ£€æŸ¥å‰å‡ ä¸ªç‰¹å¾çš„æ•°æ®ç±»å‹å’Œæ ·ä¾‹
            feature_sample = {}
            for i, (name, tensor) in enumerate(batch_features.items()):
                if i < 3:  # åªæ˜¾ç¤ºå‰3ä¸ªç‰¹å¾çš„è¯¦ç»†ä¿¡æ¯
                    feature_sample[name] = {
                        'shape': tensor.shape,
                        'dtype': tensor.dtype,
                        'sample_values': tensor.numpy()[:3] if tensor.shape[0] > 0 else 'empty'
                    }
                    print(f"   ç‰¹å¾ '{name}': shape={tensor.shape}, dtype={tensor.dtype}")
            
            # æ£€æŸ¥æ ‡ç­¾
            print(f"   æ ‡ç­¾shape: {batch_labels.shape}, dtype: {batch_labels.dtype}")
            
            break
            
        print("âœ… æ•°æ®é›†éªŒè¯é€šè¿‡")
        
    except Exception as e:
        print(f"âš ï¸  æ•°æ®é›†éªŒè¯è­¦å‘Š: {e}")


def train_and_evaluate_model(
    model: tf.keras.Model,
    full_dataset: tf.data.Dataset,
    train_dataset: tf.data.Dataset,
    validation_dataset: tf.data.Dataset,
    train_config: Optional[Dict[str, Any]] = None
) -> None:
    """
    è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹
    
    å‚æ•°:
        model: å¾…è®­ç»ƒçš„æ¨¡å‹
        full_dataset: å®Œæ•´æ•°æ®é›†
        train_dataset: è®­ç»ƒæ•°æ®é›†
        validation_dataset: éªŒè¯æ•°æ®é›†
        train_config: è®­ç»ƒé…ç½®
    """
    # 1. æµ‹è¯•æ¨¡å‹
    if not test_model_on_batch(model, full_dataset):
        print("æ¨¡å‹æµ‹è¯•å¤±è´¥ï¼Œè·³è¿‡è®­ç»ƒæ­¥éª¤")
        return
    
    # 2. è®­ç»ƒæ¨¡å‹
    print("\nå¼€å§‹è®­ç»ƒæ¨¡å‹...")
    history = train_model(
        model, full_dataset, train_dataset, validation_dataset, 
        train_config=train_config
    )
    
    # 3. å‡†å¤‡åŸå§‹æ•°æ®é›†ç”¨äºGAUCè®¡ç®—
    print("\nğŸ”„ å‡†å¤‡åŸå§‹æ•°æ®é›†ç”¨äºGAUCè®¡ç®—...")
    try:
        data_config, _ = load_configurations()
        original_full_dataset, _, original_validation_dataset, _, _ = prepare_datasets(
            data_config, train_config, TF_DTYPE_MAPPING
        )
        print("âœ… åŸå§‹æ•°æ®é›†å‡†å¤‡å®Œæˆ")
        
        # 4. è¾“å‡ºè®­ç»ƒç»“æœï¼ˆåŒ…æ‹¬AUCå’ŒGAUCï¼‰
        print_training_results_with_gauc(
            history, model, validation_dataset, original_validation_dataset, train_config
        )
        
    except Exception as e:
        print(f"âš ï¸ åŸå§‹æ•°æ®é›†å‡†å¤‡å¤±è´¥: {e}")
        print("ğŸ”„ ä½¿ç”¨åŸºæœ¬è¯„ä»·æ–¹å¼...")
        print_training_results(history)
    
    # æ‰“å°æ¨¡å‹çš„ç‰¹å¾ç®¡é“ä¿¡æ¯
    print("\næ¨¡å‹ç‰¹å¾ç®¡é“ä¿¡æ¯:")
    if hasattr(model, 'feature_pipelines'):
        print(f"å…±æœ‰ {len(model.feature_pipelines)} ä¸ªç‰¹å¾å¤„ç†ç®¡é“")
        for idx, (feature_name, processors) in enumerate(model.feature_pipelines):
            processor_names = [p.__class__.__name__ for p in processors]
            print(f"ç®¡é“ #{idx+1}: è¾“å…¥ç‰¹å¾ '{feature_name}' -> å¤„ç†å™¨: {' -> '.join(processor_names)}")
            
            # ç‰¹åˆ«æ ‡è®°BERTç›¸å…³ç‰¹å¾ç®¡é“
            for processor in processors:
                if any(bert_class in processor.__class__.__name__ for bert_class in ['BertEmbedding', 'PrecomputedEmbedding']):
                    print(f"  [é«˜çº§ç‰¹å¾] å‘ç°BERT/é¢„è®¡ç®—å¤„ç†å™¨: {processor.__class__.__name__}")
    else:
        print("æ¨¡å‹æ²¡æœ‰å®šä¹‰ç‰¹å¾å¤„ç†ç®¡é“")
    
    # 5. è¯„ä¼°ç‰¹å¾é‡è¦æ€§
    print("\nå¼€å§‹è¯„ä¼°ç‰¹å¾é‡è¦æ€§...")
    feature_importance = check_feature_importance(
        model, validation_dataset, train_config=train_config
    )
    
    # 6. ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾
    plot_feature_importance(feature_importance)


def print_training_results_with_gauc(
    history: tf.keras.callbacks.History,
    model: tf.keras.Model,
    processed_validation_dataset: tf.data.Dataset,
    original_validation_dataset: tf.data.Dataset,
    train_config: Optional[Dict[str, Any]] = None
) -> None:
    """
    æ‰“å°è®­ç»ƒç»“æœï¼ŒåŒ…æ‹¬AUCå’ŒGAUCæŒ‡æ ‡
    
    å‚æ•°:
        history: è®­ç»ƒå†å²å¯¹è±¡
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        processed_validation_dataset: ç»è¿‡ç‰¹å¾å¤„ç†çš„éªŒè¯æ•°æ®é›†
        original_validation_dataset: åŸå§‹éªŒè¯æ•°æ®é›†ï¼ˆåŒ…å«user_idï¼‰
        train_config: è®­ç»ƒé…ç½®
    """
    import os
    import json
    import csv
    import datetime
    
    print(f"\nğŸ“Š æ¨¡å‹è®­ç»ƒå®Œæˆ - æ€§èƒ½è¯„ä¼°ç»“æœ")
    
    # 1. åŸºæœ¬AUCæŒ‡æ ‡
    train_auc = history.history['auc'][-1]
    val_auc = history.history['val_auc'][-1]
    auc_diff = train_auc - val_auc
    
    print(f"ğŸ¯ AUC æŒ‡æ ‡:")
    print(f"  è®­ç»ƒé›†AUC: {train_auc:.6f}")
    print(f"  éªŒè¯é›†AUC: {val_auc:.6f}")
    
    # 2. è¿‡æ‹Ÿåˆæ£€æµ‹
    if auc_diff > 0.05:
        print(f"  âš ï¸  è¿‡æ‹Ÿåˆè­¦å‘Š: è®­ç»ƒAUC - éªŒè¯AUC = {auc_diff:.4f}")
    else:
        print(f"  âœ… æ³›åŒ–æ€§èƒ½è‰¯å¥½: å·®å¼‚ = {auc_diff:.4f}")
    
    # åˆå§‹åŒ–æ—¥å¿—æ•°æ®
    log_data = {
        "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "train_auc": float(train_auc),
        "val_auc": float(val_auc),
        "auc_diff": float(auc_diff),
        "val_gauc": 0.0,
        "auc_gauc_gap": 0.0,
        "gauc_success": False,
        "valid_users": 0,
        "total_samples": 0,
        "notes": ""
    }
    
    # 3. GAUCæŒ‡æ ‡è®¡ç®—
    print(f"\nğŸ” å¼€å§‹è®¡ç®—GAUCæŒ‡æ ‡...")
    try:
        # è®¡ç®—GAUCï¼ˆä½¿ç”¨åŸå§‹æ•°æ®é›†è·å–user_idï¼‰
        max_batches = train_config.get('gauc_eval_batches', 50) if train_config else 50
        
        gauc_score, gauc_info = calculate_gauc_with_original_data(
            model, 
            processed_validation_dataset.take(max_batches),
            original_validation_dataset.take(max_batches),
            min_samples_per_user=2,
            max_batches=max_batches,
            verbose=True
        )
        
        print(f"\nğŸ† GAUC æŒ‡æ ‡:")
        print(f"  éªŒè¯é›†GAUC: {gauc_score:.6f}")
        print(f"  æœ‰æ•ˆç”¨æˆ·æ•°: {gauc_info.get('valid_users', 0)}")
        print(f"  æ€»æ ·æœ¬æ•°: {gauc_info.get('total_samples', 0)}")
        
        # æ›´æ–°æ—¥å¿—æ•°æ®
        log_data.update({
            "val_gauc": float(gauc_score),
            "auc_gauc_gap": float(val_auc - gauc_score),
            "gauc_success": True,
            "valid_users": gauc_info.get('valid_users', 0),
            "total_samples": gauc_info.get('total_samples', 0),
            "notes": "GAUCè®¡ç®—æˆåŠŸ"
        })
        
        # 4. AUC vs GAUC å¯¹æ¯”åˆ†æ
        comparison = compare_auc_gauc(val_auc, gauc_score, verbose=True)
        
        # 5. ä¿å­˜GAUCç»“æœ
        save_path = save_gauc_results(gauc_score, gauc_info)
        print(f"\nğŸ’¾ GAUCç»“æœå·²ä¿å­˜åˆ°: {save_path}")
        
        # 6. è¾“å‡ºæ€»ç»“
        print(f"\nğŸ“‹ æ€§èƒ½æ€»ç»“:")
        print(f"  âœ… éªŒè¯é›†AUC:  {val_auc:.6f}")
        print(f"  âœ… éªŒè¯é›†GAUC: {gauc_score:.6f}")
        print(f"  ğŸ“Š AUC-GAUCå·®å¼‚: {comparison['absolute_difference']:.6f} ({comparison['relative_difference_percent']:.2f}%)")
        print(f"  ğŸ¯ æ€§èƒ½è§£è¯»: {comparison['interpretation']}")
        
    except Exception as e:
        print(f"  âŒ GAUCè®¡ç®—å¤±è´¥: {e}")
        print(f"  ğŸ’¡ å¯èƒ½åŸå› : æ•°æ®é›†ç¼ºå°‘user_idå­—æ®µæˆ–ç”¨æˆ·æ ·æœ¬ä¸è¶³")
        print(f"  ğŸ”„ ç»§ç»­ä½¿ç”¨AUCæŒ‡æ ‡è¯„ä¼°æ¨¡å‹æ€§èƒ½")
        
        # æ›´æ–°æ—¥å¿—æ•°æ®ï¼ˆGAUCå¤±è´¥æƒ…å†µï¼‰
        log_data.update({
            "notes": f"GAUCè®¡ç®—å¤±è´¥: {str(e)}"
        })
        
        import traceback
        traceback.print_exc()
    
    # 7. ä¿å­˜ç®€å•çš„AUC-GAUCå¯¹æ¯”æ—¥å¿—
    try:
        os.makedirs('./logs', exist_ok=True)
        
        # ç”Ÿæˆæ—¥å¿—æ–‡ä»¶å
        timestamp_str = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        csv_log_file = f'./logs/auc_gauc_log_{timestamp_str}.csv'
        json_log_file = f'./logs/auc_gauc_log_{timestamp_str}.json'
        
        # ä¿å­˜JSONæ ¼å¼çš„è¯¦ç»†æ—¥å¿—
        with open(json_log_file, 'w') as f:
            json.dump(log_data, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜CSVæ ¼å¼çš„ç®€åŒ–æ—¥å¿—
        csv_exists = os.path.exists('./logs/auc_gauc_summary.csv')
        with open('./logs/auc_gauc_summary.csv', 'a', newline='') as csvfile:
            writer = csv.writer(csvfile)
            
            # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå†™å…¥å¤´éƒ¨
            if not csv_exists:
                writer.writerow([
                    'timestamp', 'train_auc', 'val_auc', 'val_gauc', 
                    'auc_gauc_gap', 'gauc_success', 'valid_users', 'notes'
                ])
            
            # å†™å…¥æ•°æ®è¡Œ
            writer.writerow([
                log_data['timestamp'],
                f"{log_data['train_auc']:.6f}",
                f"{log_data['val_auc']:.6f}",
                f"{log_data['val_gauc']:.6f}",
                f"{log_data['auc_gauc_gap']:.6f}",
                log_data['gauc_success'],
                log_data['valid_users'],
                log_data['notes']
            ])
        
        print(f"\nğŸ“ AUC-GAUCå¯¹æ¯”æ—¥å¿—å·²ä¿å­˜:")
        print(f"  ğŸ“„ CSVæ±‡æ€»: ./logs/auc_gauc_summary.csv")
        print(f"  ğŸ“‹ è¯¦ç»†è®°å½•: {json_log_file}")
        
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜æ—¥å¿—å¤±è´¥: {e}")


def print_training_results(history: tf.keras.callbacks.History) -> None:
    """
    æ‰“å°è®­ç»ƒç»“æœï¼ˆä¿ç•™åŸå‡½æ•°ä½œä¸ºå‘åå…¼å®¹ï¼‰
    
    å‚æ•°:
        history: è®­ç»ƒå†å²å¯¹è±¡
    """
    print(f"\næ¨¡å‹è®­ç»ƒå®Œæˆ")
    print(f"è®­ç»ƒé›†æœ€ç»ˆAUC: {history.history['auc'][-1]:.6f}")
    print(f"éªŒè¯é›†æœ€ç»ˆAUC: {history.history['val_auc'][-1]:.6f}")
    
    # è®¡ç®—å¹¶æ‰“å°è¿‡æ‹ŸåˆæŒ‡æ ‡
    train_auc = history.history['auc'][-1]
    val_auc = history.history['val_auc'][-1]
    auc_diff = train_auc - val_auc
    
    if auc_diff > 0.05:
        print(f"è­¦å‘Š: å¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆç°è±¡ (è®­ç»ƒAUC - éªŒè¯AUC = {auc_diff:.4f})")


def setup_environment_for_training() -> None:
    """è®¾ç½®è®­ç»ƒç¯å¢ƒ"""
    setup_gpu()
    setup_environment()
    setup_directories()
    set_random_seeds()
    print("è®­ç»ƒç¯å¢ƒè®¾ç½®å®Œæˆ")


def main() -> None:
    """ä¸»å‡½æ•°ï¼Œæ‰§è¡Œè®­ç»ƒæµç¨‹"""
    # 1. ç¯å¢ƒè®¾ç½®
    setup_environment_for_training()
    
    # 2. å‡†å¤‡æ¨¡å‹å’Œæ•°æ®
    model, full_dataset, train_dataset, validation_dataset = prepare_model_and_data()
    
    # 3. é‡æ–°åŠ è½½è®­ç»ƒé…ç½®ï¼ˆç¡®ä¿ä½¿ç”¨æœ€æ–°é…ç½®ï¼‰
    _, train_config = load_configurations()
    
    # 4. è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹
    train_and_evaluate_model(
        model, full_dataset, train_dataset, validation_dataset, 
        train_config=train_config
    )
    
    print("\nè®­ç»ƒæµç¨‹å®Œæˆ")


if __name__ == "__main__":
    main() 
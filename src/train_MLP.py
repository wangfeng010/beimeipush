#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
MLPæ¨é€äºŒåˆ†ç±»æ¨¡å‹è®­ç»ƒè„šæœ¬
"""

import os
import sys
from typing import Dict, List, Tuple, Any, Optional, Union

import numpy as np
import tensorflow as tf

# è·å–é¡¹ç›®æ ¹ç›®å½•å’Œenvç›®å½•
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))  
ENV_DIR = os.path.join(PROJECT_ROOT, 'env')

# å°†å¿…è¦çš„ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)
if ENV_DIR not in sys.path:
    sys.path.insert(0, ENV_DIR)

# TensorFlowæ•°æ®ç±»å‹æ˜ å°„(å»æ‰light_cträ¾èµ–)
TF_DTYPE_MAPPING = {
    "float16": tf.float16,
    "float32": tf.float32,
    "float64": tf.float64,
    "int8": tf.int8,
    "int16": tf.int16,
    "int32": tf.int32,
    "int64": tf.int64,
    "uint8": tf.uint8,
    "uint16": tf.uint16,
    "uint32": tf.uint32,
    "uint64": tf.uint64,
    "bool": tf.bool,
    "string": tf.string,
    "complex64": tf.complex64,
    "complex128": tf.complex128,
    "qint8": tf.qint8,
    "qint16": tf.qint16,
    "qint32": tf.qint32,
    "quint8": tf.quint8,
    "quint16": tf.quint16,
}

# å¯¼å…¥æ‰€éœ€çš„å·¥å…·å‡½æ•°
from src.utils.environment_utils import setup_environment
from src.utils.training_utils import train_model
from src.utils.feature_analysis_utils import check_feature_importance, plot_feature_importance
from src.utils.config_loader import load_data_config, load_train_config
from src.data.dataset_utils import inspect_datasets
from src.data.data_preparation import prepare_datasets
from src.utils.gpu_utils import setup_gpu
from src.models.model_utils import create_and_compile_model, test_model_on_batch
# ä»æ·±åº¦æ¨¡å‹åŒ…å¯¼å…¥MLPæ¨¡å‹
from src.models.deep import MLP
from src.data.feature_preprocessor import apply_feature_preprocessing


def set_random_seeds(seed: int = 42) -> None:
    """
    è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯å¤ç°
    
    å‚æ•°:
        seed: éšæœºç§å­å€¼
    """
    tf.random.set_seed(seed)
    np.random.seed(seed)
    print(f"éšæœºç§å­å·²è®¾ç½®ä¸º: {seed}")


def setup_directories() -> None:
    """
    åˆ›å»ºå¿…è¦çš„æ—¥å¿—å’Œæ¨¡å‹ç›®å½•
    """
    os.makedirs("./logs", exist_ok=True)
    os.makedirs("./models", exist_ok=True)
    print("å·²åˆ›å»ºæ—¥å¿—å’Œæ¨¡å‹ç›®å½•")


def load_configurations() -> Tuple[Dict[str, Any], Optional[Dict[str, Any]]]:
    """
    åŠ è½½æ‰€æœ‰å¿…è¦çš„é…ç½®æ–‡ä»¶
    
    è¿”å›:
        data_config: æ•°æ®é…ç½®
        train_config: è®­ç»ƒé…ç½®
    """
    # åŠ è½½æ•°æ®é…ç½®
    data_config = load_data_config()
    print("æ•°æ®é…ç½®å·²åŠ è½½")
    
    # åŠ è½½è®­ç»ƒé…ç½®
    try:
        train_config = load_train_config()
        print("è®­ç»ƒé…ç½®å·²åŠ è½½")
    except Exception as e:
        print(f"åŠ è½½è®­ç»ƒé…ç½®å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼")
        train_config = None
        
    return data_config, train_config


def trace_model(model: tf.keras.Model, dataset: tf.data.Dataset) -> None:
    """
    é€šè¿‡æ‰§è¡Œä¸€æ¬¡å‰å‘ä¼ é€’æ¥è¿½è¸ªæ¨¡å‹çš„æ‰€æœ‰å‡½æ•°ï¼Œè§£å†³æœªè¿½è¸ªå‡½æ•°çš„è­¦å‘Šé—®é¢˜
    
    å‚æ•°:
        model: éœ€è¦è¿½è¸ªçš„æ¨¡å‹
        dataset: ç”¨äºæ‰§è¡Œå‰å‘ä¼ é€’çš„æ•°æ®é›†
    """
    # è·å–ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®
    for batch in dataset.take(1):
        # æå–ç‰¹å¾å’Œæ ‡ç­¾
        features, labels = batch
        
        # æ£€æŸ¥featuresæ˜¯å¦ä¸ºå­—å…¸ç±»å‹ï¼Œè¿™æ˜¯å¤šè¾“å…¥ç‰¹å¾çš„å¸¸è§æƒ…å†µ
        if isinstance(features, dict):
            # ä½¿ç”¨tf.functionè£…é¥°å™¨ï¼Œä¸ºå­—å…¸ç±»å‹çš„è¾“å…¥åˆ›å»ºè¾“å…¥ç­¾å
            @tf.function
            def trace_forward(inputs):
                return model(inputs, training=False)
            
            # æ‰§è¡Œå‰å‘ä¼ é€’æ¥è¿½è¸ªæ‰€æœ‰å‡½æ•°
            _ = trace_forward(features)
        else:
            # å•ä¸€è¾“å…¥ç‰¹å¾çš„æƒ…å†µï¼ˆä¸å¤ªå¯èƒ½å‡ºç°åœ¨æ­¤æ¨¡å‹ä¸­ï¼‰
            @tf.function(input_signature=[tf.TensorSpec(shape=features.shape, dtype=features.dtype)])
            def trace_forward(inputs):
                return model(inputs, training=False)
            
            # æ‰§è¡Œå‰å‘ä¼ é€’æ¥è¿½è¸ªæ‰€æœ‰å‡½æ•°
            _ = trace_forward(features)
        
        print("æ¨¡å‹å‡½æ•°è¿½è¸ªå®Œæˆï¼Œè¿™å°†å‡å°‘ä¿å­˜æ¨¡å‹æ—¶çš„æœªè¿½è¸ªå‡½æ•°è­¦å‘Š")
        break


def prepare_model_and_data() -> Tuple[
    tf.keras.Model,
    tf.data.Dataset,
    tf.data.Dataset,
    tf.data.Dataset
]:
    """
    å‡†å¤‡æ¨¡å‹å’Œæ•°æ®
    
    è¿”å›:
        model: ç¼–è¯‘å¥½çš„æ¨¡å‹
        full_dataset: å®Œæ•´æ•°æ®é›†
        train_dataset: è®­ç»ƒæ•°æ®é›†
        validation_dataset: éªŒè¯æ•°æ®é›†
    """
    # 1. åŠ è½½é…ç½®
    data_config, train_config = load_configurations()
    
    # 2. æ•°æ®å‡†å¤‡
    datasets = prepare_dataset_from_config(data_config, train_config)
    full_dataset, train_dataset, validation_dataset = datasets[:3]
    
    # 3. æ£€æŸ¥æ•°æ®é›†
    inspect_datasets(full_dataset, train_dataset, validation_dataset)
    
    # 4. åˆ›å»ºå¹¶ç¼–è¯‘æ¨¡å‹
    model = create_and_compile_model(MLP, train_config)
    
    # 5. é€šè¿‡æ‰§è¡Œä¸€æ¬¡å‰å‘ä¼ é€’æ¥è¿½è¸ªæ¨¡å‹å‡½æ•°ï¼Œå‡å°‘ä¿å­˜æ—¶çš„è­¦å‘Š
    trace_model(model, full_dataset)
    
    return model, full_dataset, train_dataset, validation_dataset


def prepare_dataset_from_config(
    data_config: Dict[str, Any], 
    train_config: Optional[Dict[str, Any]]
) -> Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset, List[str], Dict[str, tf.TensorShape]]:
    """
    ä»é…ç½®ä¸­å‡†å¤‡æ•°æ®é›†
    
    å‚æ•°:
        data_config: æ•°æ®é…ç½®
        train_config: è®­ç»ƒé…ç½®
        
    è¿”å›:
        processed_full_dataset: å¤„ç†åçš„å®Œæ•´æ•°æ®é›†
        processed_train_dataset: å¤„ç†åçš„è®­ç»ƒæ•°æ®é›†  
        processed_validation_dataset: å¤„ç†åçš„éªŒè¯æ•°æ®é›†
        column_names: åŸå§‹åˆ—ååˆ—è¡¨
        input_signature: è¾“å…¥ç­¾å
    """
    print("\n" + "="*50)
    print("å¼€å§‹æ•°æ®é›†å‡†å¤‡å’Œç‰¹å¾å¤„ç†")
    print("="*50)
    
    # 1. åŠ è½½åŸå§‹æ•°æ®é›†
    print("\nğŸ“‚ æ­¥éª¤1: åŠ è½½åŸå§‹CSVæ•°æ®...")
    full_dataset, train_dataset, validation_dataset, column_names, input_signature = prepare_datasets(
        data_config, train_config, TF_DTYPE_MAPPING
    )
    
    print(f"âœ… åŸå§‹æ•°æ®åŠ è½½å®Œæˆ")
    print(f"   åŸå§‹ç‰¹å¾åˆ—: {column_names}")
    
    # 2. åº”ç”¨ç‰¹å¾å¤„ç†
    print("\nğŸ”§ æ­¥éª¤2: åº”ç”¨UniProcessç‰¹å¾å¤„ç†...")
    try:
        # å¤„ç†å®Œæ•´æ•°æ®é›†
        print("   å¤„ç†å®Œæ•´æ•°æ®é›†...")
        processed_full_dataset = apply_feature_preprocessing(
            full_dataset, 
            feat_config_path="config/feat.yml",
            verbose=True
        )
        
        # å¤„ç†è®­ç»ƒæ•°æ®é›†
        print("   å¤„ç†è®­ç»ƒæ•°æ®é›†...")
        processed_train_dataset = apply_feature_preprocessing(
            train_dataset,
            feat_config_path="config/feat.yml", 
            verbose=False  # é¿å…é‡å¤æ—¥å¿—
        )
        
        # å¤„ç†éªŒè¯æ•°æ®é›†
        print("   å¤„ç†éªŒè¯æ•°æ®é›†...")
        processed_validation_dataset = apply_feature_preprocessing(
            validation_dataset,
            feat_config_path="config/feat.yml",
            verbose=False  # é¿å…é‡å¤æ—¥å¿—
        )
        
        print("âœ… ç‰¹å¾å¤„ç†å®Œæˆ")
        
        # 3. éªŒè¯å¤„ç†åçš„æ•°æ®é›†
        print("\nğŸ” æ­¥éª¤3: éªŒè¯å¤„ç†åçš„æ•°æ®é›†...")
        _validate_processed_datasets(
            processed_full_dataset, 
            processed_train_dataset, 
            processed_validation_dataset
        )
        
        return (processed_full_dataset, processed_train_dataset, 
                processed_validation_dataset, column_names, input_signature)
        
    except Exception as e:
        print(f"âŒ ç‰¹å¾å¤„ç†å¤±è´¥: {e}")
        print("ğŸ”„ å›é€€åˆ°åŸå§‹æ•°æ®é›†...")
        return full_dataset, train_dataset, validation_dataset, column_names, input_signature


def _validate_processed_datasets(full_dataset: tf.data.Dataset,
                                train_dataset: tf.data.Dataset, 
                                validation_dataset: tf.data.Dataset) -> None:
    """éªŒè¯å¤„ç†åçš„æ•°æ®é›†
    
    Args:
        full_dataset: å¤„ç†åçš„å®Œæ•´æ•°æ®é›†
        train_dataset: å¤„ç†åçš„è®­ç»ƒæ•°æ®é›†
        validation_dataset: å¤„ç†åçš„éªŒè¯æ•°æ®é›†
    """
    try:
        # æ£€æŸ¥å¤„ç†åçš„ç‰¹å¾åç§°å’Œæ•°æ®ç±»å‹
        for batch_features, batch_labels in full_dataset.take(1):
            print(f"   å¤„ç†åç‰¹å¾æ•°é‡: {len(batch_features)}")
            print(f"   ç‰¹å¾åç§°: {list(batch_features.keys())}")
            
            # æ£€æŸ¥å‰å‡ ä¸ªç‰¹å¾çš„æ•°æ®ç±»å‹å’Œæ ·ä¾‹
            feature_sample = {}
            for i, (name, tensor) in enumerate(batch_features.items()):
                if i < 3:  # åªæ˜¾ç¤ºå‰3ä¸ªç‰¹å¾çš„è¯¦ç»†ä¿¡æ¯
                    feature_sample[name] = {
                        'shape': tensor.shape,
                        'dtype': tensor.dtype,
                        'sample_values': tensor.numpy()[:3] if tensor.shape[0] > 0 else 'empty'
                    }
                    print(f"   ç‰¹å¾ '{name}': shape={tensor.shape}, dtype={tensor.dtype}")
            
            # æ£€æŸ¥æ ‡ç­¾
            print(f"   æ ‡ç­¾shape: {batch_labels.shape}, dtype: {batch_labels.dtype}")
            
            break
            
        print("âœ… æ•°æ®é›†éªŒè¯é€šè¿‡")
        
    except Exception as e:
        print(f"âš ï¸  æ•°æ®é›†éªŒè¯è­¦å‘Š: {e}")


def train_and_evaluate_model(
    model: tf.keras.Model,
    full_dataset: tf.data.Dataset,
    train_dataset: tf.data.Dataset,
    validation_dataset: tf.data.Dataset,
    train_config: Optional[Dict[str, Any]] = None
) -> None:
    """
    è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹
    
    å‚æ•°:
        model: å¾…è®­ç»ƒçš„æ¨¡å‹
        full_dataset: å®Œæ•´æ•°æ®é›†
        train_dataset: è®­ç»ƒæ•°æ®é›†
        validation_dataset: éªŒè¯æ•°æ®é›†
        train_config: è®­ç»ƒé…ç½®
    """
    # 1. æµ‹è¯•æ¨¡å‹
    if not test_model_on_batch(model, full_dataset):
        print("æ¨¡å‹æµ‹è¯•å¤±è´¥ï¼Œè·³è¿‡è®­ç»ƒæ­¥éª¤")
        return
    
    # 2. è®­ç»ƒæ¨¡å‹
    print("\nå¼€å§‹è®­ç»ƒæ¨¡å‹...")
    history = train_model(
        model, full_dataset, train_dataset, validation_dataset, 
        train_config=train_config
    )
    
    # 3. è¾“å‡ºè®­ç»ƒç»“æœ
    print_training_results(history)
    
    # æ‰“å°æ¨¡å‹çš„ç‰¹å¾ç®¡é“ä¿¡æ¯
    print("\næ¨¡å‹ç‰¹å¾ç®¡é“ä¿¡æ¯:")
    if hasattr(model, 'feature_pipelines'):
        print(f"å…±æœ‰ {len(model.feature_pipelines)} ä¸ªç‰¹å¾å¤„ç†ç®¡é“")
        for idx, (feature_name, processors) in enumerate(model.feature_pipelines):
            processor_names = [p.__class__.__name__ for p in processors]
            print(f"ç®¡é“ #{idx+1}: è¾“å…¥ç‰¹å¾ '{feature_name}' -> å¤„ç†å™¨: {' -> '.join(processor_names)}")
            
            # ç‰¹åˆ«æ ‡è®°BERTç›¸å…³ç‰¹å¾ç®¡é“
            for processor in processors:
                if any(bert_class in processor.__class__.__name__ for bert_class in ['BertEmbedding', 'PrecomputedEmbedding']):
                    print(f"  [é«˜çº§ç‰¹å¾] å‘ç°BERT/é¢„è®¡ç®—å¤„ç†å™¨: {processor.__class__.__name__}")
    else:
        print("æ¨¡å‹æ²¡æœ‰å®šä¹‰ç‰¹å¾å¤„ç†ç®¡é“")
    
    # 4. è¯„ä¼°ç‰¹å¾é‡è¦æ€§
    print("\nå¼€å§‹è¯„ä¼°ç‰¹å¾é‡è¦æ€§...")
    feature_importance = check_feature_importance(
        model, validation_dataset, train_config=train_config
    )
    
    # 5. ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾
    plot_feature_importance(feature_importance)


def print_training_results(history: tf.keras.callbacks.History) -> None:
    """
    æ‰“å°è®­ç»ƒç»“æœ
    
    å‚æ•°:
        history: è®­ç»ƒå†å²å¯¹è±¡
    """
    print(f"\næ¨¡å‹è®­ç»ƒå®Œæˆ")
    print(f"è®­ç»ƒé›†æœ€ç»ˆAUC: {history.history['auc'][-1]:.6f}")
    print(f"éªŒè¯é›†æœ€ç»ˆAUC: {history.history['val_auc'][-1]:.6f}")
    
    # è®¡ç®—å¹¶æ‰“å°è¿‡æ‹ŸåˆæŒ‡æ ‡
    train_auc = history.history['auc'][-1]
    val_auc = history.history['val_auc'][-1]
    auc_diff = train_auc - val_auc
    
    if auc_diff > 0.05:
        print(f"è­¦å‘Š: å¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆç°è±¡ (è®­ç»ƒAUC - éªŒè¯AUC = {auc_diff:.4f})")


def setup_environment_for_training() -> None:
    """è®¾ç½®è®­ç»ƒç¯å¢ƒ"""
    setup_gpu()
    setup_environment()
    setup_directories()
    set_random_seeds()
    print("è®­ç»ƒç¯å¢ƒè®¾ç½®å®Œæˆ")


def main() -> None:
    """ä¸»å‡½æ•°ï¼Œæ‰§è¡Œè®­ç»ƒæµç¨‹"""
    # 1. ç¯å¢ƒè®¾ç½®
    setup_environment_for_training()
    
    # 2. å‡†å¤‡æ¨¡å‹å’Œæ•°æ®
    model, full_dataset, train_dataset, validation_dataset = prepare_model_and_data()
    
    # 3. é‡æ–°åŠ è½½è®­ç»ƒé…ç½®ï¼ˆç¡®ä¿ä½¿ç”¨æœ€æ–°é…ç½®ï¼‰
    _, train_config = load_configurations()
    
    # 4. è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹
    train_and_evaluate_model(
        model, full_dataset, train_dataset, validation_dataset, 
        train_config=train_config
    )
    
    print("\nè®­ç»ƒæµç¨‹å®Œæˆ")


if __name__ == "__main__":
    main() 
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ•°æ®å‡†å¤‡å·¥å…·æ¨¡å—ï¼Œæä¾›æ•°æ®é›†æ„å»ºå’Œå‡†å¤‡çš„åŠŸèƒ½
"""

import os
import datetime
import json
import glob
from typing import Dict, List, Tuple, Any, Optional

import tensorflow as tf
from src.data.dataset_utils import build_dataset, split_dataset


def prepare_datasets(data_config: Dict[str, Any], 
                     train_config: Dict[str, Any], 
                     tf_dtype_mapping: Dict[str, Any],
                     filter_column: Optional[str] = None):
    """
    å‡†å¤‡è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†
    
    å‚æ•°:
        data_config: æ•°æ®é…ç½®å­—å…¸
        train_config: è®­ç»ƒé…ç½®å­—å…¸
        tf_dtype_mapping: æ•°æ®ç±»å‹æ˜ å°„å­—å…¸
        filter_column: æŒ‡å®šç”¨äºè¿‡æ»¤çš„åˆ—åï¼ˆåªä¿ç•™è¯¥åˆ—éç©ºçš„æ•°æ®ï¼‰
    
    è¿”å›:
        full_dataset: å®Œæ•´æ•°æ®é›†
        train_dataset: è®­ç»ƒæ•°æ®é›†
        validation_dataset: éªŒè¯æ•°æ®é›†
        column_names: åˆ—ååˆ—è¡¨
        input_signature: è¾“å…¥ç­¾å
    """
    # 1. æŸ¥æ‰¾æ•°æ®æ–‡ä»¶
    csv_files = _find_data_files(data_config)
    
    # 2. è§£æåˆ—ä¿¡æ¯
    column_names, column_defaults, column_types = _parse_column_config(data_config)
    
    # 3. è·å–æ ‡ç­¾åˆ—
    label_columns = data_config.get('label_columns', ['log_type'])
    print(f"æ•°æ®é›†åˆ—æ•°: {len(column_names)}")
    print(f"æ ‡ç­¾åˆ—: {label_columns}")
    
    # 4. æ˜¾ç¤ºè¿‡æ»¤ä¿¡æ¯
    if filter_column:
        print(f"ğŸ” å°†å¯¹æ•°æ®è¿›è¡Œè¿‡æ»¤ï¼Œåªä¿ç•™ '{filter_column}' åˆ—éç©ºçš„æ•°æ®")
    else:
        print("ğŸ“Š ä½¿ç”¨å…¨éƒ¨æ•°æ®è¿›è¡Œè®­ç»ƒ")
    
    # 5. æ„å»ºåŸå§‹æ•°æ®é›†ï¼ˆæ·»åŠ è¿‡æ»¤å‚æ•°ï¼‰
    dataset_with_userid, unique_user_ids, total_samples = build_dataset(
        _get_file_pattern(data_config), column_names, column_defaults, 
        data_config=data_config, filter_column=filter_column
    )
    
    # 6. åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_dataset, validation_dataset, val_users, train_users = _split_train_val_dataset(
        dataset_with_userid, unique_user_ids, train_config
    )
    
    # 7. é…ç½®æ‰¹å¤„ç†å’Œé¢„å–
    train_dataset, validation_dataset, full_dataset = _configure_datasets(
        dataset_with_userid, train_dataset, validation_dataset, train_config
    )
    
    # 8. è®°å½•ç”¨æˆ·åˆ’åˆ†ä¿¡æ¯
    log_user_split(train_users, val_users, unique_user_ids, total_samples, filter_column)
    
    # 9. ç¡®å®šè¾“å…¥ç­¾å
    input_signature = _determine_input_signature(full_dataset)
    
    return full_dataset, train_dataset, validation_dataset, column_names, input_signature


def _find_data_files(data_config: Dict[str, Any]) -> List[str]:
    """
    æŸ¥æ‰¾æ•°æ®æ–‡ä»¶
    
    å‚æ•°:
        data_config: æ•°æ®é…ç½®å­—å…¸
    
    è¿”å›:
        data_files: æ•°æ®æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    file_pattern = _get_file_pattern(data_config)
    
    # æŸ¥æ‰¾æ•°æ®æ–‡ä»¶
    data_files = glob.glob(file_pattern)
    
    # ç¡®å®šæ–‡ä»¶ç±»å‹ç”¨äºæ˜¾ç¤º
    file_type = "TXT" if _is_txt_format(data_config) else "CSV"
    
    print(f"æ‰¾åˆ°{file_type}æ–‡ä»¶: {len(data_files)}ä¸ª")
    print(f"ç¤ºä¾‹æ–‡ä»¶: {data_files[:5] if data_files else 'æ— '}")
    
    if not data_files:
        # çº¿ä¸Šç¯å¢ƒä¸åº”è¯¥åˆ›å»ºæµ‹è¯•æ•°æ®ï¼Œç›´æ¥æŠ›å‡ºé”™è¯¯
        raise FileNotFoundError(f"åœ¨ {data_config.get('train_dir', 'data/train')} ç›®å½•ä¸‹æœªæ‰¾åˆ°ä»»ä½•{file_type}æ–‡ä»¶ã€‚è¯·æ£€æŸ¥æ•°æ®è·¯å¾„å’Œæ–‡ä»¶æ ¼å¼é…ç½®ã€‚")
    
    return data_files


def _get_file_pattern(data_config: Dict[str, Any]) -> str:
    """
    è·å–æ–‡ä»¶åŒ¹é…æ¨¡å¼
    
    å‚æ•°:
        data_config: æ•°æ®é…ç½®å­—å…¸
    
    è¿”å›:
        file_pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼
    """
    train_dir = data_config.get('train_dir', 'data/train')
    
    # æ ¹æ®é…ç½®ç¡®å®šæ–‡ä»¶æ‰©å±•å
    if _is_txt_format(data_config):
        file_extension = "*.txt"
    else:
        file_extension = "*.csv"
    
    return os.path.join(train_dir, file_extension)


def _is_txt_format(data_config: Dict[str, Any]) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦ä½¿ç”¨TXTæ ¼å¼
    
    å‚æ•°:
        data_config: æ•°æ®é…ç½®å­—å…¸
    
    è¿”å›:
        bool: Trueè¡¨ç¤ºä½¿ç”¨TXTæ ¼å¼ï¼ŒFalseè¡¨ç¤ºä½¿ç”¨CSVæ ¼å¼
    """
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    env_format = os.getenv('DATA_FORMAT', '').lower()
    if env_format == 'txt':
        return True
    elif env_format == 'csv':
        return False
    
    # å¦‚æœæ²¡æœ‰ç¯å¢ƒå˜é‡ï¼Œæ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­æ˜¯å¦æœ‰txt_formatä¸”æœªè¢«æ³¨é‡Š
    if data_config and 'txt_format' in data_config:
        return True
    
    return False


def _parse_column_config(data_config: Dict[str, Any]) -> Tuple[List[str], List[Any], Dict[str, str]]:
    """
    è§£æåˆ—é…ç½®ä¿¡æ¯
    
    å‚æ•°:
        data_config: æ•°æ®é…ç½®å­—å…¸
    
    è¿”å›:
        column_names: åˆ—ååˆ—è¡¨
        column_defaults: åˆ—é»˜è®¤å€¼åˆ—è¡¨
        column_types: åˆ—ç±»å‹å­—å…¸
    """
    raw_columns = data_config.get('raw_data_columns', [])
    column_names = []
    column_defaults = []
    column_types = {}
    
    # è§£æåˆ—é…ç½®
    for col_item in raw_columns:
        for col_name, col_type in col_item.items():
            column_names.append(col_name)
            column_types[col_name] = col_type
            
            # è®¾ç½®é»˜è®¤å€¼
            if col_type == 'string':
                column_defaults.append('')
            else:
                column_defaults.append(0)
    
    return column_names, column_defaults, column_types


def _split_train_val_dataset(dataset_with_userid, 
                             unique_user_ids, 
                             train_config: Dict[str, Any]) -> Tuple[tf.data.Dataset, tf.data.Dataset, List[Any], List[Any]]:
    """
    æŒ‰ç”¨æˆ·IDåˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    
    å‚æ•°:
        dataset_with_userid: å¸¦ç”¨æˆ·IDçš„æ•°æ®é›†
        unique_user_ids: å”¯ä¸€ç”¨æˆ·IDåˆ—è¡¨
        train_config: è®­ç»ƒé…ç½®å­—å…¸
    
    è¿”å›:
        train_dataset: è®­ç»ƒæ•°æ®é›†
        validation_dataset: éªŒè¯æ•°æ®é›†
        val_users: éªŒè¯é›†ç”¨æˆ·IDåˆ—è¡¨
        train_users: è®­ç»ƒé›†ç”¨æˆ·IDåˆ—è¡¨
    """
    # è·å–éªŒè¯é›†æ¯”ä¾‹
    val_ratio = train_config['training'].get('validation_split', 0.2) if train_config else 0.2
    
    # æŒ‰ç”¨æˆ·åˆ’åˆ†æ•°æ®é›†
    train_dataset, validation_dataset, val_users, train_users = split_dataset(
        dataset_with_userid, unique_user_ids, val_ratio=val_ratio
    )
    
    return train_dataset, validation_dataset, val_users, train_users


def _configure_datasets(dataset_with_userid, 
                        train_dataset, 
                        validation_dataset, 
                        train_config: Dict[str, Any]) -> Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset]:
    """
    é…ç½®æ•°æ®é›†çš„æ‰¹å¤„ç†ã€é¢„å–ç­‰å‚æ•°
    
    å‚æ•°:
        dataset_with_userid: å¸¦ç”¨æˆ·IDçš„å®Œæ•´æ•°æ®é›†
        train_dataset: è®­ç»ƒæ•°æ®é›†
        validation_dataset: éªŒè¯æ•°æ®é›†
        train_config: è®­ç»ƒé…ç½®å­—å…¸
    
    è¿”å›:
        train_dataset: é…ç½®åçš„è®­ç»ƒæ•°æ®é›†
        validation_dataset: é…ç½®åçš„éªŒè¯æ•°æ®é›†
        full_dataset: é…ç½®åçš„å®Œæ•´æ•°æ®é›†
    """
    # ä»è®­ç»ƒé…ç½®è·å–æ‰¹å¤„ç†å¤§å°å’Œshuffle bufferå¤§å°
    batch_size = train_config['training'].get('batch_size', 256) if train_config else 256
    shuffle_buffer_size = (
        train_config['training'].get('shuffle_buffer_size', 20000) 
        if train_config else 20000
    )
    
    print(f"æ‰¹å¤„ç†å¤§å°: {batch_size}, Shuffleç¼“å†²åŒºå¤§å°: {shuffle_buffer_size}")
    
    # è®¾ç½®è®­ç»ƒé›†æ‰¹å¤„ç†å’Œshuffle
    configured_train_dataset = (
        train_dataset
        .shuffle(buffer_size=shuffle_buffer_size)
        .batch(batch_size)
        .prefetch(tf.data.AUTOTUNE)
    )
    
    # è®¾ç½®éªŒè¯é›†æ‰¹å¤„ç†
    configured_validation_dataset = (
        validation_dataset
        .batch(batch_size)
        .prefetch(tf.data.AUTOTUNE)
    )
    
    # åˆ›å»ºä¸€ä¸ªå®Œæ•´çš„æ•°æ®é›†ç”¨äºæ•´ä½“åˆ†æ
    full_dataset = (
        dataset_with_userid
        .batch(batch_size)
        .prefetch(tf.data.AUTOTUNE)
    )
    
    return configured_train_dataset, configured_validation_dataset, full_dataset


def _determine_input_signature(dataset: tf.data.Dataset) -> Optional[Dict[str, tf.TensorShape]]:
    """
    ç¡®å®šæ•°æ®é›†çš„è¾“å…¥ç­¾å
    
    å‚æ•°:
        dataset: æ•°æ®é›†
    
    è¿”å›:
        input_signature: è¾“å…¥ç­¾åå­—å…¸
    """
    input_signature = None
    for batch in dataset.take(1):
        features, _ = batch
        input_signature = {k: v.shape for k, v in features.items()}
    return input_signature


def log_user_split(train_users, val_users, unique_user_ids, total_samples, filter_column):
    """
    è®°å½•ç”¨æˆ·åˆ’åˆ†ä¿¡æ¯åˆ°æ—¥å¿—æ–‡ä»¶
    
    å‚æ•°:
        train_users: è®­ç»ƒé›†ç”¨æˆ·IDåˆ—è¡¨
        val_users: éªŒè¯é›†ç”¨æˆ·IDåˆ—è¡¨
        unique_user_ids: æ‰€æœ‰å”¯ä¸€ç”¨æˆ·IDåˆ—è¡¨
        total_samples: æ€»æ ·æœ¬æ•°
        filter_column: è¿‡æ»¤çš„åˆ—å
    """
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    os.makedirs("./logs", exist_ok=True)
    
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    user_split_log = os.path.join("./logs", f"user_split_{timestamp}.json")
    
    # è®¡ç®—æ¯”ä¾‹
    train_ratio = 1.0 - (len(val_users) / len(unique_user_ids))
    val_ratio = len(val_users) / len(unique_user_ids)
    
    # ä¿å­˜åˆ’åˆ†ä¿¡æ¯çš„æ‘˜è¦ï¼Œé¿å…æ–‡ä»¶è¿‡å¤§
    split_info = {
        "train_users_count": len(train_users),
        "validation_users_count": len(val_users),
        "total_users_count": len(unique_user_ids),
        "total_samples": total_samples,
        "train_ratio": train_ratio,
        "val_ratio": val_ratio,
        "filter_column": filter_column
    }
    
    with open(user_split_log, 'w') as f:
        json.dump(split_info, f, indent=2)
    
    print(f"ç”¨æˆ·åˆ’åˆ†ä¿¡æ¯å·²ä¿å­˜åˆ°: {user_split_log}") 
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Dataset utilities for push binary classification
"""

import os
import glob
import json
import datetime
import pandas as pd
import numpy as np
import tensorflow as tf
from typing import Dict, List, Tuple, Set, Any, Optional


def build_dataset(
    file_pattern: str, 
    column_names: List[str], 
    column_defaults: List[Any], 
    batch_size: int = 256, 
    data_config: Optional[Dict] = None,
    filter_column: Optional[str] = None
) -> Tuple[tf.data.Dataset, np.ndarray, int]:
    """
    æ„å»ºæ•°æ®é›†
    
    Args:
        file_pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼
        column_names: æ•°æ®åˆ—å
        column_defaults: æ¯åˆ—çš„é»˜è®¤æ•°æ®ç±»å‹
        batch_size: æ‰¹å¤„ç†å¤§å°
        data_config: æ•°æ®é…ç½®å­—å…¸ï¼ŒåŒ…å«CSVæ ¼å¼è®¾ç½®
        filter_column: æŒ‡å®šç”¨äºè¿‡æ»¤çš„åˆ—åï¼ˆåªä¿ç•™è¯¥åˆ—éç©ºçš„æ•°æ®ï¼‰
        
    Returns:
        dataset: TensorFlow æ•°æ®é›†
        unique_user_ids: å”¯ä¸€ç”¨æˆ·IDæ•°ç»„
        total_samples: æ ·æœ¬æ€»æ•°
    """
    # æŸ¥æ‰¾ç¬¦åˆæ¨¡å¼çš„æ–‡ä»¶
    files = _find_csv_files(file_pattern)
    
    # ä»é…ç½®ä¸­è·å–CSVæ ¼å¼è®¾ç½®
    csv_sep, csv_header, use_names = _get_csv_format_settings(data_config, column_names)
    
    # è¯»å–å¹¶åˆå¹¶CSVæ–‡ä»¶
    combined_df = _read_and_combine_data_files(files, csv_sep, csv_header, use_names)
    
    # åº”ç”¨æ•°æ®è¿‡æ»¤ï¼ˆå¦‚æœæŒ‡å®šäº†è¿‡æ»¤åˆ—ï¼‰
    if filter_column:
        combined_df = _filter_data_by_column(combined_df, filter_column)
    
    # æ˜¾ç¤ºæ•°æ®ä¿¡æ¯
    _display_dataframe_info(combined_df)
    
    # å¤„ç†æ ‡ç­¾åˆ—
    numeric_labels = _process_labels(combined_df)
    
    # ä¿å­˜user_idåˆ—ä»¥ä¾¿åç»­æŒ‰ç”¨æˆ·åˆ’åˆ†è®­ç»ƒå’Œæµ‹è¯•é›†
    user_ids = combined_df['user_id'].values
    
    # å°†æ•°æ®è½¬æ¢ä¸ºTensorFlowæ•°æ®é›†
    dataset = _convert_to_tf_dataset(combined_df, numeric_labels)
    
    # è¿”å›æ•°æ®é›†ã€å”¯ä¸€ç”¨æˆ·IDå’Œæ€»æ ·æœ¬æ•°
    unique_user_ids = np.unique(user_ids)
    total_samples = len(combined_df)
    
    return dataset, unique_user_ids, total_samples


def _find_csv_files(file_pattern: str) -> List[str]:
    """æŸ¥æ‰¾ç¬¦åˆæ¨¡å¼çš„æ•°æ®æ–‡ä»¶"""
    files = glob.glob(file_pattern)
    # æ ¹æ®æ–‡ä»¶æ‰©å±•åç¡®å®šæ–‡ä»¶ç±»å‹
    file_type = "TXT" if ".txt" in file_pattern else "CSV"
    print(f"æ‰¾åˆ°{file_type}æ–‡ä»¶: {len(files)}ä¸ª")
    if len(files) > 3:
        print(f"ç¤ºä¾‹æ–‡ä»¶: {files[:5]}")
    else:
        print(f"ç¤ºä¾‹æ–‡ä»¶: {files}")
    
    if not files:
        raise ValueError(f"åœ¨ {file_pattern} æ‰¾ä¸åˆ°ä»»ä½•{file_type}æ–‡ä»¶")
    
    return files


def _get_csv_format_settings(
    data_config: Optional[Dict], 
    column_names: List[str]
) -> Tuple[str, Optional[int], Optional[List[str]]]:
    """ä»é…ç½®ä¸­è·å–æ•°æ®æ ¼å¼è®¾ç½®ï¼ˆæ”¯æŒcsv_formatå’Œtxt_formatï¼‰"""
    csv_sep = ','  # é»˜è®¤é€—å·åˆ†éš”
    csv_header = 0  # é»˜è®¤æœ‰è¡¨å¤´
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡æ¥å†³å®šä½¿ç”¨å“ªç§æ ¼å¼
    env_format = os.getenv('DATA_FORMAT', '').lower()  # 'txt' æˆ– 'csv'
    
    # å¦‚æœè®¾ç½®äº†ç¯å¢ƒå˜é‡ï¼Œä¼˜å…ˆä½¿ç”¨æŒ‡å®šæ ¼å¼
    if env_format == 'txt' and data_config and 'txt_format' in data_config:
        txt_format = data_config['txt_format']
        csv_sep = txt_format.get('separator', '\t')
        csv_header = txt_format.get('header', None)
        print(f"ğŸŒ ç¯å¢ƒå˜é‡æŒ‡å®šä½¿ç”¨TXTæ ¼å¼: åˆ†éš”ç¬¦='{csv_sep}', è¡¨å¤´={csv_header}")
    elif env_format == 'csv' and data_config and 'csv_format' in data_config:
        csv_format = data_config['csv_format']
        csv_sep = csv_format.get('separator', ',')
        csv_header = csv_format.get('header', 0)
        print(f"ğŸŒ ç¯å¢ƒå˜é‡æŒ‡å®šä½¿ç”¨CSVæ ¼å¼: åˆ†éš”ç¬¦='{csv_sep}', è¡¨å¤´={csv_header}")
    # å¦‚æœæ²¡æœ‰è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œä½¿ç”¨åŸæ¥çš„ä¼˜å…ˆçº§é€»è¾‘
    elif data_config and 'txt_format' in data_config:
        txt_format = data_config['txt_format']
        csv_sep = txt_format.get('separator', '\t')
        csv_header = txt_format.get('header', None)
        print(f"ä½¿ç”¨çº¿ä¸ŠTXTæ ¼å¼: åˆ†éš”ç¬¦='{csv_sep}', è¡¨å¤´={csv_header}")
    elif data_config and 'csv_format' in data_config:
        csv_format = data_config['csv_format']
        csv_sep = csv_format.get('separator', ',')
        csv_header = csv_format.get('header', 0)
        print(f"ä½¿ç”¨æœ¬åœ°CSVæ ¼å¼: åˆ†éš”ç¬¦='{csv_sep}', è¡¨å¤´={csv_header}")
    else:
        print(f"ä½¿ç”¨é»˜è®¤æ ¼å¼: åˆ†éš”ç¬¦='{csv_sep}', è¡¨å¤´={csv_header}")
    
    # å†³å®šæ˜¯å¦éœ€è¦æä¾›åˆ—å
    use_names = None
    if csv_header is None:  # å¦‚æœæ— è¡¨å¤´ï¼Œåˆ™ä½¿ç”¨æä¾›çš„åˆ—å
        use_names = column_names
    
    return csv_sep, csv_header, use_names


def _read_and_combine_data_files(
    files: List[str], 
    csv_sep: str, 
    csv_header: Optional[int], 
    use_names: Optional[List[str]]
) -> pd.DataFrame:
    """è¯»å–å¹¶åˆå¹¶æ•°æ®æ–‡ä»¶ï¼ˆæ”¯æŒCSVå’ŒTXTæ ¼å¼ï¼‰"""
    # æ ¹æ®æ–‡ä»¶æ‰©å±•åç¡®å®šæ–‡ä»¶ç±»å‹
    file_type = "TXT" if any(".txt" in f for f in files) else "CSV"
    print(f"è¯»å–{file_type}æ–‡ä»¶...")
    dfs = []
    
    # æ˜¾å¼æŒ‡å®šæ¯åˆ—çš„ç±»å‹ï¼Œç¡®ä¿ä¸€è‡´æ€§
    dtypes = {
        'user_id': str,
        'create_time': str,
        'log_type': str,
        'watchlists': str, 
        'holdings': str,
        'country': str,
        'prefer_bid': str,
        'user_propernoun': str,
        'push_title': str,
        'push_content': str,
        'item_code': str,
        'item_tags': str,
        'submit_type': str
    }
    
    for file in files:
        print(f"è¯»å–æ–‡ä»¶: {file}")
        try:
            df = pd.read_csv(
                file, 
                sep=csv_sep, 
                header=csv_header, 
                names=use_names,
                escapechar='\\', 
                quotechar='"', 
                dtype=dtypes
            )
            print(f"  è¡Œæ•°: {len(df)}")
            dfs.append(df)
        except Exception as e:
            print(f"  è¯»å–é”™è¯¯: {e}")
    
    if not dfs:
        raise ValueError(f"æ²¡æœ‰æˆåŠŸè¯»å–ä»»ä½•{file_type}æ–‡ä»¶")
        
    # åˆå¹¶æ‰€æœ‰æ•°æ®æ¡†
    combined_df = pd.concat(dfs)
    print(f"åˆå¹¶åæ€»è¡Œæ•°: {len(combined_df)}")
    
    return combined_df


def _display_dataframe_info(df: pd.DataFrame) -> None:
    """æ˜¾ç¤ºæ•°æ®æ¡†çš„åŸºæœ¬ä¿¡æ¯"""
    print("æ•°æ®åˆ—å’Œç±»å‹:")
    print(df.dtypes)
    
    # æ£€æŸ¥æ ‡ç­¾åˆ—çš„å€¼åˆ†å¸ƒ
    print("æ ‡ç­¾å€¼åˆ†å¸ƒ:")
    print(df['log_type'].value_counts())


def _process_labels(df: pd.DataFrame) -> pd.Series:
    """å¤„ç†æ ‡ç­¾åˆ—"""
    # æå–æ ‡ç­¾åˆ—
    labels = df.pop('log_type')
    
    # å°†æ ‡ç­¾è½¬æ¢ä¸º0/1 (PR:0, PC:1)
    label_dict = {'PR': 0, 'PC': 1}
    numeric_labels = labels.map(lambda x: label_dict.get(x, -1))
    print("æ ‡ç­¾æ•°å€¼åˆ†å¸ƒ:")
    print(numeric_labels.value_counts())
    
    return numeric_labels


def _convert_to_tf_dataset(df: pd.DataFrame, labels: pd.Series) -> tf.data.Dataset:
    """å°†DataFrameå’Œæ ‡ç­¾è½¬æ¢ä¸ºTensorFlowæ•°æ®é›†"""
    features_dict = {}
    
    # å¤„ç†ç‰¹å¾ï¼Œä¿æŒåŸå§‹æ•°æ®ç±»å‹ï¼Œå¤„ç†ç©ºå€¼
    for col in df.columns:
        # å°†NaNå€¼æ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²
        df[col] = df[col].fillna('')
        features_dict[col] = tf.constant(df[col].values, dtype=tf.string)
    
    # åˆ›å»ºæ•°æ®é›†ï¼ŒåŒ…å«ç‰¹å¾å’Œæ ‡ç­¾
    labels_tensor = tf.constant(labels.values, dtype=tf.int32)
    
    # åˆ›å»ºåŒ…å«æ‰€æœ‰ç‰¹å¾çš„æ•°æ®é›†
    return tf.data.Dataset.from_tensor_slices((features_dict, labels_tensor))


def split_dataset(
    dataset: tf.data.Dataset, 
    unique_user_ids: np.ndarray, 
    val_ratio: float = 0.2
) -> Tuple[tf.data.Dataset, tf.data.Dataset, List[str], List[str]]:
    """
    æŒ‰ç”¨æˆ·åˆ’åˆ†æ•°æ®é›†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†
    
    Args:
        dataset: åŒ…å«ç”¨æˆ·IDçš„æ•°æ®é›†
        unique_user_ids: å”¯ä¸€ç”¨æˆ·IDæ•°ç»„
        val_ratio: éªŒè¯é›†æ¯”ä¾‹
        
    Returns:
        train_dataset: è®­ç»ƒæ•°æ®é›†
        validation_dataset: éªŒè¯æ•°æ®é›†
        val_users: éªŒè¯é›†ç”¨æˆ·IDåˆ—è¡¨
        train_users: è®­ç»ƒé›†ç”¨æˆ·IDåˆ—è¡¨
    """
    print(f"æŒ‰ç”¨æˆ·æ‹†åˆ†æ•°æ®é›†ï¼ŒéªŒè¯é›†æ¯”ä¾‹: {val_ratio}")
    print(f"å”¯ä¸€ç”¨æˆ·æ•°: {len(unique_user_ids)}")
    
    # è·å–è®­ç»ƒå’ŒéªŒè¯ç”¨æˆ·åˆ—è¡¨
    train_users, val_users = _split_users(unique_user_ids, val_ratio)
    
    # å°†ç”¨æˆ·IDè½¬æ¢ä¸ºé›†åˆä»¥ä¾¿å¿«é€ŸæŸ¥æ‰¾
    val_users_set = set(val_users)
    
    # æ ¹æ®ç”¨æˆ·IDåˆ’åˆ†æ•°æ®é›†
    train_dataset, validation_dataset = _filter_datasets_by_users(dataset, val_users_set)
    
    # éªŒè¯æ•°æ®é›†å¤§å°
    _validate_split_sizes(train_dataset, validation_dataset)
    
    # ä¿å­˜ç”¨æˆ·åˆ’åˆ†ä¿¡æ¯åˆ°æ—¥å¿—
    _save_user_split_info(train_users, val_users)
    
    return train_dataset, validation_dataset, val_users.tolist(), train_users.tolist()


def _split_users(user_ids: np.ndarray, val_ratio: float) -> Tuple[np.ndarray, np.ndarray]:
    """å°†ç”¨æˆ·IDæ‹†åˆ†ä¸ºè®­ç»ƒå’ŒéªŒè¯é›†"""
    # éšæœºæ‰“ä¹±ç”¨æˆ·ID
    np.random.shuffle(user_ids)
    
    # æŒ‰æ¯”ä¾‹åˆ’åˆ†ç”¨æˆ·ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†
    split_idx = int(len(user_ids) * (1 - val_ratio))
    train_users = user_ids[:split_idx]
    val_users = user_ids[split_idx:]
    
    print(f"è®­ç»ƒé›†ç”¨æˆ·æ•°: {len(train_users)}")
    print(f"éªŒè¯é›†ç”¨æˆ·æ•°: {len(val_users)}")
    
    return train_users, val_users


def _filter_datasets_by_users(
    dataset: tf.data.Dataset, 
    val_users_set: Set[str]
) -> Tuple[tf.data.Dataset, tf.data.Dataset]:
    """æ ¹æ®ç”¨æˆ·IDè¿‡æ»¤æ•°æ®é›†"""
    # å‡½æ•°ï¼šç¡®å®šæŸæ¡è®°å½•å±äºè®­ç»ƒé›†è¿˜æ˜¯éªŒè¯é›†
    def is_validation_user(features, label):
        return tf.py_function(
            lambda x: tf.constant(x.numpy().decode('utf-8') in val_users_set), 
            [features["user_id"]], 
            tf.bool
        )
    
    # ä½¿ç”¨filterå‡½æ•°æŒ‰ç”¨æˆ·IDåˆ’åˆ†æ•°æ®é›†
    validation_dataset = dataset.filter(is_validation_user)
    train_dataset = dataset.filter(
        lambda features, label: tf.logical_not(is_validation_user(features, label))
    )
    
    return train_dataset, validation_dataset


def _validate_split_sizes(
    train_dataset: tf.data.Dataset, 
    validation_dataset: tf.data.Dataset
) -> None:
    """éªŒè¯æ‹†åˆ†åçš„æ•°æ®é›†å¤§å°"""
    try:
        # è·å–å¤§è‡´çš„æ‰¹æ¬¡æ•°é‡
        train_count = sum(1 for _ in train_dataset.take(100))
        val_count = sum(1 for _ in validation_dataset.take(100))
        
        print(f"è®­ç»ƒé›†æ ·æœ¬æ•°(ä¼°è®¡): è‡³å°‘ {train_count} æ¡")
        print(f"éªŒè¯é›†æ ·æœ¬æ•°(ä¼°è®¡): è‡³å°‘ {val_count} æ¡")
    except Exception as e:
        print(f"è®¡ç®—æ•°æ®é›†å¤§å°æ—¶å‡ºé”™: {e}")


def _save_user_split_info(train_users: np.ndarray, val_users: np.ndarray) -> None:
    """ä¿å­˜ç”¨æˆ·åˆ’åˆ†ä¿¡æ¯åˆ°æ—¥å¿—æ–‡ä»¶"""
    try:
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        os.makedirs("./logs", exist_ok=True)
        
        # ä¿å­˜ç”¨æˆ·åˆ’åˆ†ä¿¡æ¯
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        split_info = {
            "train_users_count": len(train_users),
            "val_users_count": len(val_users),
            "timestamp": timestamp
        }
        
        # å†™å…¥JSONæ–‡ä»¶
        split_file = f"./logs/user_split_{timestamp}.json"
        with open(split_file, 'w') as f:
            json.dump(split_info, f, indent=2)
        
        print(f"ç”¨æˆ·åˆ’åˆ†ä¿¡æ¯å·²ä¿å­˜åˆ°: {split_file}")
    except Exception as e:
        print(f"ä¿å­˜ç”¨æˆ·åˆ’åˆ†ä¿¡æ¯æ—¶å‡ºé”™: {e}")


def validate_dataset(dataset: tf.data.Dataset) -> Dict[str, tf.TensorSpec]:
    """
    éªŒè¯æ•°æ®é›†å¹¶è¿”å›è¾“å…¥ç­¾å
    
    Args:
        dataset: è¦éªŒè¯çš„æ•°æ®é›†
        
    Returns:
        input_signature: è¾“å…¥ç­¾åå­—å…¸
    """
    print("éªŒè¯æ•°æ®é›†...")
    
    # è·å–ç¬¬ä¸€ä¸ªæ‰¹æ¬¡
    for features_batch, labels_batch in dataset.take(1):
        print(f"ç‰¹å¾æ‰¹æ¬¡ç±»å‹: {type(features_batch)}")
        print(f"æ ‡ç­¾æ‰¹æ¬¡ç±»å‹: {type(labels_batch)}")
        print(f"æ ‡ç­¾æ‰¹æ¬¡å½¢çŠ¶: {labels_batch.shape}")
        
        # æ£€æŸ¥ç‰¹å¾æ•°æ®çš„ç»“æ„
        print("\nç‰¹å¾æ•°æ®ç»“æ„:")
        for key, tensor in features_batch.items():
            print(f"  {key}: ç±»å‹={tensor.dtype}, å½¢çŠ¶={tensor.shape}")
        
        # åˆ›å»ºè¾“å…¥ç­¾åå­—å…¸
        input_signature = {}
        for key, tensor in features_batch.items():
            input_signature[key] = tf.TensorSpec(shape=[None], dtype=tensor.dtype, name=key)
        
        return input_signature
    
    raise ValueError("æ•°æ®é›†ä¸ºç©ºï¼Œæ— æ³•éªŒè¯")


def inspect_datasets(
    dataset: tf.data.Dataset, 
    train_dataset: tf.data.Dataset, 
    validation_dataset: tf.data.Dataset
) -> None:
    """
    æ£€æŸ¥æ•°æ®é›†çš„æ ·æœ¬å’Œåˆ†å¸ƒæƒ…å†µ
    
    Args:
        dataset: å®Œæ•´æ•°æ®é›†
        train_dataset: è®­ç»ƒæ•°æ®é›†
        validation_dataset: éªŒè¯æ•°æ®é›†
    """
    print("\n=== æ•°æ®é›†æ ·æœ¬æ£€æŸ¥ ===")
    
    # æ£€æŸ¥ä¸€ä¸ªæ‰¹æ¬¡çš„ç‰¹å¾å’Œæ ‡ç­¾
    _inspect_batch_features_and_labels(dataset)
    
    # æ£€æŸ¥è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ‰¹æ¬¡å¤§å°
    _inspect_batch_sizes(train_dataset, validation_dataset)
    
    print("\næ•°æ®é›†éªŒè¯å®Œæˆ")


def _inspect_batch_features_and_labels(dataset: tf.data.Dataset) -> None:
    """æ£€æŸ¥æ•°æ®æ‰¹æ¬¡çš„ç‰¹å¾å’Œæ ‡ç­¾"""
    for features, labels in dataset.take(1):
        # æ£€æŸ¥ç‰¹å¾
        _inspect_features(features)
        
        # æ£€æŸ¥æ ‡ç­¾
        _inspect_labels(labels)
        
        # åªå¤„ç†ç¬¬ä¸€ä¸ªæ‰¹æ¬¡
        break


def _inspect_features(features: Dict[str, tf.Tensor]) -> None:
    """æ£€æŸ¥å¹¶æ‰“å°ç‰¹å¾ä¿¡æ¯"""
    print("æ ·æœ¬ç‰¹å¾:")
    for name, values in features.items():
        print(f"  {name}: å½¢çŠ¶={values.shape}, ç±»å‹={values.dtype}")
        
        # å°è¯•æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬çš„å€¼
        if values.shape[0] > 0:
            _print_sample_value(name, values)


def _print_sample_value(name: str, values: tf.Tensor) -> None:
    """æ‰“å°ç‰¹å¾çš„æ ·æœ¬å€¼"""
    try:
        if values.dtype == tf.string:
            sample_value = values[0].numpy().decode('utf-8')
            # å¦‚æœæ ·æœ¬å€¼è¿‡é•¿ï¼Œæˆªæ–­æ˜¾ç¤º
            if len(sample_value) > 100:
                sample_value = sample_value[:100] + "..."
            print(f"    æ ·æœ¬å€¼: {sample_value}")
        else:
            print(f"    æ ·æœ¬å€¼: {values[0].numpy()}")
    except Exception:
        pass  # å¿½ç•¥æ— æ³•æ˜¾ç¤ºçš„å€¼


def _inspect_labels(labels: tf.Tensor) -> None:
    """æ£€æŸ¥å¹¶æ‰“å°æ ‡ç­¾ä¿¡æ¯"""
    print(f"æ ‡ç­¾å½¢çŠ¶: {labels.shape}")
    if labels.shape[0] > 0:
        print(f"  æ ·æœ¬æ ‡ç­¾: {labels[0].numpy()}")


def _inspect_batch_sizes(
    train_dataset: tf.data.Dataset, 
    validation_dataset: tf.data.Dataset
) -> None:
    """æ£€æŸ¥è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ‰¹æ¬¡å¤§å°"""
    print("\nè®­ç»ƒé›†å’ŒéªŒè¯é›†æ‰¹æ¬¡å¤§å°:")
    
    # æ£€æŸ¥è®­ç»ƒé›†æ‰¹æ¬¡å¤§å°
    for features, _ in train_dataset.take(1):
        batch_size = next(iter(features.values())).shape[0]
        print(f"è®­ç»ƒæ•°æ®é›†æ‰¹æ¬¡å¤§å°: {batch_size}")
        break
    
    # æ£€æŸ¥éªŒè¯é›†æ‰¹æ¬¡å¤§å°
    for features, _ in validation_dataset.take(1):
        batch_size = next(iter(features.values())).shape[0]
        print(f"éªŒè¯æ•°æ®é›†æ‰¹æ¬¡å¤§å°: {batch_size}")
        break 


def _filter_data_by_column(df: pd.DataFrame, filter_column: str) -> pd.DataFrame:
    """
    æ ¹æ®æŒ‡å®šåˆ—è¿‡æ»¤æ•°æ®ï¼Œåªä¿ç•™è¯¥åˆ—éç©ºçš„è®°å½•
    
    Args:
        df: åŸå§‹æ•°æ®æ¡†
        filter_column: ç”¨äºè¿‡æ»¤çš„åˆ—å
        
    Returns:
        filtered_df: è¿‡æ»¤åçš„æ•°æ®æ¡†
    """
    print(f"\nğŸ” å¼€å§‹æ ¹æ® '{filter_column}' åˆ—è¿‡æ»¤æ•°æ®...")
    
    # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
    if filter_column not in df.columns:
        print(f"âš ï¸  è­¦å‘Š: åˆ— '{filter_column}' ä¸å­˜åœ¨äºæ•°æ®ä¸­ï¼Œè·³è¿‡è¿‡æ»¤")
        return df
    
    # æ˜¾ç¤ºè¿‡æ»¤å‰çš„æ•°æ®ç»Ÿè®¡
    total_before = len(df)
    null_count = df[filter_column].isna().sum() + (df[filter_column] == '').sum()
    non_null_count = total_before - null_count
    
    print(f"è¿‡æ»¤å‰æ•°æ®ç»Ÿè®¡:")
    print(f"  æ€»æ ·æœ¬æ•°: {total_before:,}")
    print(f"  {filter_column} ä¸ºç©ºçš„æ ·æœ¬: {null_count:,} ({null_count/total_before*100:.1f}%)")
    print(f"  {filter_column} éç©ºçš„æ ·æœ¬: {non_null_count:,} ({non_null_count/total_before*100:.1f}%)")
    
    # æ‰§è¡Œè¿‡æ»¤ï¼šä¿ç•™éç©ºä¸”éç©ºå­—ç¬¦ä¸²çš„è®°å½•
    filtered_df = df[(df[filter_column].notna()) & (df[filter_column] != '')]
    
    # æ˜¾ç¤ºè¿‡æ»¤åçš„æ•°æ®ç»Ÿè®¡
    total_after = len(filtered_df)
    reduction_rate = (total_before - total_after) / total_before * 100
    
    print(f"è¿‡æ»¤åæ•°æ®ç»Ÿè®¡:")
    print(f"  ä¿ç•™æ ·æœ¬æ•°: {total_after:,}")
    print(f"  è¿‡æ»¤æ‰æ ·æœ¬æ•°: {total_before - total_after:,}")
    print(f"  æ•°æ®å‡å°‘æ¯”ä¾‹: {reduction_rate:.1f}%")
    
    # æ£€æŸ¥è¿‡æ»¤åçš„æ ‡ç­¾åˆ†å¸ƒ
    if 'log_type' in filtered_df.columns:
        print(f"è¿‡æ»¤åæ ‡ç­¾åˆ†å¸ƒ:")
        print(filtered_df['log_type'].value_counts())
    
    return filtered_df 
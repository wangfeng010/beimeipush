#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Feature Analysis Utilities - ç‰¹å¾åˆ†æå·¥å…·
"""

import os
import json
import datetime
from typing import Dict, List, Tuple, Any, Optional, Union

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.metrics import roc_auc_score

# æ·»åŠ é…ç½®å·¥å…·å¯¼å…¥
from .config_utils import load_feature_config


def check_feature_importance(
    model: tf.keras.Model, 
    dataset: tf.data.Dataset, 
    train_config: Optional[Dict[str, Any]] = None,
    feat_config_path: str = "config/feat.yml"
) -> Dict[str, float]:
    """
    ä½¿ç”¨æ’åˆ—é‡è¦æ€§æ–¹æ³•æ£€æŸ¥ç‰¹å¾é‡è¦æ€§
    
    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        dataset: æµ‹è¯•æ•°æ®é›†
        train_config: è®­ç»ƒé…ç½®
        feat_config_path: ç‰¹å¾é…ç½®æ–‡ä»¶è·¯å¾„
    
    Returns:
        ç‰¹å¾é‡è¦æ€§å­—å…¸ï¼Œå…¶ä¸­é”®ä¸ºç‰¹å¾åï¼Œå€¼ä¸ºé‡è¦æ€§åˆ†æ•°(åŸºçº¿AUC - éšæœºåŒ–åçš„AUC)
    """
    # è®¾ç½®å‚æ•°
    num_batches = train_config.get('eval_batches', 5) if train_config else 5
    num_repeats = train_config.get('importance_repeats', 3) if train_config else 3
    
    # ä½¿ç”¨é‡‡æ ·æ•°æ®è¿›è¡Œè¯„ä¼°
    print(f"ä½¿ç”¨ {num_batches} æ‰¹æ¬¡æ•°æ®è¯„ä¼°ç‰¹å¾é‡è¦æ€§...")
    print(f"ç‰¹å¾é…ç½®æ–‡ä»¶: {feat_config_path}")
    sample_dataset = dataset.take(num_batches)
    
    # è·å–åŸºçº¿æ€§èƒ½
    baseline_auc, all_labels, all_preds = _calculate_baseline_performance(model, sample_dataset)
    print(f"åŸºçº¿ AUC: {baseline_auc:.4f}")
    
    # è·å–ç‰¹å¾åç§°
    input_feature_names = _get_feature_names(sample_dataset)
    
    # ğŸš€ ç›´æ¥ä»é…ç½®æ–‡ä»¶è·å–å¤„ç†åçš„ç‰¹å¾åç§°
    processed_feature_names = _get_processed_feature_names_from_config(feat_config_path)
    
    print(f"åŸå§‹è¾“å…¥ç‰¹å¾æ•°é‡: {len(input_feature_names)}")
    print(f"å¤„ç†åç‰¹å¾æ•°é‡: {len(processed_feature_names)}")
    
    # è¯„ä¼°åŸå§‹è¾“å…¥ç‰¹å¾çš„é‡è¦æ€§
    print("\nè¯„ä¼°åŸå§‹è¾“å…¥ç‰¹å¾é‡è¦æ€§:")
    input_feature_importance = _evaluate_features(
        model, sample_dataset, input_feature_names, 
        baseline_auc, num_repeats, is_processed=False
    )
    
    # è¯„ä¼°å¤„ç†åç‰¹å¾çš„é‡è¦æ€§
    feature_importance = input_feature_importance
    if processed_feature_names:
        print("\nè¯„ä¼°å¤„ç†åç‰¹å¾é‡è¦æ€§:")
        processed_feature_importance = _evaluate_processed_features(
            model, sample_dataset, processed_feature_names,
            baseline_auc, num_repeats, feat_config_path
        )
        # åˆå¹¶ç‰¹å¾é‡è¦æ€§ç»“æœ
        feature_importance.update(processed_feature_importance)
    
    # å¤„ç†å’Œä¿å­˜ç»“æœ
    sorted_importance = _process_and_save_results(feature_importance)
    
    return sorted_importance


def _calculate_baseline_performance(
    model: tf.keras.Model, 
    dataset: tf.data.Dataset
) -> Tuple[float, List[float], List[float]]:
    """è®¡ç®—æ¨¡å‹åœ¨åŸå§‹æ•°æ®é›†ä¸Šçš„åŸºçº¿æ€§èƒ½"""
    all_labels = []
    all_preds = []
    
    for i, (x, y) in enumerate(dataset):
        try:
            preds = model(x, training=False)
            y_pred = preds.numpy().flatten()
            y_true = y.numpy().flatten()
            
            all_preds.extend(y_pred)
            all_labels.extend(y_true)
            print(f"å·²å¤„ç†æ‰¹æ¬¡ {i+1}")
        except Exception as e:
            print(f"å¤„ç†æ‰¹æ¬¡æ—¶å‡ºé”™: {str(e)}")
    
    baseline_auc = roc_auc_score(all_labels, all_preds)
    return baseline_auc, all_labels, all_preds


def _get_feature_names(dataset: tf.data.Dataset) -> List[str]:
    """ä»æ•°æ®é›†ä¸­è·å–è¾“å…¥ç‰¹å¾åç§°"""
    first_batch = next(iter(dataset))
    return list(first_batch[0].keys())


def _get_processed_feature_names_from_config(config_path: str = "config/feat.yml") -> List[str]:
    """
    ä»é…ç½®æ–‡ä»¶ä¸­è·å–å¤„ç†åçš„ç‰¹å¾åç§°
    
    Args:
        config_path: ç‰¹å¾é…ç½®æ–‡ä»¶è·¯å¾„
    
    Returns:
        å¤„ç†åç‰¹å¾åç§°åˆ—è¡¨
    """
    try:
        # è§£æé…ç½®æ˜ å°„
        mappings = _parse_feature_config_mappings(config_path)
        feature_names = list(mappings.keys())
        
        print(f"ä»é…ç½®æ–‡ä»¶è§£æåˆ°çš„å¤„ç†åç‰¹å¾: {feature_names}")
        return feature_names
        
    except Exception as e:
        print(f"ä»é…ç½®æ–‡ä»¶è·å–ç‰¹å¾åç§°å¤±è´¥: {str(e)}")
        return []


def _evaluate_features(
    model: tf.keras.Model,
    dataset: tf.data.Dataset,
    feature_names: List[str],
    baseline_auc: float,
    num_repeats: int,
    is_processed: bool = False
) -> Dict[str, float]:
    """è¯„ä¼°ç‰¹å¾é‡è¦æ€§"""
    feature_importance = {}
    
    for feature_name in feature_names:
        importance = _evaluate_single_feature(
            model, dataset, feature_name, baseline_auc, num_repeats
        )
        feature_importance[feature_name] = importance
        print(f"  ç‰¹å¾ {feature_name} é‡è¦æ€§: {importance:.6f}")
    
    return feature_importance


def _evaluate_processed_features(
    model: tf.keras.Model, 
    dataset: tf.data.Dataset, 
    feature_names: List[str], 
    baseline_auc: float,
    num_repeats: int,
    feat_config_path: str
) -> Dict[str, float]:
    """è¯„ä¼°å¤„ç†åç‰¹å¾çš„é‡è¦æ€§"""
    feature_importance = {}
    
    # ğŸš€ æ–°æ–¹æ¡ˆï¼šä»é…ç½®æ–‡ä»¶è‡ªåŠ¨è§£æç‰¹å¾æ˜ å°„
    print("æ­£åœ¨ä»é…ç½®æ–‡ä»¶è§£æç‰¹å¾æ˜ å°„å…³ç³»...")
    config_mappings = _parse_feature_config_mappings(feat_config_path)
    
    # ä»æ¨¡å‹ä¸­è·å–ç‰¹å¾æ˜ å°„ï¼ˆä½œä¸ºè¡¥å……ï¼‰
    model_mappings = {}
    if hasattr(model, 'feature_pipelines'):
        for _, processors in model.feature_pipelines:
            if processors and len(processors) > 0:
                last_processor = processors[-1]
                first_processor = processors[0]
                
                if hasattr(last_processor, 'col_out') and last_processor.col_out:
                    output_name = last_processor.col_out
                    
                    if hasattr(first_processor, 'col_in'):
                        input_features = [first_processor.col_in] if isinstance(first_processor.col_in, str) else first_processor.col_in
                        model_mappings[output_name] = input_features
    
    # åˆå¹¶æ˜ å°„ï¼šä¼˜å…ˆä½¿ç”¨é…ç½®æ–‡ä»¶è§£æçš„æ˜ å°„ï¼Œç„¶åæ˜¯æ¨¡å‹æ˜ å°„
    feature_mapping = {}
    feature_mapping.update(model_mappings)  # å…ˆæ·»åŠ æ¨¡å‹æ˜ å°„
    feature_mapping.update(config_mappings)  # é…ç½®æ˜ å°„è¦†ç›–æ¨¡å‹æ˜ å°„
    
    print(f"æ€»å…±è§£æåˆ° {len(feature_mapping)} ä¸ªç‰¹å¾æ˜ å°„å…³ç³»")
    
    # è¯„ä¼°ç‰¹å¾é‡è¦æ€§
    for processed_name in feature_names:
        # æŸ¥æ‰¾å¯¹åº”è¾“å…¥ç‰¹å¾
        input_features = feature_mapping.get(processed_name, [])
        
        # å›é€€ç­–ç•¥ï¼šå¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ å°„ï¼Œå°è¯•ç®€å•çš„åç§°åŒ¹é…
        if not input_features and "_emb" in processed_name:
            possible_input = processed_name.replace("_emb", "")
            dataset_feature_names = _get_feature_names(dataset)
            if possible_input in dataset_feature_names:
                input_features = [possible_input]
                print(f"  ä½¿ç”¨å›é€€ç­–ç•¥ä¸º {processed_name} æ‰¾åˆ°è¾“å…¥: {possible_input}")
        
        if input_features:
            print(f"è¯„ä¼°å¤„ç†åç‰¹å¾ '{processed_name}' (è¾“å…¥: {', '.join(input_features)})çš„é‡è¦æ€§...")
            
            # å¯¹äºBERTç‰¹å¾ï¼Œä½¿ç”¨ç‰¹æ®Šçš„è¯„ä¼°æ–¹æ³•
            if _is_bert_feature(processed_name):
                importance = _evaluate_bert_feature_importance(
                    model, dataset, processed_name, input_features, baseline_auc, num_repeats
                )
            else:
                # è®¡ç®—é‡è¦æ€§
                importances = [_evaluate_single_feature(model, dataset, input_name, baseline_auc, num_repeats) 
                              for input_name in input_features]
                # å–æœ€å¤§å€¼ä½œä¸ºæœ€ç»ˆé‡è¦æ€§
                importance = max(importances) if importances else 0.0
            
            feature_importance[processed_name] = importance
            print(f"  å¤„ç†åç‰¹å¾ {processed_name} é‡è¦æ€§: {importance:.6f}")
        else:
            print(f"  âš ï¸  è­¦å‘Š: æ‰¾ä¸åˆ°å¤„ç†åç‰¹å¾ {processed_name} çš„è¾“å…¥ç‰¹å¾")
            feature_importance[processed_name] = 0.0
    
    return feature_importance


def _is_bert_feature(feature_name: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦ä¸ºBERTç‰¹å¾"""
    bert_indicators = ['precomputed', 'bert', 'transformer']
    return any(indicator in feature_name.lower() for indicator in bert_indicators)


def _evaluate_single_feature(
    model: tf.keras.Model, 
    dataset: tf.data.Dataset, 
    feature_name: str, 
    baseline_auc: float,
    num_repeats: int
) -> float:
    """è¯„ä¼°å•ä¸ªç‰¹å¾çš„é‡è¦æ€§"""
    print(f"è¯„ä¼°ç‰¹å¾ '{feature_name}' çš„é‡è¦æ€§...")
    feature_aucs = []
    
    for _ in range(num_repeats):
        all_preds = []
        all_labels = []
        
        for x, y in dataset:
            try:
                # åˆ›å»ºç‰¹å¾å‰¯æœ¬å¹¶éšæœºåŒ–
                x_copy = {k: tf.identity(v) for k, v in x.items()}
                if x_copy[feature_name].shape[0] > 1:
                    x_copy[feature_name] = tf.random.shuffle(x_copy[feature_name])
                
                # é¢„æµ‹
                preds = model(x_copy, training=False)
                all_preds.extend(preds.numpy().flatten())
                all_labels.extend(y.numpy().flatten())
            except Exception as e:
                print(f"  é¢„æµ‹é”™è¯¯: {str(e)}")
                continue
        
        # è®¡ç®—AUC
        if all_preds:
            permuted_auc = roc_auc_score(all_labels, all_preds)
            feature_aucs.append(permuted_auc)
    
    # è®¡ç®—é‡è¦æ€§
    if feature_aucs:
        avg_permuted_auc = np.mean(feature_aucs)
        return baseline_auc - avg_permuted_auc
    else:
        print(f"  æ— æ³•è¯„ä¼°ç‰¹å¾ {feature_name} çš„é‡è¦æ€§")
        return 0.0


def _evaluate_bert_feature_importance(
    model: tf.keras.Model,
    dataset: tf.data.Dataset,
    feature_name: str,
    input_features: List[str],
    baseline_auc: float,
    num_repeats: int
) -> float:
    """ä¸“é—¨è¯„ä¼°BERTç‰¹å¾çš„é‡è¦æ€§"""
    print(f"  ä½¿ç”¨BERTç‰¹å¾ä¸“ç”¨æ–¹æ³•è¯„ä¼° '{feature_name}'...")
    
    # å¯¹äºBERTç‰¹å¾ï¼Œæˆ‘ä»¬ç›´æ¥è¯„ä¼°ç›¸å…³çš„æ–‡æœ¬è¾“å…¥ç‰¹å¾
    # å› ä¸ºBERTç‰¹å¾æœ¬è´¨ä¸Šæ˜¯å¯¹æ–‡æœ¬å†…å®¹çš„è¡¨ç¤º
    feature_aucs = []
    
    for _ in range(num_repeats):
        all_preds = []
        all_labels = []
        
        for x, y in dataset:
            try:
                # åˆ›å»ºç‰¹å¾å‰¯æœ¬å¹¶éšæœºåŒ–ç›¸å…³çš„æ–‡æœ¬ç‰¹å¾
                x_copy = {k: tf.identity(v) for k, v in x.items()}
                
                # éšæœºåŒ–æ‰€æœ‰ç›¸å…³çš„è¾“å…¥ç‰¹å¾
                for input_feature in input_features:
                    if input_feature in x_copy and x_copy[input_feature].shape[0] > 1:
                        x_copy[input_feature] = tf.random.shuffle(x_copy[input_feature])
                
                # é¢„æµ‹
                preds = model(x_copy, training=False)
                all_preds.extend(preds.numpy().flatten())
                all_labels.extend(y.numpy().flatten())
            except Exception as e:
                print(f"    BERTç‰¹å¾è¯„ä¼°é”™è¯¯: {str(e)}")
                continue
        
        # è®¡ç®—AUC
        if all_preds:
            permuted_auc = roc_auc_score(all_labels, all_preds)
            feature_aucs.append(permuted_auc)
    
    # è®¡ç®—é‡è¦æ€§
    if feature_aucs:
        avg_permuted_auc = np.mean(feature_aucs)
        return baseline_auc - avg_permuted_auc
    else:
        print(f"    æ— æ³•è¯„ä¼°BERTç‰¹å¾ {feature_name} çš„é‡è¦æ€§")
        return 0.0


def _process_and_save_results(feature_importance: Dict[str, float]) -> Dict[str, float]:
    """å¤„ç†å’Œä¿å­˜ç‰¹å¾é‡è¦æ€§ç»“æœ"""
    # æŒ‰é‡è¦æ€§æ’åº
    sorted_importance = {
        k: v for k, v in sorted(
            feature_importance.items(), 
            key=lambda item: abs(item[1]), 
            reverse=True
        )
    }
    
    # æ‰“å°ç‰¹å¾é‡è¦æ€§æ’å
    print("\nç‰¹å¾é‡è¦æ€§æ’å:")
    for i, (feature, importance) in enumerate(sorted_importance.items()):
        # å¯¹BERTç‰¹å¾è¿›è¡Œç‰¹æ®Šæ ‡è®°
        feature_display = feature
        if any(bert_class in feature for bert_class in ['BertEmbedding', 'PrecomputedEmbedding']):
            feature_display = f"[BERT] {feature}"
        print(f"{i+1}. {feature_display}: {importance:.6f}")
    
    # ä¿å­˜ç»“æœ
    os.makedirs("./logs", exist_ok=True)
    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    importance_file = f"./logs/feature_importance_{timestamp}.json"
    
    with open(importance_file, 'w') as f:
        json.dump(sorted_importance, f, indent=2)
    
    print(f"ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜åˆ°: {importance_file}")
    
    return sorted_importance


def plot_feature_importance(
    feature_importance: Dict[str, float],
    figsize: Tuple[int, int] = (12, 8),
    save_path: Optional[str] = None
) -> None:
    """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾è¡¨"""
    # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
    os.makedirs("./logs", exist_ok=True)
    
    # æŒ‰é‡è¦æ€§æ’åº
    sorted_features = sorted(
        feature_importance.items(), 
        key=lambda x: abs(x[1]), 
        reverse=True
    )
    
    # æå–ç‰¹å¾åç§°å’Œé‡è¦æ€§åˆ†æ•°
    features = []
    importance = []
    colors = []
    
    # å¤„ç†ç‰¹å¾åç§°ï¼Œçªå‡ºæ˜¾ç¤ºBERTç‰¹å¾
    for name, imp in sorted_features:
        # æ˜¯å¦ä¸ºBERTç‰¹å¾
        is_bert = any(bert_class in name for bert_class in ['BertEmbedding', 'PrecomputedEmbedding'])
        feature_display = f"[BERT] {name}" if is_bert else name
        colors.append('darkred' if is_bert and imp > 0 else 
                     'salmon' if is_bert else 
                     'teal' if imp > 0 else 'skyblue')
        
        features.append(feature_display)
        importance.append(imp)
    
    # é™åˆ¶æ˜¾ç¤ºçš„ç‰¹å¾æ•°é‡
    max_features = 20
    if len(features) > max_features:
        features = features[:max_features]
        importance = importance[:max_features]
        colors = colors[:max_features]
        print(f"æ³¨æ„: åªæ˜¾ç¤ºå‰{max_features}ä¸ªæœ€é‡è¦çš„ç‰¹å¾")
    
    # åˆ›å»ºå›¾è¡¨
    plt.figure(figsize=figsize)
    plt.barh(features, importance, color=colors)
    
    # æ·»åŠ æ ‡ç­¾å’Œæ ‡é¢˜
    plt.xlabel('é‡è¦æ€§åˆ†æ•° (åŸºçº¿AUC - éšæœºåŒ–åçš„AUC)')
    plt.ylabel('ç‰¹å¾')
    plt.title('ç‰¹å¾é‡è¦æ€§æ’å (çº¢è‰²æ ‡è®°BERTç‰¹å¾)')
    plt.grid(axis='x', linestyle='--', alpha=0.7)
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    if save_path is None:
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        save_path = f'./logs/feature_importance_{timestamp}.png'
    
    plt.savefig(save_path, dpi=300)
    print(f"ç‰¹å¾é‡è¦æ€§å›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")
    plt.close()


def _parse_feature_config_mappings(config_path: str = "config/feat.yml") -> Dict[str, List[str]]:
    """
    ä»feat.ymlé…ç½®æ–‡ä»¶è‡ªåŠ¨è§£æç‰¹å¾æ˜ å°„å…³ç³»
    
    Args:
        config_path: ç‰¹å¾é…ç½®æ–‡ä»¶è·¯å¾„
    
    Returns:
        ç‰¹å¾æ˜ å°„å­—å…¸ï¼Œæ ¼å¼ä¸º {è¾“å‡ºç‰¹å¾å: [è¾“å…¥ç‰¹å¾åˆ—è¡¨]}
    """
    print(f"æ­£åœ¨è§£æç‰¹å¾é…ç½®æ–‡ä»¶: {config_path}")
    
    try:
        # åŠ è½½ç‰¹å¾é…ç½®
        feature_configs = load_feature_config(config_path, exclude_features=[])  # æš‚æ—¶ä¸æ’é™¤ä»»ä½•ç‰¹å¾
        
        feature_mappings = {}
        
        for config in feature_configs:
            feat_name = config.get('feat_name')
            operations = config.get('operations', [])
            
            if not feat_name or not operations:
                continue
            
            # è·å–è¾“å…¥ç‰¹å¾
            input_features = _extract_input_features_from_operations(operations)
            
            # è·å–è¾“å‡ºç‰¹å¾å
            output_feature = _extract_output_feature_from_operations(operations)
            
            if output_feature and input_features:
                # å¤„ç†ç‰¹æ®Šçš„BERTç‰¹å¾
                actual_inputs = _resolve_bert_feature_inputs(feat_name, input_features, operations)
                feature_mappings[output_feature] = actual_inputs
                
                print(f"  è§£æç‰¹å¾: {output_feature} <- {actual_inputs}")
            elif output_feature:
                # å³ä½¿æ²¡æœ‰æ‰¾åˆ°è¾“å…¥ç‰¹å¾ï¼Œä¹Ÿè¦è®°å½•è¿™ä¸ªè¾“å‡ºç‰¹å¾
                # è¿™æ ·å¯ä»¥ç¡®ä¿æ‰€æœ‰ç‰¹å¾éƒ½è¢«è¯„ä¼°
                if input_features:
                    feature_mappings[output_feature] = input_features
                else:
                    # å°è¯•ä»feat_nameæ¨æ–­åŸå§‹è¾“å…¥
                    inferred_input = _infer_input_from_feature_name(feat_name)
                    feature_mappings[output_feature] = [inferred_input] if inferred_input else [feat_name]
                print(f"  è§£æç‰¹å¾ (å›é€€): {output_feature} <- {feature_mappings[output_feature]}")
        
        print(f"æˆåŠŸè§£æäº† {len(feature_mappings)} ä¸ªç‰¹å¾æ˜ å°„å…³ç³»")
        return feature_mappings
        
    except Exception as e:
        print(f"è§£æç‰¹å¾é…ç½®å¤±è´¥: {str(e)}")
        print("å›é€€åˆ°åŸºæœ¬æ˜ å°„ç­–ç•¥...")
        return {}


def _extract_input_features_from_operations(operations: List[Dict[str, Any]]) -> List[str]:
    """ä»æ“ä½œåºåˆ—ä¸­æå–**åŸå§‹è¾“å…¥ç‰¹å¾**ï¼Œè¿‡æ»¤ä¸­é—´å¤„ç†æ­¥éª¤"""
    if not operations:
        return []
    
    # æ”¶é›†æ‰€æœ‰è¾“å…¥ç‰¹å¾
    all_inputs = []
    for operation in operations:
        # æ£€æŸ¥å•ä¸ªè¾“å…¥
        if 'col_in' in operation:
            col_in = operation['col_in']
            if isinstance(col_in, str):
                all_inputs.append(col_in)
        
        # æ£€æŸ¥å¤šä¸ªè¾“å…¥
        if 'col_in_list' in operation:
            col_in_list = operation['col_in_list']
            if isinstance(col_in_list, list):
                for col in col_in_list:
                    if isinstance(col, str):
                        all_inputs.append(col)
    
    # æ”¶é›†æ‰€æœ‰è¾“å‡ºç‰¹å¾ï¼ˆè¿™äº›æ˜¯ä¸­é—´æ­¥éª¤äº§ç”Ÿçš„ï¼‰
    all_outputs = []
    for operation in operations:
        if 'col_out' in operation:
            col_out = operation['col_out']
            if isinstance(col_out, str):
                all_outputs.append(col_out)
    
    # ğŸ¯ å…³é”®é€»è¾‘ï¼šåªä¿ç•™é‚£äº›**ä¸æ˜¯ç”±å‰é¢æ­¥éª¤äº§ç”Ÿçš„**è¾“å…¥ç‰¹å¾
    # å³ï¼šä¸åœ¨outputsåˆ—è¡¨ä¸­çš„inputsæ‰æ˜¯çœŸæ­£çš„åŸå§‹è¾“å…¥
    original_inputs = []
    for input_feature in all_inputs:
        if input_feature not in all_outputs:
            if input_feature not in original_inputs:  # å»é‡
                original_inputs.append(input_feature)
    
    return original_inputs


def _extract_output_feature_from_operations(operations: List[Dict[str, Any]]) -> Optional[str]:
    """ä»æ“ä½œåºåˆ—ä¸­æå–æœ€ç»ˆè¾“å‡ºç‰¹å¾å"""
    if not operations:
        return None
    
    # å–æœ€åä¸€ä¸ªæ“ä½œçš„è¾“å‡º
    last_operation = operations[-1]
    return last_operation.get('col_out')


def _resolve_bert_feature_inputs(
    feat_name: str, 
    input_features: List[str], 
    operations: List[Dict[str, Any]]
) -> List[str]:
    """
    è§£æBERTç‰¹å¾çš„å®é™…è¾“å…¥
    
    BERTç‰¹å¾çš„ç‰¹æ®Šä¹‹å¤„ï¼š
    - é…ç½®ä¸­è¾“å…¥æ˜¯create_timeï¼ˆç”¨äºç´¢å¼•ï¼‰
    - å®é™…å¤„ç†çš„æ˜¯æ¨é€å†…å®¹
    """
    # æ£€æŸ¥æ˜¯å¦ä¸ºBERTç‰¹å¾
    is_bert_feature = any(
        op.get('func_name') == 'PrecomputedEmbedding' 
        for op in operations
    )
    
    if not is_bert_feature:
        return input_features
    
    # æ ¹æ®ç‰¹å¾åç§°æ¨æ–­å®é™…å¤„ç†çš„å†…å®¹
    bert_mappings = {
        'title_content_precomputed_emb': ['push_title', 'push_content'],
        'push_title_bert_emb': ['push_title'],
        'push_content_bert_emb': ['push_content']
    }
    
    # æ£€æŸ¥æ˜¯å¦åŒ¹é…å·²çŸ¥çš„BERTç‰¹å¾
    for pattern, actual_inputs in bert_mappings.items():
        if pattern in feat_name:
            print(f"    æ£€æµ‹åˆ°BERTç‰¹å¾ {feat_name}ï¼Œæ˜ å°„åˆ°å®é™…è¾“å…¥: {actual_inputs}")
            return actual_inputs
    
    # å¦‚æœæ˜¯æœªçŸ¥çš„BERTç‰¹å¾ï¼Œå°è¯•ä»ç‰¹å¾åæ¨æ–­
    if 'bert' in feat_name.lower() or 'precomputed' in feat_name.lower():
        if 'title' in feat_name and 'content' in feat_name:
            return ['push_title', 'push_content']
        elif 'title' in feat_name:
            return ['push_title']
        elif 'content' in feat_name:
            return ['push_content']
    
    # é»˜è®¤è¿”å›åŸå§‹è¾“å…¥ç‰¹å¾
    return input_features 


def _infer_input_from_feature_name(feat_name: str) -> Optional[str]:
    """
    ä»ç‰¹å¾åç§°æ¨æ–­åŸå§‹è¾“å…¥ç‰¹å¾
    
    Args:
        feat_name: ç‰¹å¾åç§°ï¼Œå¦‚ "watchlists_emb"
    
    Returns:
        æ¨æ–­çš„è¾“å…¥ç‰¹å¾åç§°ï¼Œå¦‚ "watchlists"
    """
    # ç§»é™¤å¸¸è§çš„åç¼€
    suffixes_to_remove = ['_emb', '_embedding', '_feature']
    
    inferred = feat_name
    for suffix in suffixes_to_remove:
        if inferred.endswith(suffix):
            inferred = inferred[:-len(suffix)]
            break
    
    # å¦‚æœæ¨æ–­ç»“æœä¸åŸåç§°ä¸åŒï¼Œåˆ™è¿”å›æ¨æ–­ç»“æœ
    if inferred != feat_name:
        return inferred
    
    return None 
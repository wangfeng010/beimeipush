from typing import Dict, Optional, Any

import tensorflow as tf
import numpy as np
import os
import pickle
from tensorflow.keras.layers import Layer


class PrecomputedEmbedding(Layer):
    """
    é¢„å…ˆè®¡ç®—çš„embeddingåŠ è½½å±‚ã€‚
    
    æ­¤å±‚ä»é¢„å…ˆè®¡ç®—çš„embeddingæ–‡ä»¶ä¸­åŠ è½½å¯¹åº”çš„å‘é‡è¡¨ç¤ºã€‚
    
    Attributes:
        config: åŒ…å«ä»¥ä¸‹é…ç½®é¡¹çš„å­—å…¸ï¼š
            embedding_dir (str): é¢„å…ˆè®¡ç®—çš„embeddingç›®å½•
            embedding_dim (int): embeddingå‘é‡çš„ç»´åº¦
            id_map_file (str): å¯é€‰ï¼ŒIDåˆ°ç´¢å¼•çš„æ˜ å°„æ–‡ä»¶
            embedding_type (str): åµŒå…¥ç±»å‹ï¼Œå¯é€‰å€¼ä¸º"title_content"ã€"push_title"ã€"push_content"
    """

    # é»˜è®¤é…ç½®
    DEFAULT_CONFIG = {
        "embedding_dir": "data/precomputed_embeddings",
        "embedding_dim": 128,
        "id_map_file": None,
        "embedding_type": "title_content",  # æ–°å¢å‚æ•°ï¼Œæ”¯æŒä¸åŒç±»å‹çš„åµŒå…¥
    }

    def __init__(self, config: Optional[Dict[str, Any]] = None, **kwargs):
        """
        åˆå§‹åŒ–PrecomputedEmbeddingå±‚ã€‚

        Args:
            config: é…ç½®å‚æ•°å­—å…¸
            **kwargs: ä¼ é€’ç»™çˆ¶ç±»çš„å…¶ä»–å‚æ•°
        """
        # é¦–å…ˆè°ƒç”¨çˆ¶ç±»çš„åˆå§‹åŒ–æ–¹æ³•ï¼Œåªä¼ é€’kerasèƒ½ç†è§£çš„å‚æ•°
        super(PrecomputedEmbedding, self).__init__(**kwargs)
        
        # ä½¿ç”¨é»˜è®¤é…ç½®ï¼Œç„¶åç”¨æä¾›çš„é…ç½®è¦†ç›–
        self.config = self.DEFAULT_CONFIG.copy()
        if config:
            self.config.update(config)
        
        # åŠ è½½é…ç½®
        self.embedding_dir = self.config["embedding_dir"]
        self.embedding_dim = self.config["embedding_dim"]
        self.id_map_file = self.config["id_map_file"]
        self.embedding_type = self.config["embedding_type"]
        
        # åŠ è½½æ•°æ®ç»“æ„
        self.embeddings = {}
        self.id_map = {}
        self._load_embeddings()
        
        # ä¸ºæœªæ‰¾åˆ°çš„embeddingå‡†å¤‡ä¸€ä¸ªé›¶å‘é‡
        self.zero_vector = tf.zeros([self.embedding_dim], dtype=tf.float32)
        
        # âœ… æ–°å¢ï¼šæ·»åŠ å…¨å±€è¡Œè®¡æ•°å™¨ï¼Œç”¨äºè·Ÿè¸ªå½“å‰å¤„ç†åˆ°ç¬¬å‡ è¡Œ  
        self.global_row_counter = tf.Variable(0, trainable=False, dtype=tf.int64)

    def _load_embeddings(self):
        """åŠ è½½æ‰€æœ‰é¢„å…ˆè®¡ç®—çš„embeddings"""
        print(f"åŠ è½½é¢„å…ˆè®¡ç®—çš„embeddingsï¼Œç›®å½•: {self.embedding_dir}ï¼Œç±»å‹: {self.embedding_type}")
        
        # æ£€æŸ¥embeddingç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.embedding_dir):
            print(f"è­¦å‘Š: embeddingç›®å½•ä¸å­˜åœ¨: {self.embedding_dir}")
            return
        
        # æ ¹æ®embedding_typeé€‰æ‹©å¯¹åº”çš„æ–‡ä»¶åç¼€
        embedding_suffix = f"{self.embedding_type}_embeddings.pkl"
        
        # åŠ è½½æ‰€æœ‰embeddingæ–‡ä»¶
        for filename in os.listdir(self.embedding_dir):
            if filename.endswith(embedding_suffix):
                file_path = os.path.join(self.embedding_dir, filename)
                # ä»æ–‡ä»¶åä¸­æå–æ•°æ®é›†åç§°
                dataset_name = filename.split(f"_{self.embedding_type}_embeddings.pkl")[0]
                
                with open(file_path, 'rb') as f:
                    embeddings = pickle.load(f)
                    self.embeddings[dataset_name] = tf.convert_to_tensor(embeddings, dtype=tf.float32)
                
                print(f"åŠ è½½äº†{self.embedding_type} embeddingæ–‡ä»¶: {filename}, å½¢çŠ¶: {embeddings.shape}")
        
        # å¦‚æœæŒ‡å®šäº†IDæ˜ å°„æ–‡ä»¶ï¼Œåˆ™åŠ è½½å®ƒ
        if self.id_map_file and os.path.exists(self.id_map_file):
            with open(self.id_map_file, 'rb') as f:
                self.id_map = pickle.load(f)

    def call(self, inputs):
        """æ‰§è¡Œå±‚çš„å‰å‘ä¼ æ’­"""
        
        def get_embedding_by_row_index(input_str, row_idx):
            """
            ğŸš€ æ–°æ–¹æ¡ˆï¼šæ ¹æ®æ•°æ®åœ¨CSVä¸­çš„å®é™…è¡Œä½ç½®è·å–å¯¹åº”çš„é¢„è®¡ç®—åµŒå…¥
            
            Args:
                input_str: æ—¶é—´æˆ³å­—ç¬¦ä¸²ï¼ˆç”¨äºç¡®å®šæ•°æ®é›†ï¼‰  
                row_idx: åœ¨å½“å‰æ‰¹æ¬¡ä¸­çš„è¡Œç´¢å¼•
                
            Returns:
                å¯¹åº”çš„BERTåµŒå…¥å‘é‡
            """
            try:
                # ä»æ—¥æœŸæ—¶é—´å­—ç¬¦ä¸²ä¸­æå–æ—¥æœŸéƒ¨åˆ†ä½œä¸ºæ•°æ®é›†åç§°
                if isinstance(input_str, tf.Tensor):
                    input_str = input_str.numpy().decode('utf-8')
                else:
                    input_str = input_str.decode('utf-8')
                
                # æå–æ—¥æœŸéƒ¨åˆ†ï¼Œæ ¼å¼ä¸ºYYYYMMDD
                date_parts = input_str.split(' ')[0].split('-')
                if len(date_parts) == 3:
                    dataset_name = f"{date_parts[0]}{date_parts[1]}{date_parts[2]}"
                else:
                    return self.zero_vector
                
                if dataset_name in self.embeddings:
                    dataset_embs = self.embeddings[dataset_name]
                    
                    # ğŸ¯ å…³é”®æ”¹è¿›ï¼šä½¿ç”¨å…¨å±€è¡Œè®¡æ•°å™¨è€Œä¸æ˜¯æ—¶é—´æˆ³å“ˆå¸Œ
                    # è¿™ç¡®ä¿äº†æ¯ä¸€è¡Œæ•°æ®å¯¹åº”å…¶åœ¨CSVä¸­çš„å®é™…ä½ç½®
                    current_global_idx = self.global_row_counter.numpy()
                    embedding_idx = current_global_idx % dataset_embs.shape[0]
                    
                    # æ›´æ–°å…¨å±€è®¡æ•°å™¨
                    self.global_row_counter.assign_add(1)
                    
                    return dataset_embs[embedding_idx]
                
            except Exception as e:
                print(f"é”™è¯¯å¤„ç†è¾“å…¥ {input_str}: {str(e)}")
            
            return self.zero_vector
        
        # ğŸš€ æ‰¹å¤„ç†ä¼˜åŒ–ï¼šé¢„å…ˆè®¡ç®—æ‰¹æ¬¡å¤§å°å¹¶é‡ç½®è®¡æ•°å™¨
        batch_size = tf.shape(inputs)[0]
        
        def process_batch_with_indices(inputs_batch):
            """å¤„ç†æ•´ä¸ªæ‰¹æ¬¡ï¼Œæ¯ä¸ªæ ·æœ¬ä½¿ç”¨å…¶åœ¨æ‰¹æ¬¡ä¸­çš„ä½ç½®"""
            results = []
            for i in tf.range(batch_size):
                input_str = inputs_batch[i]
                embedding = tf.py_function(
                    lambda: get_embedding_by_row_index(input_str, i),
                    [],
                    tf.float32
                )
                embedding.set_shape([self.embedding_dim])
                results.append(embedding)
            
            return tf.stack(results)
        
        # æ‰§è¡Œæ‰¹å¤„ç†
        result = process_batch_with_indices(inputs)
        
        return result

    def get_config(self):
        """è·å–å±‚çš„é…ç½®ï¼Œç”¨äºåºåˆ—åŒ–"""
        base_config = super(PrecomputedEmbedding, self).get_config()
        base_config.update({"config": self.config})
        return base_config


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    import numpy as np
    from tensorflow.keras.layers import Input
    from tensorflow.keras.models import Model
    
    print("æµ‹è¯• PrecomputedEmbedding")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    os.makedirs("test_embeddings", exist_ok=True)
    test_embeddings = np.random.random((100, 128)).astype(np.float32)
    with open("test_embeddings/test_title_content_embeddings.pkl", 'wb') as f:
        pickle.dump(test_embeddings, f)
    
    # åˆ›å»ºæ¨¡å‹
    input_layer = Input(shape=(), dtype=tf.string)
    embedding_layer = PrecomputedEmbedding(config={
        "embedding_dir": "test_embeddings",
        "embedding_dim": 128,
        "embedding_type": "title_content"
    })
    output = embedding_layer(input_layer)
    model = Model(inputs=input_layer, outputs=output)
    
    # æµ‹è¯•è¾“å…¥
    test_inputs = tf.constant(["test:0", "test:50", "unknown"])
    
    # é¢„æµ‹
    results = model.predict(test_inputs)
    print(f"è¾“å‡ºå½¢çŠ¶: {results.shape}")
    print(f"ç¬¬ä¸€ä¸ªembedding: {results[0][:5]}...")  # æ˜¾ç¤ºå‰5ä¸ªå€¼
    
    # æ¸…ç†æµ‹è¯•æ•°æ®
    import shutil
    shutil.rmtree("test_embeddings") 
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程流水线深度解析\n",
    "\n",
    "本 Notebook 旨在逐步分解并展示项目中从原始数据到特征生成的完整特征工程流程。\n",
    "\n",
    "## 架构概述\n",
    "\n",
    "```\n",
    "Raw Data (CSV) → YAML Config → UniProcess Operations → Final Features\n",
    "     ↓              ↓              ↓                    ↓\n",
    "data/train/    config/feat.yml   env/UniProcess/    Processed DataFrame\n",
    "```\n",
    "\n",
    "## 流程步骤\n",
    "\n",
    "1. **加载原始数据**：从 `data/train/` 目录加载样本数据\n",
    "2. **解析 YAML 配置**：读取 `config/feat.yml` 并解析为结构化对象\n",
    "3. **构建操作中心**：创建 OP_HUB 映射表\n",
    "4. **执行特征工程**：逐步应用每个操作\n",
    "5. **查看最终结果**：分析处理后的特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤 1: 环境设置与导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基础库导入完成\n",
      "已添加环境路径: /Users/main/Documents/02推荐算法/02同花顺实习/02ainvest-push-recall-group-master_wf/env\n",
      "\n",
      "环境设置完成！\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "from hashlib import md5\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from glob import glob\n",
    "\n",
    "print(\"基础库导入完成\")\n",
    "\n",
    "# 添加项目路径\n",
    "project_root = os.getcwd()\n",
    "env_path = os.path.join(project_root, 'env')\n",
    "if env_path not in sys.path:\n",
    "    sys.path.insert(0, env_path)\n",
    "    print(f\"已添加环境路径: {env_path}\")\n",
    "\n",
    "print(\"\\n环境设置完成！\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 步骤 2: 加载原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 1800001088\n",
      "create_time: 2025-05-31 08:39:07\n",
      "log_type: PR\n",
      "watchlists: nan\n",
      "holdings: nan\n",
      "country: Germany\n",
      "prefer_bid: nan\n",
      "user_propernoun: germany#3.06|mid-america#1.02\n",
      "push_title: Ainvest Newswire\n",
      "push_content: Hims & Hers Health Lays Off 4% of Staff Amid Strategy Shift\n",
      "item_code: [{\"market\":\"169\",\"score\":0,\"code\":\"HIMS\",\"tagId\":\"U000012934\",\"name\":\"Hims & Hers Health\",\"type\":0,\"parentId\":\"US_ROBOT0f37d7fd3fca6a41\"},{\"market\":\"169\",\"score\":0,\"code\":\"NVO\",\"tagId\":\"U000002999\",\"name\":\"Novo Nordisk\",\"type\":0,\"parentId\":\"US_ROBOT0f37d7fd3fca6a41\"}]\n",
      "item_tags: [{\"score\":0.7803922295570374,\"tagId\":\"51510\",\"name\":\"us_high_importance\",\"type\":4,\"parentId\":\"US_ROBOT0f37d7fd3fca6a41\"},{\"score\":0.7803922295570374,\"tagId\":\"57967\",\"name\":\"Fusion\",\"type\":4,\"parentId\":\"US_ROBOT0f37d7fd3fca6a41\"},{\"tagId\":\"1002\",\"name\":\"no_penny_stock\",\"type\":4,\"parentId\":\"US_ROBOT0f37d7fd3fca6a41\"}]\n",
      "submit_type: autoFlash\n"
     ]
    }
   ],
   "source": [
    "data_path = 'data/train/*.csv'\n",
    "# 使用glob获取所有匹配的CSV文件路径\n",
    "csv_files = glob(data_path)\n",
    "if not csv_files:\n",
    "    raise ValueError(f\"No CSV files found in {data_path}\")\n",
    "\n",
    "# 读取并合并所有CSV文件\n",
    "df_raw = pd.concat([pd.read_csv(f) for f in csv_files], ignore_index=True)\n",
    "for col in df_raw.columns: print(f\"{col}: {df_raw[col].iloc[0]}\") # 直观展示各列的具体数据的样式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "各列数据样例:\n",
      "user_id: 1800001088\n",
      "create_time: 2025-05-31 08:39:07\n",
      "log_type: PR\n",
      "watchlists: nan\n",
      "holdings: nan\n",
      "country: Germany\n",
      "prefer_bid: nan\n",
      "user_propernoun: germany#3.06|mid-america#1.02\n",
      "push_title: Ainvest Newswire\n",
      "push_content: Hims & Hers Health Lays Off 4% of Staff Amid Strategy Shift\n",
      "item_code: [{\"market\":\"169\",\"score\":0,\"code\":\"HIMS\",\"tagId\":\"U000012934\",\"name\":\"Hims & Hers Health\",\"type\":0,\"parentId\":\"US_ROBOT0f37d7fd3fca6a41\"},{\"market\":\"169\",\"score\":0,\"code\":\"NVO\",\"tagId\":\"U000002999\",\"name\":\"Novo Nordisk\",\"type\":0,\"parentId\":\"US_ROBOT0f37d7fd3fca6a41\"}]\n",
      "item_tags: [{\"score\":0.7803922295570374,\"tagId\":\"51510\",\"name\":\"us_high_importance\",\"type\":4,\"parentId\":\"US_ROBOT0f37d7fd3fca6a41\"},{\"score\":0.7803922295570374,\"tagId\":\"57967\",\"name\":\"Fusion\",\"type\":4,\"parentId\":\"US_ROBOT0f37d7fd3fca6a41\"},{\"tagId\":\"1002\",\"name\":\"no_penny_stock\",\"type\":4,\"parentId\":\"US_ROBOT0f37d7fd3fca6a41\"}]\n",
      "submit_type: autoFlash\n"
     ]
    }
   ],
   "source": [
    "# 查看具体数据样例\n",
    "print(\"各列数据样例:\")\n",
    "for col in df_raw.columns:\n",
    "    print(f\"{col}: {df_raw[col].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 步骤 3: 加载并解析 YAML 配置\n",
    "这是特征工程的\"蓝图\"，定义了每个特征如何从原始数据中提取和转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exclude_features': {'current': 'default', 'default': [], 'exclude_user_behavior': ['user_watch_stk_code', 'prefer_bid_code', 'hold_bid_code', 'user_propernoun'], 'exclude_user_propernoun': ['user_propernoun']}, 'pipelines': [{'embedding_dim': 8, 'feat_name': 'hour', 'feat_type': 'sparse', 'input_sample': '2024-08-02 00:44:05', 'operations': [{'col_in': 'create_time', 'col_out': 'create_time', 'func_name': 'fillna', 'func_parameters': {'na_value': '2024-08-02 00:16:34'}}, {'col_in': 'create_time', 'col_out': 'hour', 'func_name': 'to_hour', 'func_parameters': {}}], 'vocabulary_size': 24}, {'embedding_dim': 8, 'feat_name': 'weekday', 'feat_type': 'sparse', 'input_sample': '2024-08-02 00:44:05', 'operations': [{'col_in': 'create_time', 'col_out': 'weekday', 'func_name': 'to_weekday', 'func_parameters': {}}], 'vocabulary_size': 7}, {'embedding_dim': 8, 'feat_name': 'user_watch_stk_code_hash', 'feat_type': 'varlen_sparse', 'input_sample': 'AAPL_185 & TSLA_185', 'operations': [{'col_in': 'watchlists', 'col_out': 'watchlists', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null_0 & null_0'}}, {'col_in': 'watchlists', 'col_out': 'watchlists', 'func_name': 'split', 'func_parameters': {'sep': ' & '}}, {'col_in': 'watchlists', 'col_out': 'watchlists', 'func_name': 'seperation', 'func_parameters': {'sep': '_'}}, {'col_in': 'watchlists', 'col_out': 'user_watch_stk_code', 'func_name': 'list_get', 'func_parameters': {'item_index': 0}}, {'col_in': 'user_watch_stk_code', 'col_out': 'user_watch_stk_code', 'func_name': 'remove_items', 'func_parameters': {'target_values': ['AAPL', 'AMZN', 'GOOGL', 'TSLA']}}, {'col_in': 'user_watch_stk_code', 'col_out': 'user_watch_stk_code', 'func_name': 'padding', 'func_parameters': {'max_len': 5, 'pad_value': 'null'}}, {'col_in': 'user_watch_stk_code', 'col_out': 'user_watch_stk_code_hash', 'func_name': 'list_hash', 'func_parameters': {'vocabulary_size': 10000}}], 'vocabulary_size': 10000}, {'embedding_dim': 8, 'feat_name': 'country_hash', 'feat_type': 'sparse', 'input_sample': 'United States', 'operations': [{'col_in': 'country', 'col_out': 'country', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null'}}, {'col_in': 'country', 'col_out': 'country_hash', 'func_name': 'str_hash', 'func_parameters': {'vocabulary_size': 200}}], 'vocabulary_size': 200}, {'embedding_dim': 8, 'feat_name': 'prefer_bid_code_hash', 'feat_type': 'varlen_sparse', 'input_sample': 'AAPL#0.24809|AMZN#0.24809|GOOGL#0.24809|TSLA#0.24809', 'operations': [{'col_in': 'prefer_bid', 'col_out': 'prefer_bid', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null#0'}}, {'col_in': 'prefer_bid', 'col_out': 'prefer_bid', 'func_name': 'split', 'func_parameters': {'sep': '|'}}, {'col_in': 'prefer_bid', 'col_out': 'prefer_bid', 'func_name': 'seperation', 'func_parameters': {'sep': '#'}}, {'col_in': 'prefer_bid', 'col_out': 'prefer_bid_code', 'func_name': 'list_get', 'func_parameters': {'item_index': 0}}, {'col_in': 'prefer_bid_code', 'col_out': 'prefer_bid_code', 'func_name': 'padding', 'func_parameters': {'max_len': 5, 'pad_value': 'null'}}, {'col_in': 'prefer_bid_code', 'col_out': 'prefer_bid_code_hash', 'func_name': 'list_hash', 'func_parameters': {'vocabulary_size': 10000}}], 'vocabulary_size': 10000}, {'embedding_dim': 8, 'feat_name': 'hold_bid_code_hash', 'feat_type': 'varlen_sparse', 'input_sample': 'JD,185|BAC,169|TSLA,185', 'operations': [{'col_in': 'holdings', 'col_out': 'holdings', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null,0'}}, {'col_in': 'holdings', 'col_out': 'holdings', 'func_name': 'split', 'func_parameters': {'sep': '|'}}, {'col_in': 'holdings', 'col_out': 'holdings', 'func_name': 'seperation', 'func_parameters': {'sep': ','}}, {'col_in': 'holdings', 'col_out': 'hold_bid_code', 'func_name': 'list_get', 'func_parameters': {'item_index': 0}}, {'col_in': 'hold_bid_code', 'col_out': 'hold_bid_code', 'func_name': 'padding', 'func_parameters': {'max_len': 5, 'pad_value': 'null'}}, {'col_in': 'hold_bid_code', 'col_out': 'hold_bid_code_hash', 'func_name': 'list_hash', 'func_parameters': {'vocabulary_size': 10000}}], 'vocabulary_size': 10000}, {'embedding_dim': 8, 'feat_name': 'user_propernoun_hash', 'feat_type': 'varlen_sparse', 'input_sample': 'apple#1.02|nike#1.02', 'operations': [{'col_in': 'user_propernoun', 'col_out': 'user_propernoun', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null#0'}}, {'col_in': 'user_propernoun', 'col_out': 'user_propernoun', 'func_name': 'split', 'func_parameters': {'sep': '|'}}, {'col_in': 'user_propernoun', 'col_out': 'user_propernoun', 'func_name': 'seperation', 'func_parameters': {'sep': '#'}}, {'col_in': 'user_propernoun', 'col_out': 'user_propernoun_code', 'func_name': 'list_get', 'func_parameters': {'item_index': 0}}, {'col_in': 'user_propernoun_code', 'col_out': 'user_propernoun_code', 'func_name': 'padding', 'func_parameters': {'max_len': 5, 'pad_value': 'null'}}, {'col_in': 'user_propernoun_code', 'col_out': 'user_propernoun_hash', 'func_name': 'list_hash', 'func_parameters': {'vocabulary_size': 10000}}], 'vocabulary_size': 10000}, {'embedding_dim': 8, 'feat_name': 'push_title_hash', 'feat_type': 'sparse', 'input_sample': 'Breaking News', 'operations': [{'col_in': 'push_title', 'col_out': 'push_title', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null'}}, {'col_in': 'push_title', 'col_out': 'push_title_hash', 'func_name': 'str_hash', 'func_parameters': {'vocabulary_size': 8}}], 'vocabulary_size': 8}, {'embedding_dim': 8, 'feat_name': 'title_len', 'feat_type': 'sparse', 'input_sample': \"Hong Kong's Leading Broker Futu Securities Introduces Bitcoin and XRP Trading with Lucrative Incentives\", 'operations': [{'col_in': 'push_content', 'col_out': 'push_content', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null'}}, {'col_in': 'push_content', 'col_out': 'push_content', 'func_name': 'split', 'func_parameters': {'sep': ' '}}, {'col_in': 'push_content', 'col_out': 'title_len', 'func_name': 'list_len', 'func_parameters': {}}, {'col_in': 'title_len', 'col_out': 'title_len', 'func_name': 'int_max', 'func_parameters': {'max_value': 31}}], 'vocabulary_size': 32}, {'embedding_dim': 8, 'feat_name': 'item_code_hash', 'feat_type': 'varlen_sparse', 'input_sample': '[{\"market\":\"185\",\"score\":1,\"code\":\"META\",\"name\":\"Meta\",\"type\":0,\"parentId\":\"0339437d07195361\"}]', 'operations': [{'col_in': 'item_code', 'col_out': 'item_code', 'func_name': 'fillna', 'func_parameters': {'na_value': '[{\"market\":\"null\",\"score\":0,\"code\":\"null\",\"name\":\"null\",\"type\":\"null\",\"parentId\":\"null\"}]'}}, {'col_in': 'item_code', 'col_out': 'item_code', 'func_name': 'json_object_to_list', 'func_parameters': {'key': 'code'}}, {'col_in': 'item_code', 'col_out': 'item_code', 'func_name': 'padding', 'func_parameters': {'max_len': 5, 'pad_value': 'null'}}, {'col_in': 'item_code', 'col_out': 'item_code_hash', 'func_name': 'list_hash', 'func_parameters': {'vocabulary_size': 10000}}], 'vocabulary_size': 10000}, {'embedding_dim': 8, 'feat_name': 'submit_type_hash', 'feat_type': 'sparse', 'input_sample': 'auto_flash', 'operations': [{'col_in': 'submit_type', 'col_out': 'submit_type', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null'}}, {'col_in': 'submit_type', 'col_out': 'submit_type_hash', 'func_name': 'str_hash', 'func_parameters': {'vocabulary_size': 10}}], 'vocabulary_size': 10}, {'embedding_dim': 8, 'feat_name': 'tag_id_hash', 'feat_type': 'varlen_sparse', 'input_sample': '[{\"score\":0,\"tagId\":\"57967\",\"name\":\"Fusion\",\"type\":4,\"parentId\":\"0339437d07195361\"}]', 'operations': [{'col_in': 'item_tags', 'col_out': 'item_tags', 'func_name': 'fillna', 'func_parameters': {'na_value': '[{\"score\":0,\"tagId\":\"null\",\"name\":\"null\",\"type\":0,\"parentId\":\"null\"}]'}}, {'col_in': 'item_tags', 'col_out': 'tagIds', 'func_name': 'json_object_to_list', 'func_parameters': {'key': 'tagId'}}, {'col_in': 'tagIds', 'col_out': 'tagIds', 'func_name': 'padding', 'func_parameters': {'max_len': 3, 'pad_value': 'null'}}, {'col_in': 'tagIds', 'col_out': 'tag_id_hash', 'func_name': 'list_hash', 'func_parameters': {'vocabulary_size': 10000}}], 'vocabulary_size': 10000}]}\n"
     ]
    }
   ],
   "source": [
    "feat_config_path = 'config/feat.yml'\n",
    "\n",
    "with open(feat_config_path, 'r', encoding='utf-8') as f:\n",
    "    feat_config = yaml.safe_load(f)\n",
    "\n",
    "print(feat_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "示例流水线: hour\n",
      "{'embedding_dim': 8,\n",
      " 'feat_name': 'hour',\n",
      " 'feat_type': 'sparse',\n",
      " 'input_sample': '2024-08-02 00:44:05',\n",
      " 'operations': [{'col_in': 'create_time',\n",
      "                 'col_out': 'create_time',\n",
      "                 'func_name': 'fillna',\n",
      "                 'func_parameters': {'na_value': '2024-08-02 00:16:34'}},\n",
      "                {'col_in': 'create_time',\n",
      "                 'col_out': 'hour',\n",
      "                 'func_name': 'to_hour',\n",
      "                 'func_parameters': {}}],\n",
      " 'vocabulary_size': 24}\n"
     ]
    }
   ],
   "source": [
    "# 查看一个完整的流水线配置示例\n",
    "example_pipeline = feat_config['pipelines'][0]  # 取第一个流水线\n",
    "print(f\"示例流水线: {example_pipeline['feat_name']}\")\n",
    "pprint(example_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 步骤 4: 定义特征操作函数\n",
    "\n",
    "由于我们无法直接导入 UniProcess 库，我们将手动实现一些关键的特征操作函数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义缺失值常量\n",
    "MISSING_VALUE = [None, '', 'null', 'NULL', 'None', np.nan]\n",
    "\n",
    "def fillna(x: Union[float, int, str], na_value: Union[float, int, str]) -> Union[float, int, str]:\n",
    "    \"\"\"填充缺失值\"\"\"\n",
    "    if x in MISSING_VALUE or (isinstance(x, float) and pd.isna(x)):\n",
    "        return na_value\n",
    "    return x\n",
    "\n",
    "def split(x: str, sep: str) -> List[str]:\n",
    "    \"\"\"字符串分割\"\"\"\n",
    "    return str(x).split(sep)\n",
    "\n",
    "def seperation(x: List[str], sep: str) -> List[List[str]]:\n",
    "    \"\"\"列表元素二次分割\"\"\"\n",
    "    if not isinstance(x, list):\n",
    "        return []\n",
    "    return [item.split(sep) for item in x]\n",
    "\n",
    "def list_get(x: List[List[Any]], item_index: int) -> List[Any]:\n",
    "    \"\"\"获取嵌套列表中指定位置的元素\"\"\"\n",
    "    if not isinstance(x, list):\n",
    "        return []\n",
    "    result = []\n",
    "    for sublist in x:\n",
    "        if isinstance(sublist, list) and len(sublist) > item_index:\n",
    "            result.append(sublist[item_index])\n",
    "        else:\n",
    "            result.append('null')\n",
    "    return result\n",
    "\n",
    "def remove_items(x: List[str], target_values: List[str]) -> List[str]:\n",
    "    \"\"\"移除列表中的指定元素\"\"\"\n",
    "    if not isinstance(x, list):\n",
    "        return []\n",
    "    return [item for item in x if item not in target_values]\n",
    "\n",
    "def padding(x: List[Any], pad_value: Union[str, float, int], max_len: int) -> List[Any]:\n",
    "    \"\"\"列表填充到指定长度\"\"\"\n",
    "    if not isinstance(x, list):\n",
    "        x = []\n",
    "    if len(x) >= max_len:\n",
    "        return x[:max_len]\n",
    "    else:\n",
    "        return x + [pad_value] * (max_len - len(x))\n",
    "\n",
    "def list_hash(x: List[str], vocabulary_size: int) -> List[int]:\n",
    "    \"\"\"对列表中每个元素进行哈希\"\"\"\n",
    "    if not isinstance(x, list):\n",
    "        return []\n",
    "    result = []\n",
    "    for item in x:\n",
    "        hash_val = int(md5(str(item).encode()).hexdigest(), 16) % vocabulary_size\n",
    "        result.append(hash_val)\n",
    "    return result\n",
    "\n",
    "def str_hash(x: str, vocabulary_size: int) -> int:\n",
    "    \"\"\"字符串哈希\"\"\"\n",
    "    return int(md5(str(x).encode()).hexdigest(), 16) % vocabulary_size\n",
    "\n",
    "def to_hour(x: str) -> int:\n",
    "    \"\"\"提取时间中的小时\"\"\"\n",
    "    try:\n",
    "        dt = pd.to_datetime(x)\n",
    "        return dt.hour\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def to_weekday(x: str) -> int:\n",
    "    \"\"\"提取时间中的星期\"\"\"\n",
    "    try:\n",
    "        dt = pd.to_datetime(x)\n",
    "        return dt.weekday()\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def list_len(x: List) -> int:\n",
    "    \"\"\"列表长度\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return len(x)\n",
    "    return 0\n",
    "\n",
    "def int_max(x: int, max_value: int) -> int:\n",
    "    \"\"\"限制整数最大值\"\"\"\n",
    "    return min(int(x), max_value)\n",
    "\n",
    "def json_object_to_list(x: str, key: str) -> List[str]:\n",
    "    \"\"\"从JSON对象列表中提取指定键的值\"\"\"\n",
    "    try:\n",
    "        data = json.loads(x)\n",
    "        if isinstance(data, list):\n",
    "            return [item.get(key, 'null') for item in data if isinstance(item, dict)]\n",
    "        return ['null']\n",
    "    except:\n",
    "        return ['null']\n",
    "\n",
    "def map_to_int(x: Union[str, List], map_dict: Dict[str, int], default_code: int = 0) -> Union[List[int], int]:\n",
    "    \"\"\"映射到整数\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return [map_dict.get(item, default_code) for item in x]\n",
    "    else:\n",
    "        return map_dict.get(str(x), default_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OP_HUB 构建完成，包含 14 个操作函数\n",
      "可用函数: ['fillna', 'split', 'seperation', 'list_get', 'remove_items', 'padding', 'list_hash', 'str_hash', 'to_hour', 'to_weekday', 'list_len', 'int_max', 'json_object_to_list', 'map_to_int']\n"
     ]
    }
   ],
   "source": [
    "# 构建操作中心 (OP_HUB)\n",
    "OP_HUB = {\n",
    "    'fillna': fillna,\n",
    "    'split': split,\n",
    "    'seperation': seperation,\n",
    "    'list_get': list_get,\n",
    "    'remove_items': remove_items,\n",
    "    'padding': padding,\n",
    "    'list_hash': list_hash,\n",
    "    'str_hash': str_hash,\n",
    "    'to_hour': to_hour,\n",
    "    'to_weekday': to_weekday,\n",
    "    'list_len': list_len,\n",
    "    'int_max': int_max,\n",
    "    'json_object_to_list': json_object_to_list,\n",
    "    'map_to_int': map_to_int\n",
    "}\n",
    "\n",
    "print(f\"OP_HUB 构建完成，包含 {len(OP_HUB)} 个操作函数\")\n",
    "print(f\"可用函数: {list(OP_HUB.keys())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 步骤 5: 实现特征工程执行引擎\n",
    "\n",
    "这是核心部分：我们将实现 `run_one_op` 函数来执行单个操作。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_op(df: pd.DataFrame, operation: dict) -> pd.DataFrame:\n",
    "    \"\"\"执行单个特征操作\"\"\"\n",
    "    # 获取操作配置\n",
    "    col_in = operation['col_in']\n",
    "    col_out = operation['col_out']\n",
    "    func_name = operation['func_name']\n",
    "    parameters = operation.get('func_parameters', {})\n",
    "    \n",
    "    # 检查函数是否存在\n",
    "    if func_name not in OP_HUB:\n",
    "        return df\n",
    "    \n",
    "    # 检查输入列是否存在\n",
    "    input_cols = [col_in] if isinstance(col_in, str) else col_in\n",
    "    if not all(col in df.columns for col in input_cols):\n",
    "        return df\n",
    "    \n",
    "    # 准备特征转换函数\n",
    "    transform_func = partial(OP_HUB[func_name], **parameters)\n",
    "    \n",
    "    # 执行特征转换\n",
    "    if isinstance(col_in, list):\n",
    "        df[col_out] = df[col_in].apply(lambda row: transform_func(*row), axis=1)\n",
    "    else:\n",
    "        df[col_out] = df[col_in].apply(transform_func)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 步骤 6: 执行完整的特征工程流水线\n",
    "\n",
    "现在我们将遍历所有的特征流水线，逐一执行每个操作。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_feature_pipelines(df_raw: pd.DataFrame, feat_config: dict) -> tuple[pd.DataFrame, list]:\n",
    "    \"\"\"执行特征工程流水线\"\"\"\n",
    "    # 创建数据副本\n",
    "    df = df_raw.copy()\n",
    "    \n",
    "    # 获取需要处理的流水线\n",
    "    pipelines = feat_config['pipelines']\n",
    "\n",
    "    # 记录成功处理的特征\n",
    "    processed_features = []\n",
    "    \n",
    "    # 执行每个特征处理流水线\n",
    "    for pipeline in pipelines:\n",
    "        feat_name = pipeline['feat_name']\n",
    "        operations = pipeline['operations']\n",
    "        \n",
    "        # 执行流水线中的每个操作\n",
    "        for operation in operations:\n",
    "            df = run_one_op(df, operation)\n",
    "\n",
    "        # 记录处理成功的特征\n",
    "        processed_features.append(feat_name)\n",
    "    \n",
    "    return df,processed_features\n",
    "\n",
    "# 处理特征\n",
    "df_processed, processed_features = process_feature_pipelines(df_raw, feat_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 步骤 7: 分析处理结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据结构对比:\n",
      "原始列数: 13\n",
      "处理后列数: 30\n",
      "新增列数: 17\n",
      "\n",
      "原始列名:\n",
      "['user_id', 'create_time', 'log_type', 'watchlists', 'holdings', 'country', 'prefer_bid', 'user_propernoun', 'push_title', 'push_content', 'item_code', 'item_tags', 'submit_type']\n",
      "\n",
      "新增列名:\n",
      "['hour', 'weekday', 'user_watch_stk_code', 'user_watch_stk_code_hash', 'country_hash', 'prefer_bid_code', 'prefer_bid_code_hash', 'hold_bid_code', 'hold_bid_code_hash', 'user_propernoun_code', 'user_propernoun_hash', 'push_title_hash', 'title_len', 'item_code_hash', 'submit_type_hash', 'tagIds', 'tag_id_hash']\n"
     ]
    }
   ],
   "source": [
    "# 比较处理前后的数据结构\n",
    "print(\"数据结构对比:\")\n",
    "print(f\"原始列数: {len(df_raw.columns)}\")\n",
    "print(f\"处理后列数: {len(df_processed.columns)}\")\n",
    "print(f\"新增列数: {len(df_processed.columns) - len(df_raw.columns)}\")\n",
    "\n",
    "print(\"\\n原始列名:\")\n",
    "print(list(df_raw.columns))\n",
    "\n",
    "print(\"\\n新增列名:\")\n",
    "new_columns = [col for col in df_processed.columns if col not in df_raw.columns]\n",
    "print(new_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功生成的特征详情:\n",
      "  hour: int64 = 8\n",
      "  weekday: int64 = 5\n",
      "  user_watch_stk_code_hash: list = [8381, 8381, 8381, 8381, 8381]\n",
      "  country_hash: int64 = 71\n",
      "  prefer_bid_code_hash: list = [8381, 8381, 8381, 8381, 8381]\n",
      "  hold_bid_code_hash: list = [8381, 8381, 8381, 8381, 8381]\n",
      "  user_propernoun_hash: list = [178, 417, 8381, 8381, 8381]\n",
      "  push_title_hash: int64 = 7\n",
      "  title_len: int64 = 12\n",
      "  item_code_hash: list = [6837, 3491, 8381, 8381, 8381]\n",
      "  submit_type_hash: int64 = 6\n",
      "  tag_id_hash: list = [8139, 8993, 880]\n"
     ]
    }
   ],
   "source": [
    "# 查看成功生成的特征\n",
    "print(\"成功生成的特征详情:\")\n",
    "for feat_name in processed_features:\n",
    "    if feat_name in df_processed.columns:\n",
    "        sample_data = df_processed[feat_name].iloc[0]\n",
    "        data_type = type(sample_data).__name__\n",
    "        print(f\"  {feat_name}: {data_type} = {sample_data}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最终处理结果预览:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>log_type</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "      <th>user_watch_stk_code_hash</th>\n",
       "      <th>country_hash</th>\n",
       "      <th>prefer_bid_code_hash</th>\n",
       "      <th>hold_bid_code_hash</th>\n",
       "      <th>user_propernoun_hash</th>\n",
       "      <th>push_title_hash</th>\n",
       "      <th>title_len</th>\n",
       "      <th>item_code_hash</th>\n",
       "      <th>submit_type_hash</th>\n",
       "      <th>tag_id_hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1800001088</td>\n",
       "      <td>PR</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>[8381, 8381, 8381, 8381, 8381]</td>\n",
       "      <td>71</td>\n",
       "      <td>[8381, 8381, 8381, 8381, 8381]</td>\n",
       "      <td>[8381, 8381, 8381, 8381, 8381]</td>\n",
       "      <td>[178, 417, 8381, 8381, 8381]</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>[6837, 3491, 8381, 8381, 8381]</td>\n",
       "      <td>6</td>\n",
       "      <td>[8139, 8993, 880]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1800001417</td>\n",
       "      <td>PR</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>[8381, 8381, 8381, 8381, 8381]</td>\n",
       "      <td>145</td>\n",
       "      <td>[8381, 8381, 8381, 8381, 8381]</td>\n",
       "      <td>[8381, 8381, 8381, 8381, 8381]</td>\n",
       "      <td>[8381, 8381, 8381, 8381, 8381]</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>[7762, 6902, 6157, 1986, 5551]</td>\n",
       "      <td>4</td>\n",
       "      <td>[4634, 2106, 9827]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1800001501</td>\n",
       "      <td>PC</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>[1895, 8808, 1021, 8381, 8381]</td>\n",
       "      <td>145</td>\n",
       "      <td>[8381, 8381, 8381, 8381, 8381]</td>\n",
       "      <td>[8381, 8381, 8381, 8381, 8381]</td>\n",
       "      <td>[323, 9351, 3453, 8381, 8381]</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>[9724, 8381, 8381, 8381, 8381]</td>\n",
       "      <td>1</td>\n",
       "      <td>[2601, 4380, 4797]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1800001501</td>\n",
       "      <td>PR</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>[1895, 8808, 1021, 8381, 8381]</td>\n",
       "      <td>145</td>\n",
       "      <td>[8381, 8381, 8381, 8381, 8381]</td>\n",
       "      <td>[8381, 8381, 8381, 8381, 8381]</td>\n",
       "      <td>[323, 9351, 3453, 8381, 8381]</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>[7762, 6902, 6157, 1986, 5551]</td>\n",
       "      <td>4</td>\n",
       "      <td>[4634, 2106, 9827]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1800001819</td>\n",
       "      <td>PR</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>[8381, 8381, 8381, 8381, 8381]</td>\n",
       "      <td>145</td>\n",
       "      <td>[8381, 8381, 8381, 8381, 8381]</td>\n",
       "      <td>[8381, 8381, 8381, 8381, 8381]</td>\n",
       "      <td>[8381, 8381, 8381, 8381, 8381]</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>[916, 8381, 8381, 8381, 8381]</td>\n",
       "      <td>1</td>\n",
       "      <td>[9593, 4380, 8381]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id log_type  hour  weekday        user_watch_stk_code_hash  \\\n",
       "0  1800001088       PR     8        5  [8381, 8381, 8381, 8381, 8381]   \n",
       "1  1800001417       PR    22        5  [8381, 8381, 8381, 8381, 8381]   \n",
       "2  1800001501       PC    10        5  [1895, 8808, 1021, 8381, 8381]   \n",
       "3  1800001501       PR    22        5  [1895, 8808, 1021, 8381, 8381]   \n",
       "4  1800001819       PR    21        5  [8381, 8381, 8381, 8381, 8381]   \n",
       "\n",
       "   country_hash            prefer_bid_code_hash  \\\n",
       "0            71  [8381, 8381, 8381, 8381, 8381]   \n",
       "1           145  [8381, 8381, 8381, 8381, 8381]   \n",
       "2           145  [8381, 8381, 8381, 8381, 8381]   \n",
       "3           145  [8381, 8381, 8381, 8381, 8381]   \n",
       "4           145  [8381, 8381, 8381, 8381, 8381]   \n",
       "\n",
       "               hold_bid_code_hash            user_propernoun_hash  \\\n",
       "0  [8381, 8381, 8381, 8381, 8381]    [178, 417, 8381, 8381, 8381]   \n",
       "1  [8381, 8381, 8381, 8381, 8381]  [8381, 8381, 8381, 8381, 8381]   \n",
       "2  [8381, 8381, 8381, 8381, 8381]   [323, 9351, 3453, 8381, 8381]   \n",
       "3  [8381, 8381, 8381, 8381, 8381]   [323, 9351, 3453, 8381, 8381]   \n",
       "4  [8381, 8381, 8381, 8381, 8381]  [8381, 8381, 8381, 8381, 8381]   \n",
       "\n",
       "   push_title_hash  title_len                  item_code_hash  \\\n",
       "0                7         12  [6837, 3491, 8381, 8381, 8381]   \n",
       "1                7         10  [7762, 6902, 6157, 1986, 5551]   \n",
       "2                5         17  [9724, 8381, 8381, 8381, 8381]   \n",
       "3                7         10  [7762, 6902, 6157, 1986, 5551]   \n",
       "4                4         11   [916, 8381, 8381, 8381, 8381]   \n",
       "\n",
       "   submit_type_hash         tag_id_hash  \n",
       "0                 6   [8139, 8993, 880]  \n",
       "1                 4  [4634, 2106, 9827]  \n",
       "2                 1  [2601, 4380, 4797]  \n",
       "3                 4  [4634, 2106, 9827]  \n",
       "4                 1  [9593, 4380, 8381]  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 显示最终处理结果的部分数据\n",
    "print(\"最终处理结果预览:\")\n",
    "display_cols = ['user_id', 'log_type'] + processed_features  # 显示前6个新特征\n",
    "display_cols = [col for col in display_cols if col in df_processed.columns]\n",
    "\n",
    "df_processed[display_cols].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 总结\n",
    "\n",
    "通过这个 Notebook，我们完整地演示了特征工程流水线的执行过程：\n",
    "\n",
    "### 🎯 核心流程\n",
    "\n",
    "1. **数据加载**: 从 CSV 文件加载原始数据\n",
    "2. **配置解析**: 将 YAML 配置文件解析为可执行的操作序列\n",
    "3. **操作执行**: 通过 OP_HUB 查找并执行具体的特征变换函数\n",
    "4. **结果生成**: 生成最终的特征数据\n",
    "\n",
    "### 🔧 关键组件\n",
    "\n",
    "- **OP_HUB**: 操作函数注册表，连接配置文件中的 `func_name` 和实际的 Python 函数\n",
    "- **run_one_op**: 执行引擎，负责应用单个操作到 DataFrame\n",
    "- **Pipeline**: 操作序列，定义了特征的完整变换过程\n",
    "\n",
    "### 📊 特征类型\n",
    "\n",
    "- **sparse**: 稀疏特征，通常是分类变量的哈希值\n",
    "- **varlen_sparse**: 变长稀疏特征，处理列表型数据\n",
    "- **dense**: 稠密特征，数值型特征\n",
    "\n",
    "### 💡 设计优势\n",
    "\n",
    "这个流水线的设计使得特征工程变得：\n",
    "- **高度可配置**: 通过 YAML 文件定义特征变换\n",
    "- **模块化**: 每个操作函数都是独立的\n",
    "- **可复用**: 操作函数可以在不同的流水线中重复使用\n",
    "- **易于扩展**: 添加新的操作函数只需要在 OP_HUB 中注册\n",
    "\n",
    "这种架构在生产环境中非常有价值，因为它允许数据科学家快速实验不同的特征工程策略，而无需修改核心代码。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 特征表格分析模块已加载!\n",
      "使用方法: run_feature_analysis(df_processed, processed_features)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "特征分析模块 - 表格化展示\n",
    "对特征工程处理后的数据进行统计分析，以表格形式呈现结果\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class FeatureTableAnalyzer:\n",
    "    \"\"\"特征表格分析器 - 专注于表格化结果展示\"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame, feature_list: List[str]):\n",
    "        self.df = df\n",
    "        self.features = feature_list\n",
    "        self.analysis_results = {}\n",
    "    \n",
    "    def analyze_all_features(self) -> None:\n",
    "        \"\"\"分析所有特征并生成表格化报告\"\"\"\n",
    "        print(\"🔍 开始特征分析...\")\n",
    "        \n",
    "        # 1. 特征概览表\n",
    "        self.create_feature_overview_table()\n",
    "        \n",
    "        # 2. 数值型特征详细分析\n",
    "        self.analyze_numeric_features_table()\n",
    "        \n",
    "        # 3. 列表型特征详细分析\n",
    "        self.analyze_list_features_table()\n",
    "        \n",
    "        # 4. 数据质量报告\n",
    "        self.create_data_quality_table()\n",
    "        \n",
    "        print(\"\\n✅ 特征分析完成!\")\n",
    "    \n",
    "    def create_feature_overview_table(self) -> None:\n",
    "        \"\"\"创建特征概览表\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"📊 特征概览表\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        overview_data = []\n",
    "        for feature in self.features:\n",
    "            if feature in self.df.columns:\n",
    "                sample_value = self.df[feature].iloc[0]\n",
    "                feature_type = \"列表型\" if isinstance(sample_value, list) else \"数值型\"\n",
    "                \n",
    "                # 基本统计\n",
    "                unique_count = self.df[feature].nunique() if not isinstance(sample_value, list) else \"N/A\"\n",
    "                missing_count = self.df[feature].isnull().sum()\n",
    "                missing_rate = f\"{missing_count/len(self.df)*100:.2f}%\" if missing_count > 0 else \"0%\"\n",
    "                \n",
    "                # 样例值\n",
    "                if isinstance(sample_value, list):\n",
    "                    sample_str = f\"[{', '.join(map(str, sample_value[:3]))}...]\" if len(sample_value) > 3 else str(sample_value)\n",
    "                    if len(sample_str) > 50:\n",
    "                        sample_str = sample_str[:47] + \"...\"\n",
    "                else:\n",
    "                    sample_str = str(sample_value)\n",
    "                \n",
    "                overview_data.append({\n",
    "                    '特征名': feature,\n",
    "                    '类型': feature_type,\n",
    "                    '样本数': len(self.df),\n",
    "                    '唯一值数': unique_count,\n",
    "                    '缺失值': missing_rate,\n",
    "                    '样例值': sample_str\n",
    "                })\n",
    "        \n",
    "        overview_df = pd.DataFrame(overview_data)\n",
    "        print(overview_df.to_string(index=False))\n",
    "    \n",
    "    def analyze_numeric_features_table(self) -> None:\n",
    "        \"\"\"分析数值型特征并生成表格\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"📈 数值型特征统计表\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        numeric_features = []\n",
    "        stats_data = []\n",
    "        \n",
    "        for feature in self.features:\n",
    "            if feature in self.df.columns:\n",
    "                sample_value = self.df[feature].iloc[0]\n",
    "                if not isinstance(sample_value, list):\n",
    "                    numeric_features.append(feature)\n",
    "                    data = self.df[feature]\n",
    "                    \n",
    "                    stats_data.append({\n",
    "                        '特征名': feature,\n",
    "                        '最小值': data.min(),\n",
    "                        '最大值': data.max(),\n",
    "                        '均值': f\"{data.mean():.4f}\",\n",
    "                        '中位数': data.median(),\n",
    "                        '标准差': f\"{data.std():.4f}\",\n",
    "                        '25%分位': data.quantile(0.25),\n",
    "                        '75%分位': data.quantile(0.75),\n",
    "                        '偏度': f\"{data.skew():.4f}\"\n",
    "                    })\n",
    "        \n",
    "        if stats_data:\n",
    "            stats_df = pd.DataFrame(stats_data)\n",
    "            print(stats_df.to_string(index=False))\n",
    "            \n",
    "            # 数值型特征分布表\n",
    "            print(\"\\n📊 数值型特征分布表 (前5个高频值)\")\n",
    "            print(\"-\"*80)\n",
    "            for feature in numeric_features[:5]:  # 只显示前5个特征避免输出过长\n",
    "                data = self.df[feature]\n",
    "                value_counts = data.value_counts().head(5)\n",
    "                print(f\"\\n{feature}:\")\n",
    "                dist_data = []\n",
    "                for value, count in value_counts.items():\n",
    "                    dist_data.append({\n",
    "                        '值': value,\n",
    "                        '频次': count,\n",
    "                        '占比': f\"{count/len(data)*100:.2f}%\"\n",
    "                    })\n",
    "                dist_df = pd.DataFrame(dist_data)\n",
    "                print(dist_df.to_string(index=False))\n",
    "        else:\n",
    "            print(\"未找到数值型特征\")\n",
    "    \n",
    "    def analyze_list_features_table(self) -> None:\n",
    "        \"\"\"分析列表型特征并生成表格\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"📋 列表型特征统计表\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        list_features = []\n",
    "        list_stats_data = []\n",
    "        \n",
    "        for feature in self.features:\n",
    "            if feature in self.df.columns:\n",
    "                sample_value = self.df[feature].iloc[0]\n",
    "                if isinstance(sample_value, list):\n",
    "                    list_features.append(feature)\n",
    "                    data = self.df[feature]\n",
    "                    \n",
    "                    # 计算长度统计\n",
    "                    lengths = [len(x) if isinstance(x, list) else 0 for x in data]\n",
    "                    \n",
    "                    # 计算元素统计\n",
    "                    all_elements = []\n",
    "                    for item in data:\n",
    "                        if isinstance(item, list):\n",
    "                            all_elements.extend(item)\n",
    "                    \n",
    "                    element_counter = Counter(all_elements)\n",
    "                    \n",
    "                    list_stats_data.append({\n",
    "                        '特征名': feature,\n",
    "                        '平均长度': f\"{np.mean(lengths):.2f}\",\n",
    "                        '最小长度': min(lengths),\n",
    "                        '最大长度': max(lengths),\n",
    "                        '总元素数': len(all_elements),\n",
    "                        '唯一元素数': len(element_counter),\n",
    "                        '元素重复率': f\"{(1-len(element_counter)/len(all_elements))*100:.2f}%\" if all_elements else \"0%\"\n",
    "                    })\n",
    "        \n",
    "        if list_stats_data:\n",
    "            list_stats_df = pd.DataFrame(list_stats_data)\n",
    "            print(list_stats_df.to_string(index=False))\n",
    "            \n",
    "            # 列表长度分布表\n",
    "            print(\"\\n📏 列表长度分布表\")\n",
    "            print(\"-\"*80)\n",
    "            for feature in list_features:\n",
    "                data = self.df[feature]\n",
    "                lengths = [len(x) if isinstance(x, list) else 0 for x in data]\n",
    "                length_dist = Counter(lengths)\n",
    "                \n",
    "                print(f\"\\n{feature}:\")\n",
    "                length_data = []\n",
    "                for length, count in sorted(length_dist.items()):\n",
    "                    length_data.append({\n",
    "                        '长度': length,\n",
    "                        '样本数': count,\n",
    "                        '占比': f\"{count/len(data)*100:.2f}%\"\n",
    "                    })\n",
    "                length_df = pd.DataFrame(length_data)\n",
    "                print(length_df.to_string(index=False))\n",
    "                \n",
    "                # 高频元素表\n",
    "                print(f\"\\n{feature} - 高频元素 (前8个):\")\n",
    "                all_elements = []\n",
    "                for item in data:\n",
    "                    if isinstance(item, list):\n",
    "                        all_elements.extend(item)\n",
    "                \n",
    "                if all_elements:\n",
    "                    element_counter = Counter(all_elements)\n",
    "                    element_data = []\n",
    "                    for element, count in element_counter.most_common(8):\n",
    "                        element_data.append({\n",
    "                            '元素': element,\n",
    "                            '出现次数': count,\n",
    "                            '占比': f\"{count/len(all_elements)*100:.2f}%\"\n",
    "                        })\n",
    "                    element_df = pd.DataFrame(element_data)\n",
    "                    print(element_df.to_string(index=False))\n",
    "        else:\n",
    "            print(\"未找到列表型特征\")\n",
    "    \n",
    "    def create_data_quality_table(self) -> None:\n",
    "        \"\"\"创建数据质量报告表\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"🔍 数据质量报告\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        quality_data = []\n",
    "        for feature in self.features:\n",
    "            if feature in self.df.columns:\n",
    "                data = self.df[feature]\n",
    "                missing_count = data.isnull().sum()\n",
    "                \n",
    "                # 数据一致性检查\n",
    "                sample_value = data.iloc[0]\n",
    "                if isinstance(sample_value, list):\n",
    "                    # 检查列表特征的一致性\n",
    "                    lengths = [len(x) if isinstance(x, list) else 0 for x in data]\n",
    "                    length_variance = np.var(lengths)\n",
    "                    consistency = \"高\" if length_variance < 1 else \"中\" if length_variance < 4 else \"低\"\n",
    "                else:\n",
    "                    # 检查数值特征的分布\n",
    "                    cv = data.std() / data.mean() if data.mean() != 0 else 0\n",
    "                    consistency = \"高\" if cv < 0.1 else \"中\" if cv < 0.5 else \"低\"\n",
    "                \n",
    "                quality_data.append({\n",
    "                    '特征名': feature,\n",
    "                    '完整性': f\"{(1-missing_count/len(data))*100:.1f}%\",\n",
    "                    '缺失值数量': missing_count,\n",
    "                    '数据一致性': consistency,\n",
    "                    '数据类型': \"列表型\" if isinstance(sample_value, list) else \"数值型\",\n",
    "                    '是否可用': \"✅\" if missing_count < len(data) * 0.5 else \"⚠️\"\n",
    "                })\n",
    "        \n",
    "        quality_df = pd.DataFrame(quality_data)\n",
    "        print(quality_df.to_string(index=False))\n",
    "        \n",
    "        # 总体质量评估\n",
    "        total_features = len(quality_data)\n",
    "        usable_features = sum(1 for item in quality_data if item['是否可用'] == \"✅\")\n",
    "        avg_completeness = np.mean([float(item['完整性'].strip('%')) for item in quality_data])\n",
    "        \n",
    "        print(f\"\\n📋 总体评估:\")\n",
    "        print(f\"   • 特征总数: {total_features}\")\n",
    "        print(f\"   • 可用特征: {usable_features} ({usable_features/total_features*100:.1f}%)\")\n",
    "        print(f\"   • 平均完整性: {avg_completeness:.1f}%\")\n",
    "        print(f\"   • 数据质量: {'优秀' if avg_completeness > 95 else '良好' if avg_completeness > 85 else '需改进'}\")\n",
    "\n",
    "# 使用示例函数\n",
    "def run_feature_analysis(df_processed: pd.DataFrame, processed_features: List[str]) -> None:\n",
    "    \"\"\"运行特征分析的便捷函数\"\"\"\n",
    "    analyzer = FeatureTableAnalyzer(df_processed, processed_features)\n",
    "    analyzer.analyze_all_features()\n",
    "\n",
    "print(\"📊 特征表格分析模块已加载!\")\n",
    "print(\"使用方法: run_feature_analysis(df_processed, processed_features)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 特征分析使用说明:\n",
      "   在完成特征工程后，运行: run_comprehensive_feature_analysis()\n",
      "   或者直接运行: run_feature_analysis(df_processed, processed_features)\n",
      "\n",
      "💡 提示: 建议在notebook最后执行特征工程后再运行此分析\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 🚀 运行完整特征分析 \n",
    "# ===============================\n",
    "# 注意：此代码块需要在特征工程完成后运行\n",
    "# 将在notebook末尾执行特征工程后，再回到此处运行分析\n",
    "\n",
    "def run_comprehensive_feature_analysis():\n",
    "    \"\"\"运行完整的特征分析 - 在特征工程完成后调用\"\"\"\n",
    "    try:\n",
    "        # 检查是否已经完成特征工程\n",
    "        if 'df_processed' in globals() and 'processed_features' in globals():\n",
    "            print(\"🎯 开始运行完整特征分析...\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            # 运行表格化特征分析\n",
    "            run_feature_analysis(df_processed, processed_features)\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"🎉 特征分析完成！以上报告展示了所有特征的详细统计信息\")\n",
    "            \n",
    "        else:\n",
    "            print(\"⚠️  请先运行特征工程流水线，生成 df_processed 和 processed_features\")\n",
    "            print(\"   建议按顺序执行以下步骤：\")\n",
    "            print(\"   1. 运行数据加载代码块\")\n",
    "            print(\"   2. 运行YAML配置解析代码块\") \n",
    "            print(\"   3. 运行特征工程执行代码块\")\n",
    "            print(\"   4. 再回到此处运行特征分析\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 分析过程中出现错误: {e}\")\n",
    "        print(\"   请确保已经正确执行了前面的特征工程步骤\")\n",
    "\n",
    "# 显示使用说明\n",
    "print(\"📋 特征分析使用说明:\")\n",
    "print(\"   在完成特征工程后，运行: run_comprehensive_feature_analysis()\")\n",
    "print(\"   或者直接运行: run_feature_analysis(df_processed, processed_features)\")\n",
    "print(\"\\n💡 提示: 建议在notebook最后执行特征工程后再运行此分析\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 开始运行完整特征分析...\n",
      "================================================================================\n",
      "🔍 开始特征分析...\n",
      "\n",
      "================================================================================\n",
      "📊 特征概览表\n",
      "================================================================================\n",
      "                     特征名  类型   样本数 唯一值数 缺失值                   样例值\n",
      "                    hour 数值型 50000   24  0%                     8\n",
      "                 weekday 数值型 50000    6  0%                     5\n",
      "user_watch_stk_code_hash 列表型 50000  N/A  0% [8381, 8381, 8381...]\n",
      "            country_hash 数值型 50000  120  0%                    71\n",
      "    prefer_bid_code_hash 列表型 50000  N/A  0% [8381, 8381, 8381...]\n",
      "      hold_bid_code_hash 列表型 50000  N/A  0% [8381, 8381, 8381...]\n",
      "    user_propernoun_hash 列表型 50000  N/A  0%   [178, 417, 8381...]\n",
      "         push_title_hash 数值型 50000    8  0%                     7\n",
      "               title_len 数值型 50000   27  0%                    12\n",
      "          item_code_hash 列表型 50000  N/A  0% [6837, 3491, 8381...]\n",
      "        submit_type_hash 数值型 50000    3  0%                     6\n",
      "             tag_id_hash 列表型 50000  N/A  0%     [8139, 8993, 880]\n",
      "\n",
      "================================================================================\n",
      "📈 数值型特征统计表\n",
      "================================================================================\n",
      "             特征名  最小值  最大值       均值   中位数     标准差  25%分位  75%分位      偏度\n",
      "            hour    0   23  11.3420  10.0  6.6374    6.0   17.0  0.1431\n",
      "         weekday    0    6   3.2000   4.0  2.3150    1.0    5.0 -0.2436\n",
      "    country_hash    0  198 124.6826 145.0 47.3443   99.0  145.0 -1.0634\n",
      " push_title_hash    0    7   5.6323   7.0  2.2084    5.0    7.0 -1.3758\n",
      "       title_len    4   31  13.8898  14.0  3.9751   11.0   16.0  0.7432\n",
      "submit_type_hash    1    6   3.2370   4.0  1.9639    1.0    4.0  0.0338\n",
      "\n",
      "📊 数值型特征分布表 (前5个高频值)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "hour:\n",
      " 值   频次    占比\n",
      " 9 4108 8.22%\n",
      " 8 3697 7.39%\n",
      "10 2940 5.88%\n",
      "22 2751 5.50%\n",
      "21 2421 4.84%\n",
      "\n",
      "weekday:\n",
      " 值    频次     占比\n",
      " 1 10005 20.01%\n",
      " 4 10001 20.00%\n",
      " 6  9999 20.00%\n",
      " 5  9998 20.00%\n",
      " 0  9995 19.99%\n",
      "\n",
      "country_hash:\n",
      "  值    频次     占比\n",
      "145 17772 35.54%\n",
      "181  7335 14.67%\n",
      " 99  5182 10.36%\n",
      "106  3149  6.30%\n",
      "127  2342  4.68%\n",
      "\n",
      "push_title_hash:\n",
      " 值    频次     占比\n",
      " 7 32539 65.08%\n",
      " 2  2981  5.96%\n",
      " 5  2979  5.96%\n",
      " 3  2738  5.48%\n",
      " 6  2639  5.28%\n",
      "\n",
      "title_len:\n",
      " 值   频次     占比\n",
      "14 5890 11.78%\n",
      "15 5730 11.46%\n",
      "16 4850  9.70%\n",
      "10 4806  9.61%\n",
      "13 4450  8.90%\n",
      "\n",
      "================================================================================\n",
      "📋 列表型特征统计表\n",
      "================================================================================\n",
      "                     特征名 平均长度  最小长度  最大长度   总元素数  唯一元素数  元素重复率\n",
      "user_watch_stk_code_hash 5.00     5     5 250000   3612 98.56%\n",
      "    prefer_bid_code_hash 5.00     5     5 250000   1515 99.39%\n",
      "      hold_bid_code_hash 5.00     5     5 250000    829 99.67%\n",
      "    user_propernoun_hash 5.00     5     5 250000    876 99.65%\n",
      "          item_code_hash 5.00     5     5 250000    928 99.63%\n",
      "             tag_id_hash 3.00     3     3 150000     35 99.98%\n",
      "\n",
      "📏 列表长度分布表\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "user_watch_stk_code_hash:\n",
      " 长度   样本数      占比\n",
      "  5 50000 100.00%\n",
      "\n",
      "user_watch_stk_code_hash - 高频元素 (前8个):\n",
      "  元素   出现次数     占比\n",
      "8381 190585 76.23%\n",
      " 304   3009  1.20%\n",
      "1645    929  0.37%\n",
      "2299    898  0.36%\n",
      "6050    817  0.33%\n",
      "2290    749  0.30%\n",
      "5377    692  0.28%\n",
      "4996    683  0.27%\n",
      "\n",
      "prefer_bid_code_hash:\n",
      " 长度   样本数      占比\n",
      "  5 50000 100.00%\n",
      "\n",
      "prefer_bid_code_hash - 高频元素 (前8个):\n",
      "  元素   出现次数     占比\n",
      "8381 234507 93.80%\n",
      " 304    474  0.19%\n",
      "1863    379  0.15%\n",
      "9210    274  0.11%\n",
      "8853    220  0.09%\n",
      "5377    213  0.09%\n",
      "9919    195  0.08%\n",
      "5291    181  0.07%\n",
      "\n",
      "hold_bid_code_hash:\n",
      " 长度   样本数      占比\n",
      "  5 50000 100.00%\n",
      "\n",
      "hold_bid_code_hash - 高频元素 (前8个):\n",
      "  元素   出现次数     占比\n",
      "8381 243816 97.53%\n",
      " 304    183  0.07%\n",
      "9210     97  0.04%\n",
      "9830     87  0.03%\n",
      "1863     83  0.03%\n",
      "2291     76  0.03%\n",
      "4996     75  0.03%\n",
      "4693     69  0.03%\n",
      "\n",
      "user_propernoun_hash:\n",
      " 长度   样本数      占比\n",
      "  5 50000 100.00%\n",
      "\n",
      "user_propernoun_hash - 高频元素 (前8个):\n",
      "  元素   出现次数     占比\n",
      "8381 212013 84.81%\n",
      "1018   3121  1.25%\n",
      "9489   2588  1.04%\n",
      "7480   2531  1.01%\n",
      "1778   1788  0.72%\n",
      "6272   1703  0.68%\n",
      "2035   1222  0.49%\n",
      "2015    952  0.38%\n",
      "\n",
      "item_code_hash:\n",
      " 长度   样本数      占比\n",
      "  5 50000 100.00%\n",
      "\n",
      "item_code_hash - 高频元素 (前8个):\n",
      "  元素   出现次数     占比\n",
      "8381 208950 83.58%\n",
      "1863   2534  1.01%\n",
      "9210   2397  0.96%\n",
      "8853   1162  0.46%\n",
      "2299   1024  0.41%\n",
      " 304    891  0.36%\n",
      "1645    758  0.30%\n",
      "7995    568  0.23%\n",
      "\n",
      "tag_id_hash:\n",
      " 长度   样本数      占比\n",
      "  3 50000 100.00%\n",
      "\n",
      "tag_id_hash - 高频元素 (前8个):\n",
      "  元素  出现次数     占比\n",
      " 880 26034 17.36%\n",
      "8381 21313 14.21%\n",
      "4634 16513 11.01%\n",
      "4797 11601  7.73%\n",
      "2601 10150  6.77%\n",
      "8993  9951  6.63%\n",
      " 542  9722  6.48%\n",
      "8139  6013  4.01%\n",
      "\n",
      "================================================================================\n",
      "🔍 数据质量报告\n",
      "================================================================================\n",
      "                     特征名    完整性  缺失值数量 数据一致性 数据类型 是否可用\n",
      "                    hour 100.0%      0     低  数值型    ✅\n",
      "                 weekday 100.0%      0     低  数值型    ✅\n",
      "user_watch_stk_code_hash 100.0%      0     高  列表型    ✅\n",
      "            country_hash 100.0%      0     中  数值型    ✅\n",
      "    prefer_bid_code_hash 100.0%      0     高  列表型    ✅\n",
      "      hold_bid_code_hash 100.0%      0     高  列表型    ✅\n",
      "    user_propernoun_hash 100.0%      0     高  列表型    ✅\n",
      "         push_title_hash 100.0%      0     中  数值型    ✅\n",
      "               title_len 100.0%      0     中  数值型    ✅\n",
      "          item_code_hash 100.0%      0     高  列表型    ✅\n",
      "        submit_type_hash 100.0%      0     低  数值型    ✅\n",
      "             tag_id_hash 100.0%      0     高  列表型    ✅\n",
      "\n",
      "📋 总体评估:\n",
      "   • 特征总数: 12\n",
      "   • 可用特征: 12 (100.0%)\n",
      "   • 平均完整性: 100.0%\n",
      "   • 数据质量: 优秀\n",
      "\n",
      "✅ 特征分析完成!\n",
      "\n",
      "================================================================================\n",
      "🎉 特征分析完成！以上报告展示了所有特征的详细统计信息\n"
     ]
    }
   ],
   "source": [
    "run_comprehensive_feature_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08 树模型定义与训练评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def prepare_features(df_processed, processed_features, max_list_length=5):\n",
    "    \"\"\"展开列表特征\"\"\"\n",
    "    df_tree = df_processed[processed_features].copy()\n",
    "    \n",
    "    for feat in processed_features:\n",
    "        if isinstance(df_tree[feat].iloc[0], list):\n",
    "            expanded = df_tree[feat].apply(pd.Series).iloc[:, :max_list_length]\n",
    "            expanded.columns = [f\"{feat}_{i}\" for i in range(expanded.shape[1])]\n",
    "            df_tree = df_tree.drop(columns=[feat]).join(expanded)\n",
    "    \n",
    "    return df_tree\n",
    "\n",
    "def train_model(X, y, train_params):\n",
    "    \"\"\"训练LightGBM模型\"\"\"\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train, y_train)\n",
    "    val_data = lgb.Dataset(X_val, y_val, reference=train_data)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        train_params,\n",
    "        train_data,\n",
    "        num_boost_round=train_params.pop('num_iterations', 1000),\n",
    "        callbacks=[lgb.early_stopping(train_params.pop('early_stopping_rounds', 100))],\n",
    "        valid_sets=[train_data, val_data],\n",
    "        valid_names=['train', 'valid']\n",
    "    )\n",
    "    \n",
    "    return model, X_train, X_val, y_train, y_val\n",
    "\n",
    "def evaluate_model(model, X_train, X_val, y_train, y_val):\n",
    "    \"\"\"评估模型性能\"\"\"\n",
    "    y_train_pred = model.predict(X_train, num_iteration=model.best_iteration)\n",
    "    y_val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    \n",
    "    train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "    val_auc = roc_auc_score(y_val, y_val_pred)\n",
    "    \n",
    "    print(f\"训练集 AUC: {train_auc:.4f}\")\n",
    "    print(f\"验证集 AUC: {val_auc:.4f}\")\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'feature': model.feature_name(),\n",
    "        'importance': model.feature_importance(importance_type='gain')\n",
    "    }).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[300]\ttrain's auc: 0.863989\tvalid's auc: 0.847059\n",
      "训练集 AUC: 0.8640\n",
      "验证集 AUC: 0.8471\n",
      "\n",
      "特征重要性 (Top 20):\n",
      "                       feature     importance\n",
      "23      user_propernoun_hash_2  346906.059675\n",
      "21      user_propernoun_hash_0  116131.237506\n",
      "2                 country_hash   60277.225478\n",
      "22      user_propernoun_hash_1   55774.542730\n",
      "1                      weekday   24687.482483\n",
      "0                         hour    9143.383609\n",
      "5             submit_type_hash    5356.176477\n",
      "10  user_watch_stk_code_hash_4    4607.537248\n",
      "3              push_title_hash    3455.920555\n",
      "6   user_watch_stk_code_hash_0    3359.792433\n",
      "4                    title_len    3198.852174\n",
      "7   user_watch_stk_code_hash_1    2804.459620\n",
      "8   user_watch_stk_code_hash_2    2645.200194\n",
      "33               tag_id_hash_2    2603.419945\n",
      "9   user_watch_stk_code_hash_3    2591.612250\n",
      "32               tag_id_hash_1    2223.176195\n",
      "11      prefer_bid_code_hash_0    2154.012811\n",
      "31               tag_id_hash_0    2150.902098\n",
      "27            item_code_hash_1    2011.556514\n",
      "26            item_code_hash_0    1682.390932\n"
     ]
    }
   ],
   "source": [
    "# 主流程\n",
    "with open('config/config.yml', 'r', encoding='utf-8') as f:\n",
    "    train_config = yaml.safe_load(f)\n",
    "\n",
    "if 'log_type' in df_processed.columns:\n",
    "    # 准备数据\n",
    "    df_processed['label'] = df_processed['log_type'].apply(lambda x: 1 if x == 'PC' else 0)\n",
    "    X = prepare_features(df_processed, processed_features)\n",
    "    y = df_processed['label']\n",
    "    \n",
    "    # 训练模型\n",
    "    train_params = {**train_config['train'], 'verbose': -1, 'n_jobs': -1, 'seed': 42}\n",
    "    model, X_train, X_val, y_train, y_val = train_model(X, y, train_params)\n",
    "    \n",
    "    # 评估并输出结果\n",
    "    feature_importance = evaluate_model(model, X_train, X_val, y_train, y_val)\n",
    "    print(\"\\n特征重要性 (Top 20):\")\n",
    "    print(feature_importance.head(20))\n",
    "else:\n",
    "    print(\"错误: 找不到 'log_type' 列，无法进行模型训练。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09 深度模型全流程：类似Huggig Face的框架 这里用的是tensorflow的框架"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

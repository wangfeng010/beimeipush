{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç‰¹å¾å·¥ç¨‹æµæ°´çº¿æ·±åº¦è§£æ\n",
    "\n",
    "æœ¬ Notebook æ—¨åœ¨é€æ­¥åˆ†è§£å¹¶å±•ç¤ºé¡¹ç›®ä¸­ä»åŸå§‹æ•°æ®åˆ°ç‰¹å¾ç”Ÿæˆçš„å®Œæ•´ç‰¹å¾å·¥ç¨‹æµç¨‹ã€‚\n",
    "\n",
    "## æ¶æ„æ¦‚è¿°\n",
    "\n",
    "```\n",
    "Raw Data (CSV) â†’ YAML Config â†’ UniProcess Operations â†’ Final Features\n",
    "     â†“              â†“              â†“                    â†“\n",
    "data/train/    config/feat.yml   env/UniProcess/    Processed DataFrame\n",
    "```\n",
    "\n",
    "## æµç¨‹æ­¥éª¤\n",
    "\n",
    "1. **åŠ è½½åŸå§‹æ•°æ®**ï¼šä» `data/train/` ç›®å½•åŠ è½½æ ·æœ¬æ•°æ®\n",
    "2. **è§£æ YAML é…ç½®**ï¼šè¯»å– `config/feat.yml` å¹¶è§£æä¸ºç»“æ„åŒ–å¯¹è±¡\n",
    "3. **æ„å»ºæ“ä½œä¸­å¿ƒ**ï¼šåˆ›å»º OP_HUB æ˜ å°„è¡¨\n",
    "4. **æ‰§è¡Œç‰¹å¾å·¥ç¨‹**ï¼šé€æ­¥åº”ç”¨æ¯ä¸ªæ“ä½œ\n",
    "5. **æŸ¥çœ‹æœ€ç»ˆç»“æœ**ï¼šåˆ†æå¤„ç†åçš„ç‰¹å¾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 1: ç¯å¢ƒè®¾ç½®ä¸å¯¼å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸºç¡€åº“å¯¼å…¥å®Œæˆ\n",
      "å·²æ·»åŠ ç¯å¢ƒè·¯å¾„: /Users/main/Documents/02æ¨èç®—æ³•/02åŒèŠ±é¡ºå®ä¹ /02ainvest-push-recall-group-master_wf/env\n",
      "\n",
      "ç¯å¢ƒè®¾ç½®å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "from hashlib import md5\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from glob import glob\n",
    "\n",
    "print(\"åŸºç¡€åº“å¯¼å…¥å®Œæˆ\")\n",
    "\n",
    "# æ·»åŠ é¡¹ç›®è·¯å¾„\n",
    "project_root = os.getcwd()\n",
    "env_path = os.path.join(project_root, 'env')\n",
    "if env_path not in sys.path:\n",
    "    sys.path.insert(0, env_path)\n",
    "    print(f\"å·²æ·»åŠ ç¯å¢ƒè·¯å¾„: {env_path}\")\n",
    "\n",
    "print(\"\\nç¯å¢ƒè®¾ç½®å®Œæˆï¼\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## æ­¥éª¤ 2: åŠ è½½åŸå§‹æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 1800001088\n",
      "create_time: 2025-05-31 08:39:07\n",
      "log_type: PR\n",
      "watchlists: nan\n",
      "holdings: nan\n",
      "country: Germany\n",
      "prefer_bid: nan\n",
      "user_propernoun: germany#3.06|mid-america#1.02\n",
      "push_title: Ainvest Newswire\n",
      "push_content: Hims & Hers Health Lays Off 4% of Staff Amid Strategy Shift\n",
      "item_code: [{\"market\":\"169\",\"score\":0,\"code\":\"HIMS\",\"tagId\":\"U000012934\",\"name\":\"Hims & Hers Health\",\"type\":0,\"parentId\":\"US_ROBOT0f37d7fd3fca6a41\"},{\"market\":\"169\",\"score\":0,\"code\":\"NVO\",\"tagId\":\"U000002999\",\"name\":\"Novo Nordisk\",\"type\":0,\"parentId\":\"US_ROBOT0f37d7fd3fca6a41\"}]\n",
      "item_tags: [{\"score\":0.7803922295570374,\"tagId\":\"51510\",\"name\":\"us_high_importance\",\"type\":4,\"parentId\":\"US_ROBOT0f37d7fd3fca6a41\"},{\"score\":0.7803922295570374,\"tagId\":\"57967\",\"name\":\"Fusion\",\"type\":4,\"parentId\":\"US_ROBOT0f37d7fd3fca6a41\"},{\"tagId\":\"1002\",\"name\":\"no_penny_stock\",\"type\":4,\"parentId\":\"US_ROBOT0f37d7fd3fca6a41\"}]\n",
      "submit_type: autoFlash\n"
     ]
    }
   ],
   "source": [
    "data_path = 'data/train/*.csv'\n",
    "# ä½¿ç”¨globè·å–æ‰€æœ‰åŒ¹é…çš„CSVæ–‡ä»¶è·¯å¾„\n",
    "csv_files = glob(data_path)\n",
    "if not csv_files:\n",
    "    raise ValueError(f\"No CSV files found in {data_path}\")\n",
    "\n",
    "# è¯»å–å¹¶åˆå¹¶æ‰€æœ‰CSVæ–‡ä»¶\n",
    "df_raw = pd.concat([pd.read_csv(f) for f in csv_files], ignore_index=True)\n",
    "for col in df_raw.columns: print(f\"{col}: {df_raw[col].iloc[0]}\") # ç›´è§‚å±•ç¤ºå„åˆ—çš„å…·ä½“æ•°æ®çš„æ ·å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å„åˆ—æ•°æ®æ ·ä¾‹:\n",
      "user_id: 1800001088\n",
      "create_time: 2025-05-31 08:39:07\n",
      "log_type: PR\n",
      "watchlists: nan\n",
      "holdings: nan\n",
      "country: Germany\n",
      "prefer_bid: nan\n",
      "user_propernoun: germany#3.06|mid-america#1.02\n",
      "push_title: Ainvest Newswire\n",
      "push_content: Hims & Hers Health Lays Off 4% of Staff Amid Strategy Shift\n",
      "item_code: [{\"market\":\"169\",\"score\":0,\"code\":\"HIMS\",\"tagId\":\"U000012934\",\"name\":\"Hims & Hers Health\",\"type\":0,\"parentId\":\"US_ROBOT0f37d7fd3fca6a41\"},{\"market\":\"169\",\"score\":0,\"code\":\"NVO\",\"tagId\":\"U000002999\",\"name\":\"Novo Nordisk\",\"type\":0,\"parentId\":\"US_ROBOT0f37d7fd3fca6a41\"}]\n",
      "item_tags: [{\"score\":0.7803922295570374,\"tagId\":\"51510\",\"name\":\"us_high_importance\",\"type\":4,\"parentId\":\"US_ROBOT0f37d7fd3fca6a41\"},{\"score\":0.7803922295570374,\"tagId\":\"57967\",\"name\":\"Fusion\",\"type\":4,\"parentId\":\"US_ROBOT0f37d7fd3fca6a41\"},{\"tagId\":\"1002\",\"name\":\"no_penny_stock\",\"type\":4,\"parentId\":\"US_ROBOT0f37d7fd3fca6a41\"}]\n",
      "submit_type: autoFlash\n"
     ]
    }
   ],
   "source": [
    "# æŸ¥çœ‹å…·ä½“æ•°æ®æ ·ä¾‹\n",
    "print(\"å„åˆ—æ•°æ®æ ·ä¾‹:\")\n",
    "for col in df_raw.columns:\n",
    "    print(f\"{col}: {df_raw[col].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## æ­¥éª¤ 3: åŠ è½½å¹¶è§£æ YAML é…ç½®\n",
    "è¿™æ˜¯ç‰¹å¾å·¥ç¨‹çš„\"è“å›¾\"ï¼Œå®šä¹‰äº†æ¯ä¸ªç‰¹å¾å¦‚ä½•ä»åŸå§‹æ•°æ®ä¸­æå–å’Œè½¬æ¢ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exclude_features': {'current': 'default', 'default': [], 'exclude_user_behavior': ['user_watch_stk_code', 'prefer_bid_code', 'hold_bid_code', 'user_propernoun'], 'exclude_user_propernoun': ['user_propernoun']}, 'pipelines': [{'embedding_dim': 8, 'feat_name': 'hour', 'feat_type': 'sparse', 'input_sample': '2024-08-02 00:44:05', 'operations': [{'col_in': 'create_time', 'col_out': 'create_time', 'func_name': 'fillna', 'func_parameters': {'na_value': '2024-08-02 00:16:34'}}, {'col_in': 'create_time', 'col_out': 'hour', 'func_name': 'to_hour', 'func_parameters': {}}], 'vocabulary_size': 24}, {'embedding_dim': 8, 'feat_name': 'weekday', 'feat_type': 'sparse', 'input_sample': '2024-08-02 00:44:05', 'operations': [{'col_in': 'create_time', 'col_out': 'weekday', 'func_name': 'to_weekday', 'func_parameters': {}}], 'vocabulary_size': 7}, {'embedding_dim': 8, 'feat_name': 'user_watch_stk_code_hash', 'feat_type': 'varlen_sparse', 'input_sample': 'AAPL_185 & TSLA_185', 'operations': [{'col_in': 'watchlists', 'col_out': 'watchlists', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null_0 & null_0'}}, {'col_in': 'watchlists', 'col_out': 'watchlists', 'func_name': 'split', 'func_parameters': {'sep': ' & '}}, {'col_in': 'watchlists', 'col_out': 'watchlists', 'func_name': 'seperation', 'func_parameters': {'sep': '_'}}, {'col_in': 'watchlists', 'col_out': 'user_watch_stk_code', 'func_name': 'list_get', 'func_parameters': {'item_index': 0}}, {'col_in': 'user_watch_stk_code', 'col_out': 'user_watch_stk_code', 'func_name': 'remove_items', 'func_parameters': {'target_values': ['AAPL', 'AMZN', 'GOOGL', 'TSLA']}}, {'col_in': 'user_watch_stk_code', 'col_out': 'user_watch_stk_code', 'func_name': 'padding', 'func_parameters': {'max_len': 5, 'pad_value': 'null'}}, {'col_in': 'user_watch_stk_code', 'col_out': 'user_watch_stk_code_hash', 'func_name': 'list_hash', 'func_parameters': {'vocabulary_size': 10000}}], 'vocabulary_size': 10000}, {'embedding_dim': 8, 'feat_name': 'country_hash', 'feat_type': 'sparse', 'input_sample': 'United States', 'operations': [{'col_in': 'country', 'col_out': 'country', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null'}}, {'col_in': 'country', 'col_out': 'country_hash', 'func_name': 'str_hash', 'func_parameters': {'vocabulary_size': 200}}], 'vocabulary_size': 200}, {'embedding_dim': 8, 'feat_name': 'prefer_bid_code_hash', 'feat_type': 'varlen_sparse', 'input_sample': 'AAPL#0.24809|AMZN#0.24809|GOOGL#0.24809|TSLA#0.24809', 'operations': [{'col_in': 'prefer_bid', 'col_out': 'prefer_bid', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null#0'}}, {'col_in': 'prefer_bid', 'col_out': 'prefer_bid', 'func_name': 'split', 'func_parameters': {'sep': '|'}}, {'col_in': 'prefer_bid', 'col_out': 'prefer_bid', 'func_name': 'seperation', 'func_parameters': {'sep': '#'}}, {'col_in': 'prefer_bid', 'col_out': 'prefer_bid_code', 'func_name': 'list_get', 'func_parameters': {'item_index': 0}}, {'col_in': 'prefer_bid_code', 'col_out': 'prefer_bid_code', 'func_name': 'padding', 'func_parameters': {'max_len': 5, 'pad_value': 'null'}}, {'col_in': 'prefer_bid_code', 'col_out': 'prefer_bid_code_hash', 'func_name': 'list_hash', 'func_parameters': {'vocabulary_size': 10000}}], 'vocabulary_size': 10000}, {'embedding_dim': 8, 'feat_name': 'hold_bid_code_hash', 'feat_type': 'varlen_sparse', 'input_sample': 'JD,185|BAC,169|TSLA,185', 'operations': [{'col_in': 'holdings', 'col_out': 'holdings', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null,0'}}, {'col_in': 'holdings', 'col_out': 'holdings', 'func_name': 'split', 'func_parameters': {'sep': '|'}}, {'col_in': 'holdings', 'col_out': 'holdings', 'func_name': 'seperation', 'func_parameters': {'sep': ','}}, {'col_in': 'holdings', 'col_out': 'hold_bid_code', 'func_name': 'list_get', 'func_parameters': {'item_index': 0}}, {'col_in': 'hold_bid_code', 'col_out': 'hold_bid_code', 'func_name': 'padding', 'func_parameters': {'max_len': 5, 'pad_value': 'null'}}, {'col_in': 'hold_bid_code', 'col_out': 'hold_bid_code_hash', 'func_name': 'list_hash', 'func_parameters': {'vocabulary_size': 10000}}], 'vocabulary_size': 10000}, {'embedding_dim': 8, 'feat_name': 'user_propernoun_hash', 'feat_type': 'varlen_sparse', 'input_sample': 'apple#1.02|nike#1.02', 'operations': [{'col_in': 'user_propernoun', 'col_out': 'user_propernoun', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null#0'}}, {'col_in': 'user_propernoun', 'col_out': 'user_propernoun', 'func_name': 'split', 'func_parameters': {'sep': '|'}}, {'col_in': 'user_propernoun', 'col_out': 'user_propernoun', 'func_name': 'seperation', 'func_parameters': {'sep': '#'}}, {'col_in': 'user_propernoun', 'col_out': 'user_propernoun_code', 'func_name': 'list_get', 'func_parameters': {'item_index': 0}}, {'col_in': 'user_propernoun_code', 'col_out': 'user_propernoun_code', 'func_name': 'padding', 'func_parameters': {'max_len': 5, 'pad_value': 'null'}}, {'col_in': 'user_propernoun_code', 'col_out': 'user_propernoun_hash', 'func_name': 'list_hash', 'func_parameters': {'vocabulary_size': 10000}}], 'vocabulary_size': 10000}, {'embedding_dim': 8, 'feat_name': 'push_title_hash', 'feat_type': 'sparse', 'input_sample': 'Breaking News', 'operations': [{'col_in': 'push_title', 'col_out': 'push_title', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null'}}, {'col_in': 'push_title', 'col_out': 'push_title_hash', 'func_name': 'str_hash', 'func_parameters': {'vocabulary_size': 8}}], 'vocabulary_size': 8}, {'embedding_dim': 8, 'feat_name': 'title_len', 'feat_type': 'sparse', 'input_sample': \"Hong Kong's Leading Broker Futu Securities Introduces Bitcoin and XRP Trading with Lucrative Incentives\", 'operations': [{'col_in': 'push_content', 'col_out': 'push_content', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null'}}, {'col_in': 'push_content', 'col_out': 'push_content', 'func_name': 'split', 'func_parameters': {'sep': ' '}}, {'col_in': 'push_content', 'col_out': 'title_len', 'func_name': 'list_len', 'func_parameters': {}}, {'col_in': 'title_len', 'col_out': 'title_len', 'func_name': 'int_max', 'func_parameters': {'max_value': 31}}], 'vocabulary_size': 32}, {'embedding_dim': 8, 'feat_name': 'item_code_hash', 'feat_type': 'varlen_sparse', 'input_sample': '[{\"market\":\"185\",\"score\":1,\"code\":\"META\",\"name\":\"Meta\",\"type\":0,\"parentId\":\"0339437d07195361\"}]', 'operations': [{'col_in': 'item_code', 'col_out': 'item_code', 'func_name': 'fillna', 'func_parameters': {'na_value': '[{\"market\":\"null\",\"score\":0,\"code\":\"null\",\"name\":\"null\",\"type\":\"null\",\"parentId\":\"null\"}]'}}, {'col_in': 'item_code', 'col_out': 'item_code', 'func_name': 'json_object_to_list', 'func_parameters': {'key': 'code'}}, {'col_in': 'item_code', 'col_out': 'item_code', 'func_name': 'padding', 'func_parameters': {'max_len': 5, 'pad_value': 'null'}}, {'col_in': 'item_code', 'col_out': 'item_code_hash', 'func_name': 'list_hash', 'func_parameters': {'vocabulary_size': 10000}}], 'vocabulary_size': 10000}, {'embedding_dim': 8, 'feat_name': 'submit_type_hash', 'feat_type': 'sparse', 'input_sample': 'auto_flash', 'operations': [{'col_in': 'submit_type', 'col_out': 'submit_type', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null'}}, {'col_in': 'submit_type', 'col_out': 'submit_type_hash', 'func_name': 'str_hash', 'func_parameters': {'vocabulary_size': 10}}], 'vocabulary_size': 10}, {'embedding_dim': 8, 'feat_name': 'tag_id_hash', 'feat_type': 'varlen_sparse', 'input_sample': '[{\"score\":0,\"tagId\":\"57967\",\"name\":\"Fusion\",\"type\":4,\"parentId\":\"0339437d07195361\"}]', 'operations': [{'col_in': 'item_tags', 'col_out': 'item_tags', 'func_name': 'fillna', 'func_parameters': {'na_value': '[{\"score\":0,\"tagId\":\"null\",\"name\":\"null\",\"type\":0,\"parentId\":\"null\"}]'}}, {'col_in': 'item_tags', 'col_out': 'tagIds', 'func_name': 'json_object_to_list', 'func_parameters': {'key': 'tagId'}}, {'col_in': 'tagIds', 'col_out': 'tagIds', 'func_name': 'padding', 'func_parameters': {'max_len': 3, 'pad_value': 'null'}}, {'col_in': 'tagIds', 'col_out': 'tag_id_hash', 'func_name': 'list_hash', 'func_parameters': {'vocabulary_size': 10000}}], 'vocabulary_size': 10000}]}\n"
     ]
    }
   ],
   "source": [
    "feat_config_path = 'config/feat.yml'\n",
    "\n",
    "with open(feat_config_path, 'r', encoding='utf-8') as f:\n",
    "    feat_config = yaml.safe_load(f)\n",
    "\n",
    "print(feat_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç¤ºä¾‹æµæ°´çº¿: hour\n",
      "{'embedding_dim': 8,\n",
      " 'feat_name': 'hour',\n",
      " 'feat_type': 'sparse',\n",
      " 'input_sample': '2024-08-02 00:44:05',\n",
      " 'operations': [{'col_in': 'create_time',\n",
      "                 'col_out': 'create_time',\n",
      "                 'func_name': 'fillna',\n",
      "                 'func_parameters': {'na_value': '2024-08-02 00:16:34'}},\n",
      "                {'col_in': 'create_time',\n",
      "                 'col_out': 'hour',\n",
      "                 'func_name': 'to_hour',\n",
      "                 'func_parameters': {}}],\n",
      " 'vocabulary_size': 24}\n"
     ]
    }
   ],
   "source": [
    "# æŸ¥çœ‹ä¸€ä¸ªå®Œæ•´çš„æµæ°´çº¿é…ç½®ç¤ºä¾‹\n",
    "example_pipeline = feat_config['pipelines'][0]  # å–ç¬¬ä¸€ä¸ªæµæ°´çº¿\n",
    "print(f\"ç¤ºä¾‹æµæ°´çº¿: {example_pipeline['feat_name']}\")\n",
    "pprint(example_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## æ­¥éª¤ 4: å®šä¹‰ç‰¹å¾æ“ä½œå‡½æ•°\n",
    "\n",
    "ç”±äºæˆ‘ä»¬æ— æ³•ç›´æ¥å¯¼å…¥ UniProcess åº“ï¼Œæˆ‘ä»¬å°†æ‰‹åŠ¨å®ç°ä¸€äº›å…³é”®çš„ç‰¹å¾æ“ä½œå‡½æ•°ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰ç¼ºå¤±å€¼å¸¸é‡\n",
    "MISSING_VALUE = [None, '', 'null', 'NULL', 'None', np.nan]\n",
    "\n",
    "def fillna(x: Union[float, int, str], na_value: Union[float, int, str]) -> Union[float, int, str]:\n",
    "    \"\"\"å¡«å……ç¼ºå¤±å€¼\"\"\"\n",
    "    if x in MISSING_VALUE or (isinstance(x, float) and pd.isna(x)):\n",
    "        return na_value\n",
    "    return x\n",
    "\n",
    "def split(x: str, sep: str) -> List[str]:\n",
    "    \"\"\"å­—ç¬¦ä¸²åˆ†å‰²\"\"\"\n",
    "    return str(x).split(sep)\n",
    "\n",
    "def seperation(x: List[str], sep: str) -> List[List[str]]:\n",
    "    \"\"\"åˆ—è¡¨å…ƒç´ äºŒæ¬¡åˆ†å‰²\"\"\"\n",
    "    if not isinstance(x, list):\n",
    "        return []\n",
    "    return [item.split(sep) for item in x]\n",
    "\n",
    "def list_get(x: List[List[Any]], item_index: int) -> List[Any]:\n",
    "    \"\"\"è·å–åµŒå¥—åˆ—è¡¨ä¸­æŒ‡å®šä½ç½®çš„å…ƒç´ \"\"\"\n",
    "    if not isinstance(x, list):\n",
    "        return []\n",
    "    result = []\n",
    "    for sublist in x:\n",
    "        if isinstance(sublist, list) and len(sublist) > item_index:\n",
    "            result.append(sublist[item_index])\n",
    "        else:\n",
    "            result.append('null')\n",
    "    return result\n",
    "\n",
    "def remove_items(x: List[str], target_values: List[str]) -> List[str]:\n",
    "    \"\"\"ç§»é™¤åˆ—è¡¨ä¸­çš„æŒ‡å®šå…ƒç´ \"\"\"\n",
    "    if not isinstance(x, list):\n",
    "        return []\n",
    "    return [item for item in x if item not in target_values]\n",
    "\n",
    "def padding(x: List[Any], pad_value: Union[str, float, int], max_len: int) -> List[Any]:\n",
    "    \"\"\"åˆ—è¡¨å¡«å……åˆ°æŒ‡å®šé•¿åº¦\"\"\"\n",
    "    if not isinstance(x, list):\n",
    "        x = []\n",
    "    if len(x) >= max_len:\n",
    "        return x[:max_len]\n",
    "    else:\n",
    "        return x + [pad_value] * (max_len - len(x))\n",
    "\n",
    "def list_hash(x: List[str], vocabulary_size: int) -> List[int]:\n",
    "    \"\"\"å¯¹åˆ—è¡¨ä¸­æ¯ä¸ªå…ƒç´ è¿›è¡Œå“ˆå¸Œ\"\"\"\n",
    "    if not isinstance(x, list):\n",
    "        return []\n",
    "    result = []\n",
    "    for item in x:\n",
    "        hash_val = int(md5(str(item).encode()).hexdigest(), 16) % vocabulary_size\n",
    "        result.append(hash_val)\n",
    "    return result\n",
    "\n",
    "def str_hash(x: str, vocabulary_size: int) -> int:\n",
    "    \"\"\"å­—ç¬¦ä¸²å“ˆå¸Œ\"\"\"\n",
    "    return int(md5(str(x).encode()).hexdigest(), 16) % vocabulary_size\n",
    "\n",
    "def to_hour(x: str) -> int:\n",
    "    \"\"\"æå–æ—¶é—´ä¸­çš„å°æ—¶\"\"\"\n",
    "    try:\n",
    "        dt = pd.to_datetime(x)\n",
    "        return dt.hour\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def to_weekday(x: str) -> int:\n",
    "    \"\"\"æå–æ—¶é—´ä¸­çš„æ˜ŸæœŸ\"\"\"\n",
    "    try:\n",
    "        dt = pd.to_datetime(x)\n",
    "        return dt.weekday()\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def list_len(x: List) -> int:\n",
    "    \"\"\"åˆ—è¡¨é•¿åº¦\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return len(x)\n",
    "    return 0\n",
    "\n",
    "def int_max(x: int, max_value: int) -> int:\n",
    "    \"\"\"é™åˆ¶æ•´æ•°æœ€å¤§å€¼\"\"\"\n",
    "    return min(int(x), max_value)\n",
    "\n",
    "def json_object_to_list(x: str, key: str) -> List[str]:\n",
    "    \"\"\"ä»JSONå¯¹è±¡åˆ—è¡¨ä¸­æå–æŒ‡å®šé”®çš„å€¼\"\"\"\n",
    "    try:\n",
    "        data = json.loads(x)\n",
    "        if isinstance(data, list):\n",
    "            return [item.get(key, 'null') for item in data if isinstance(item, dict)]\n",
    "        return ['null']\n",
    "    except:\n",
    "        return ['null']\n",
    "\n",
    "def map_to_int(x: Union[str, List], map_dict: Dict[str, int], default_code: int = 0) -> Union[List[int], int]:\n",
    "    \"\"\"æ˜ å°„åˆ°æ•´æ•°\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return [map_dict.get(item, default_code) for item in x]\n",
    "    else:\n",
    "        return map_dict.get(str(x), default_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OP_HUB æ„å»ºå®Œæˆï¼ŒåŒ…å« 14 ä¸ªæ“ä½œå‡½æ•°\n",
      "å¯ç”¨å‡½æ•°: ['fillna', 'split', 'seperation', 'list_get', 'remove_items', 'padding', 'list_hash', 'str_hash', 'to_hour', 'to_weekday', 'list_len', 'int_max', 'json_object_to_list', 'map_to_int']\n"
     ]
    }
   ],
   "source": [
    "# æ„å»ºæ“ä½œä¸­å¿ƒ (OP_HUB)\n",
    "OP_HUB = {\n",
    "    'fillna': fillna,\n",
    "    'split': split,\n",
    "    'seperation': seperation,\n",
    "    'list_get': list_get,\n",
    "    'remove_items': remove_items,\n",
    "    'padding': padding,\n",
    "    'list_hash': list_hash,\n",
    "    'str_hash': str_hash,\n",
    "    'to_hour': to_hour,\n",
    "    'to_weekday': to_weekday,\n",
    "    'list_len': list_len,\n",
    "    'int_max': int_max,\n",
    "    'json_object_to_list': json_object_to_list,\n",
    "    'map_to_int': map_to_int\n",
    "}\n",
    "\n",
    "print(f\"OP_HUB æ„å»ºå®Œæˆï¼ŒåŒ…å« {len(OP_HUB)} ä¸ªæ“ä½œå‡½æ•°\")\n",
    "print(f\"å¯ç”¨å‡½æ•°: {list(OP_HUB.keys())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## æ­¥éª¤ 5: å®ç°ç‰¹å¾å·¥ç¨‹æ‰§è¡Œå¼•æ“\n",
    "\n",
    "è¿™æ˜¯æ ¸å¿ƒéƒ¨åˆ†ï¼šæˆ‘ä»¬å°†å®ç° `run_one_op` å‡½æ•°æ¥æ‰§è¡Œå•ä¸ªæ“ä½œã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_op(df: pd.DataFrame, operation: dict) -> pd.DataFrame:\n",
    "    \"\"\"æ‰§è¡Œå•ä¸ªç‰¹å¾æ“ä½œ\"\"\"\n",
    "    # è·å–æ“ä½œé…ç½®\n",
    "    col_in = operation['col_in']\n",
    "    col_out = operation['col_out']\n",
    "    func_name = operation['func_name']\n",
    "    parameters = operation.get('func_parameters', {})\n",
    "    \n",
    "    # æ£€æŸ¥å‡½æ•°æ˜¯å¦å­˜åœ¨\n",
    "    if func_name not in OP_HUB:\n",
    "        return df\n",
    "    \n",
    "    # æ£€æŸ¥è¾“å…¥åˆ—æ˜¯å¦å­˜åœ¨\n",
    "    input_cols = [col_in] if isinstance(col_in, str) else col_in\n",
    "    if not all(col in df.columns for col in input_cols):\n",
    "        return df\n",
    "    \n",
    "    # å‡†å¤‡ç‰¹å¾è½¬æ¢å‡½æ•°\n",
    "    transform_func = partial(OP_HUB[func_name], **parameters)\n",
    "    \n",
    "    # æ‰§è¡Œç‰¹å¾è½¬æ¢\n",
    "    if isinstance(col_in, list):\n",
    "        df[col_out] = df[col_in].apply(lambda row: transform_func(*row), axis=1)\n",
    "    else:\n",
    "        df[col_out] = df[col_in].apply(transform_func)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## æ­¥éª¤ 6: æ‰§è¡Œå®Œæ•´çš„ç‰¹å¾å·¥ç¨‹æµæ°´çº¿\n",
    "\n",
    "ç°åœ¨æˆ‘ä»¬å°†éå†æ‰€æœ‰çš„ç‰¹å¾æµæ°´çº¿ï¼Œé€ä¸€æ‰§è¡Œæ¯ä¸ªæ“ä½œã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_feature_pipelines(df_raw: pd.DataFrame, feat_config: dict) -> tuple[pd.DataFrame, list]:\n",
    "    \"\"\"æ‰§è¡Œç‰¹å¾å·¥ç¨‹æµæ°´çº¿\"\"\"\n",
    "    # åˆ›å»ºæ•°æ®å‰¯æœ¬\n",
    "    df = df_raw.copy()\n",
    "    \n",
    "    # è·å–éœ€è¦å¤„ç†çš„æµæ°´çº¿\n",
    "    pipelines = feat_config['pipelines']\n",
    "\n",
    "    # è®°å½•æˆåŠŸå¤„ç†çš„ç‰¹å¾\n",
    "    processed_features = []\n",
    "    \n",
    "    # æ‰§è¡Œæ¯ä¸ªç‰¹å¾å¤„ç†æµæ°´çº¿\n",
    "    for pipeline in pipelines:\n",
    "        feat_name = pipeline['feat_name']\n",
    "        operations = pipeline['operations']\n",
    "        \n",
    "        # æ‰§è¡Œæµæ°´çº¿ä¸­çš„æ¯ä¸ªæ“ä½œ\n",
    "        for operation in operations:\n",
    "            df = run_one_op(df, operation)\n",
    "\n",
    "        # è®°å½•å¤„ç†æˆåŠŸçš„ç‰¹å¾\n",
    "        processed_features.append(feat_name)\n",
    "    \n",
    "    return df,processed_features\n",
    "\n",
    "# å¤„ç†ç‰¹å¾\n",
    "df_processed, processed_features = process_feature_pipelines(df_raw, feat_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## æ­¥éª¤ 7: åˆ†æå¤„ç†ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•°æ®ç»“æ„å¯¹æ¯”:\n",
      "åŸå§‹åˆ—æ•°: 13\n",
      "å¤„ç†ååˆ—æ•°: 30\n",
      "æ–°å¢åˆ—æ•°: 17\n",
      "\n",
      "åŸå§‹åˆ—å:\n",
      "['user_id', 'create_time', 'log_type', 'watchlists', 'holdings', 'country', 'prefer_bid', 'user_propernoun', 'push_title', 'push_content', 'item_code', 'item_tags', 'submit_type']\n",
      "\n",
      "æ–°å¢åˆ—å:\n",
      "['hour', 'weekday', 'user_watch_stk_code', 'user_watch_stk_code_hash', 'country_hash', 'prefer_bid_code', 'prefer_bid_code_hash', 'hold_bid_code', 'hold_bid_code_hash', 'user_propernoun_code', 'user_propernoun_hash', 'push_title_hash', 'title_len', 'item_code_hash', 'submit_type_hash', 'tagIds', 'tag_id_hash']\n"
     ]
    }
   ],
   "source": [
    "# æ¯”è¾ƒå¤„ç†å‰åçš„æ•°æ®ç»“æ„\n",
    "print(\"æ•°æ®ç»“æ„å¯¹æ¯”:\")\n",
    "print(f\"åŸå§‹åˆ—æ•°: {len(df_raw.columns)}\")\n",
    "print(f\"å¤„ç†ååˆ—æ•°: {len(df_processed.columns)}\")\n",
    "print(f\"æ–°å¢åˆ—æ•°: {len(df_processed.columns) - len(df_raw.columns)}\")\n",
    "\n",
    "print(\"\\nåŸå§‹åˆ—å:\")\n",
    "print(list(df_raw.columns))\n",
    "\n",
    "print(\"\\næ–°å¢åˆ—å:\")\n",
    "new_columns = [col for col in df_processed.columns if col not in df_raw.columns]\n",
    "print(new_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆåŠŸç”Ÿæˆçš„ç‰¹å¾è¯¦æƒ…:\n",
      "  hour: int64 = 8\n",
      "  weekday: int64 = 5\n",
      "  user_watch_stk_code_hash: list = [8381, 8381, 8381, 8381, 8381]\n",
      "  country_hash: int64 = 71\n",
      "  prefer_bid_code_hash: list = [8381, 8381, 8381, 8381, 8381]\n",
      "  hold_bid_code_hash: list = [8381, 8381, 8381, 8381, 8381]\n",
      "  user_propernoun_hash: list = [178, 417, 8381, 8381, 8381]\n",
      "  push_title_hash: int64 = 7\n",
      "  title_len: int64 = 12\n",
      "  item_code_hash: list = [6837, 3491, 8381, 8381, 8381]\n",
      "  submit_type_hash: int64 = 6\n",
      "  tag_id_hash: list = [8139, 8993, 880]\n"
     ]
    }
   ],
   "source": [
    "# æŸ¥çœ‹æˆåŠŸç”Ÿæˆçš„ç‰¹å¾\n",
    "print(\"æˆåŠŸç”Ÿæˆçš„ç‰¹å¾è¯¦æƒ…:\")\n",
    "for feat_name in processed_features:\n",
    "    if feat_name in df_processed.columns:\n",
    "        sample_data = df_processed[feat_name].iloc[0]\n",
    "        data_type = type(sample_data).__name__\n",
    "        print(f\"  {feat_name}: {data_type} = {sample_data}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æœ€ç»ˆå¤„ç†ç»“æœé¢„è§ˆ:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>log_type</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "      <th>user_watch_stk_code_hash</th>\n",
       "      <th>country_hash</th>\n",
       "      <th>prefer_bid_code_hash</th>\n",
       "      <th>hold_bid_code_hash</th>\n",
       "      <th>user_propernoun_hash</th>\n",
       "      <th>push_title_hash</th>\n",
       "      <th>title_len</th>\n",
       "      <th>item_code_hash</th>\n",
       "      <th>submit_type_hash</th>\n",
       "      <th>tag_id_hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1800001088</td>\n",
       "      <td>PR</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>[8381, 8381, 8381, 8381, 8381]</td>\n",
       "      <td>71</td>\n",
       "      <td>[8381, 8381, 8381, 8381, 8381]</td>\n",
       "      <td>[8381, 8381, 8381, 8381, 8381]</td>\n",
       "      <td>[178, 417, 8381, 8381, 8381]</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>[6837, 3491, 8381, 8381, 8381]</td>\n",
       "      <td>6</td>\n",
       "      <td>[8139, 8993, 880]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1800001417</td>\n",
       "      <td>PR</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>[8381, 8381, 8381, 8381, 8381]</td>\n",
       "      <td>145</td>\n",
       "      <td>[8381, 8381, 8381, 8381, 8381]</td>\n",
       "      <td>[8381, 8381, 8381, 8381, 8381]</td>\n",
       "      <td>[8381, 8381, 8381, 8381, 8381]</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>[7762, 6902, 6157, 1986, 5551]</td>\n",
       "      <td>4</td>\n",
       "      <td>[4634, 2106, 9827]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1800001501</td>\n",
       "      <td>PC</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>[1895, 8808, 1021, 8381, 8381]</td>\n",
       "      <td>145</td>\n",
       "      <td>[8381, 8381, 8381, 8381, 8381]</td>\n",
       "      <td>[8381, 8381, 8381, 8381, 8381]</td>\n",
       "      <td>[323, 9351, 3453, 8381, 8381]</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>[9724, 8381, 8381, 8381, 8381]</td>\n",
       "      <td>1</td>\n",
       "      <td>[2601, 4380, 4797]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1800001501</td>\n",
       "      <td>PR</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>[1895, 8808, 1021, 8381, 8381]</td>\n",
       "      <td>145</td>\n",
       "      <td>[8381, 8381, 8381, 8381, 8381]</td>\n",
       "      <td>[8381, 8381, 8381, 8381, 8381]</td>\n",
       "      <td>[323, 9351, 3453, 8381, 8381]</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>[7762, 6902, 6157, 1986, 5551]</td>\n",
       "      <td>4</td>\n",
       "      <td>[4634, 2106, 9827]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1800001819</td>\n",
       "      <td>PR</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>[8381, 8381, 8381, 8381, 8381]</td>\n",
       "      <td>145</td>\n",
       "      <td>[8381, 8381, 8381, 8381, 8381]</td>\n",
       "      <td>[8381, 8381, 8381, 8381, 8381]</td>\n",
       "      <td>[8381, 8381, 8381, 8381, 8381]</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>[916, 8381, 8381, 8381, 8381]</td>\n",
       "      <td>1</td>\n",
       "      <td>[9593, 4380, 8381]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id log_type  hour  weekday        user_watch_stk_code_hash  \\\n",
       "0  1800001088       PR     8        5  [8381, 8381, 8381, 8381, 8381]   \n",
       "1  1800001417       PR    22        5  [8381, 8381, 8381, 8381, 8381]   \n",
       "2  1800001501       PC    10        5  [1895, 8808, 1021, 8381, 8381]   \n",
       "3  1800001501       PR    22        5  [1895, 8808, 1021, 8381, 8381]   \n",
       "4  1800001819       PR    21        5  [8381, 8381, 8381, 8381, 8381]   \n",
       "\n",
       "   country_hash            prefer_bid_code_hash  \\\n",
       "0            71  [8381, 8381, 8381, 8381, 8381]   \n",
       "1           145  [8381, 8381, 8381, 8381, 8381]   \n",
       "2           145  [8381, 8381, 8381, 8381, 8381]   \n",
       "3           145  [8381, 8381, 8381, 8381, 8381]   \n",
       "4           145  [8381, 8381, 8381, 8381, 8381]   \n",
       "\n",
       "               hold_bid_code_hash            user_propernoun_hash  \\\n",
       "0  [8381, 8381, 8381, 8381, 8381]    [178, 417, 8381, 8381, 8381]   \n",
       "1  [8381, 8381, 8381, 8381, 8381]  [8381, 8381, 8381, 8381, 8381]   \n",
       "2  [8381, 8381, 8381, 8381, 8381]   [323, 9351, 3453, 8381, 8381]   \n",
       "3  [8381, 8381, 8381, 8381, 8381]   [323, 9351, 3453, 8381, 8381]   \n",
       "4  [8381, 8381, 8381, 8381, 8381]  [8381, 8381, 8381, 8381, 8381]   \n",
       "\n",
       "   push_title_hash  title_len                  item_code_hash  \\\n",
       "0                7         12  [6837, 3491, 8381, 8381, 8381]   \n",
       "1                7         10  [7762, 6902, 6157, 1986, 5551]   \n",
       "2                5         17  [9724, 8381, 8381, 8381, 8381]   \n",
       "3                7         10  [7762, 6902, 6157, 1986, 5551]   \n",
       "4                4         11   [916, 8381, 8381, 8381, 8381]   \n",
       "\n",
       "   submit_type_hash         tag_id_hash  \n",
       "0                 6   [8139, 8993, 880]  \n",
       "1                 4  [4634, 2106, 9827]  \n",
       "2                 1  [2601, 4380, 4797]  \n",
       "3                 4  [4634, 2106, 9827]  \n",
       "4                 1  [9593, 4380, 8381]  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# æ˜¾ç¤ºæœ€ç»ˆå¤„ç†ç»“æœçš„éƒ¨åˆ†æ•°æ®\n",
    "print(\"æœ€ç»ˆå¤„ç†ç»“æœé¢„è§ˆ:\")\n",
    "display_cols = ['user_id', 'log_type'] + processed_features  # æ˜¾ç¤ºå‰6ä¸ªæ–°ç‰¹å¾\n",
    "display_cols = [col for col in display_cols if col in df_processed.columns]\n",
    "\n",
    "df_processed[display_cols].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## æ€»ç»“\n",
    "\n",
    "é€šè¿‡è¿™ä¸ª Notebookï¼Œæˆ‘ä»¬å®Œæ•´åœ°æ¼”ç¤ºäº†ç‰¹å¾å·¥ç¨‹æµæ°´çº¿çš„æ‰§è¡Œè¿‡ç¨‹ï¼š\n",
    "\n",
    "### ğŸ¯ æ ¸å¿ƒæµç¨‹\n",
    "\n",
    "1. **æ•°æ®åŠ è½½**: ä» CSV æ–‡ä»¶åŠ è½½åŸå§‹æ•°æ®\n",
    "2. **é…ç½®è§£æ**: å°† YAML é…ç½®æ–‡ä»¶è§£æä¸ºå¯æ‰§è¡Œçš„æ“ä½œåºåˆ—\n",
    "3. **æ“ä½œæ‰§è¡Œ**: é€šè¿‡ OP_HUB æŸ¥æ‰¾å¹¶æ‰§è¡Œå…·ä½“çš„ç‰¹å¾å˜æ¢å‡½æ•°\n",
    "4. **ç»“æœç”Ÿæˆ**: ç”Ÿæˆæœ€ç»ˆçš„ç‰¹å¾æ•°æ®\n",
    "\n",
    "### ğŸ”§ å…³é”®ç»„ä»¶\n",
    "\n",
    "- **OP_HUB**: æ“ä½œå‡½æ•°æ³¨å†Œè¡¨ï¼Œè¿æ¥é…ç½®æ–‡ä»¶ä¸­çš„ `func_name` å’Œå®é™…çš„ Python å‡½æ•°\n",
    "- **run_one_op**: æ‰§è¡Œå¼•æ“ï¼Œè´Ÿè´£åº”ç”¨å•ä¸ªæ“ä½œåˆ° DataFrame\n",
    "- **Pipeline**: æ“ä½œåºåˆ—ï¼Œå®šä¹‰äº†ç‰¹å¾çš„å®Œæ•´å˜æ¢è¿‡ç¨‹\n",
    "\n",
    "### ğŸ“Š ç‰¹å¾ç±»å‹\n",
    "\n",
    "- **sparse**: ç¨€ç–ç‰¹å¾ï¼Œé€šå¸¸æ˜¯åˆ†ç±»å˜é‡çš„å“ˆå¸Œå€¼\n",
    "- **varlen_sparse**: å˜é•¿ç¨€ç–ç‰¹å¾ï¼Œå¤„ç†åˆ—è¡¨å‹æ•°æ®\n",
    "- **dense**: ç¨ å¯†ç‰¹å¾ï¼Œæ•°å€¼å‹ç‰¹å¾\n",
    "\n",
    "### ğŸ’¡ è®¾è®¡ä¼˜åŠ¿\n",
    "\n",
    "è¿™ä¸ªæµæ°´çº¿çš„è®¾è®¡ä½¿å¾—ç‰¹å¾å·¥ç¨‹å˜å¾—ï¼š\n",
    "- **é«˜åº¦å¯é…ç½®**: é€šè¿‡ YAML æ–‡ä»¶å®šä¹‰ç‰¹å¾å˜æ¢\n",
    "- **æ¨¡å—åŒ–**: æ¯ä¸ªæ“ä½œå‡½æ•°éƒ½æ˜¯ç‹¬ç«‹çš„\n",
    "- **å¯å¤ç”¨**: æ“ä½œå‡½æ•°å¯ä»¥åœ¨ä¸åŒçš„æµæ°´çº¿ä¸­é‡å¤ä½¿ç”¨\n",
    "- **æ˜“äºæ‰©å±•**: æ·»åŠ æ–°çš„æ“ä½œå‡½æ•°åªéœ€è¦åœ¨ OP_HUB ä¸­æ³¨å†Œ\n",
    "\n",
    "è¿™ç§æ¶æ„åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éå¸¸æœ‰ä»·å€¼ï¼Œå› ä¸ºå®ƒå…è®¸æ•°æ®ç§‘å­¦å®¶å¿«é€Ÿå®éªŒä¸åŒçš„ç‰¹å¾å·¥ç¨‹ç­–ç•¥ï¼Œè€Œæ— éœ€ä¿®æ”¹æ ¸å¿ƒä»£ç ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ç‰¹å¾è¡¨æ ¼åˆ†ææ¨¡å—å·²åŠ è½½!\n",
      "ä½¿ç”¨æ–¹æ³•: run_feature_analysis(df_processed, processed_features)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "ç‰¹å¾åˆ†ææ¨¡å— - è¡¨æ ¼åŒ–å±•ç¤º\n",
    "å¯¹ç‰¹å¾å·¥ç¨‹å¤„ç†åçš„æ•°æ®è¿›è¡Œç»Ÿè®¡åˆ†æï¼Œä»¥è¡¨æ ¼å½¢å¼å‘ˆç°ç»“æœ\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class FeatureTableAnalyzer:\n",
    "    \"\"\"ç‰¹å¾è¡¨æ ¼åˆ†æå™¨ - ä¸“æ³¨äºè¡¨æ ¼åŒ–ç»“æœå±•ç¤º\"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame, feature_list: List[str]):\n",
    "        self.df = df\n",
    "        self.features = feature_list\n",
    "        self.analysis_results = {}\n",
    "    \n",
    "    def analyze_all_features(self) -> None:\n",
    "        \"\"\"åˆ†ææ‰€æœ‰ç‰¹å¾å¹¶ç”Ÿæˆè¡¨æ ¼åŒ–æŠ¥å‘Š\"\"\"\n",
    "        print(\"ğŸ” å¼€å§‹ç‰¹å¾åˆ†æ...\")\n",
    "        \n",
    "        # 1. ç‰¹å¾æ¦‚è§ˆè¡¨\n",
    "        self.create_feature_overview_table()\n",
    "        \n",
    "        # 2. æ•°å€¼å‹ç‰¹å¾è¯¦ç»†åˆ†æ\n",
    "        self.analyze_numeric_features_table()\n",
    "        \n",
    "        # 3. åˆ—è¡¨å‹ç‰¹å¾è¯¦ç»†åˆ†æ\n",
    "        self.analyze_list_features_table()\n",
    "        \n",
    "        # 4. æ•°æ®è´¨é‡æŠ¥å‘Š\n",
    "        self.create_data_quality_table()\n",
    "        \n",
    "        print(\"\\nâœ… ç‰¹å¾åˆ†æå®Œæˆ!\")\n",
    "    \n",
    "    def create_feature_overview_table(self) -> None:\n",
    "        \"\"\"åˆ›å»ºç‰¹å¾æ¦‚è§ˆè¡¨\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ğŸ“Š ç‰¹å¾æ¦‚è§ˆè¡¨\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        overview_data = []\n",
    "        for feature in self.features:\n",
    "            if feature in self.df.columns:\n",
    "                sample_value = self.df[feature].iloc[0]\n",
    "                feature_type = \"åˆ—è¡¨å‹\" if isinstance(sample_value, list) else \"æ•°å€¼å‹\"\n",
    "                \n",
    "                # åŸºæœ¬ç»Ÿè®¡\n",
    "                unique_count = self.df[feature].nunique() if not isinstance(sample_value, list) else \"N/A\"\n",
    "                missing_count = self.df[feature].isnull().sum()\n",
    "                missing_rate = f\"{missing_count/len(self.df)*100:.2f}%\" if missing_count > 0 else \"0%\"\n",
    "                \n",
    "                # æ ·ä¾‹å€¼\n",
    "                if isinstance(sample_value, list):\n",
    "                    sample_str = f\"[{', '.join(map(str, sample_value[:3]))}...]\" if len(sample_value) > 3 else str(sample_value)\n",
    "                    if len(sample_str) > 50:\n",
    "                        sample_str = sample_str[:47] + \"...\"\n",
    "                else:\n",
    "                    sample_str = str(sample_value)\n",
    "                \n",
    "                overview_data.append({\n",
    "                    'ç‰¹å¾å': feature,\n",
    "                    'ç±»å‹': feature_type,\n",
    "                    'æ ·æœ¬æ•°': len(self.df),\n",
    "                    'å”¯ä¸€å€¼æ•°': unique_count,\n",
    "                    'ç¼ºå¤±å€¼': missing_rate,\n",
    "                    'æ ·ä¾‹å€¼': sample_str\n",
    "                })\n",
    "        \n",
    "        overview_df = pd.DataFrame(overview_data)\n",
    "        print(overview_df.to_string(index=False))\n",
    "    \n",
    "    def analyze_numeric_features_table(self) -> None:\n",
    "        \"\"\"åˆ†ææ•°å€¼å‹ç‰¹å¾å¹¶ç”Ÿæˆè¡¨æ ¼\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ğŸ“ˆ æ•°å€¼å‹ç‰¹å¾ç»Ÿè®¡è¡¨\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        numeric_features = []\n",
    "        stats_data = []\n",
    "        \n",
    "        for feature in self.features:\n",
    "            if feature in self.df.columns:\n",
    "                sample_value = self.df[feature].iloc[0]\n",
    "                if not isinstance(sample_value, list):\n",
    "                    numeric_features.append(feature)\n",
    "                    data = self.df[feature]\n",
    "                    \n",
    "                    stats_data.append({\n",
    "                        'ç‰¹å¾å': feature,\n",
    "                        'æœ€å°å€¼': data.min(),\n",
    "                        'æœ€å¤§å€¼': data.max(),\n",
    "                        'å‡å€¼': f\"{data.mean():.4f}\",\n",
    "                        'ä¸­ä½æ•°': data.median(),\n",
    "                        'æ ‡å‡†å·®': f\"{data.std():.4f}\",\n",
    "                        '25%åˆ†ä½': data.quantile(0.25),\n",
    "                        '75%åˆ†ä½': data.quantile(0.75),\n",
    "                        'ååº¦': f\"{data.skew():.4f}\"\n",
    "                    })\n",
    "        \n",
    "        if stats_data:\n",
    "            stats_df = pd.DataFrame(stats_data)\n",
    "            print(stats_df.to_string(index=False))\n",
    "            \n",
    "            # æ•°å€¼å‹ç‰¹å¾åˆ†å¸ƒè¡¨\n",
    "            print(\"\\nğŸ“Š æ•°å€¼å‹ç‰¹å¾åˆ†å¸ƒè¡¨ (å‰5ä¸ªé«˜é¢‘å€¼)\")\n",
    "            print(\"-\"*80)\n",
    "            for feature in numeric_features[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªç‰¹å¾é¿å…è¾“å‡ºè¿‡é•¿\n",
    "                data = self.df[feature]\n",
    "                value_counts = data.value_counts().head(5)\n",
    "                print(f\"\\n{feature}:\")\n",
    "                dist_data = []\n",
    "                for value, count in value_counts.items():\n",
    "                    dist_data.append({\n",
    "                        'å€¼': value,\n",
    "                        'é¢‘æ¬¡': count,\n",
    "                        'å æ¯”': f\"{count/len(data)*100:.2f}%\"\n",
    "                    })\n",
    "                dist_df = pd.DataFrame(dist_data)\n",
    "                print(dist_df.to_string(index=False))\n",
    "        else:\n",
    "            print(\"æœªæ‰¾åˆ°æ•°å€¼å‹ç‰¹å¾\")\n",
    "    \n",
    "    def analyze_list_features_table(self) -> None:\n",
    "        \"\"\"åˆ†æåˆ—è¡¨å‹ç‰¹å¾å¹¶ç”Ÿæˆè¡¨æ ¼\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ğŸ“‹ åˆ—è¡¨å‹ç‰¹å¾ç»Ÿè®¡è¡¨\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        list_features = []\n",
    "        list_stats_data = []\n",
    "        \n",
    "        for feature in self.features:\n",
    "            if feature in self.df.columns:\n",
    "                sample_value = self.df[feature].iloc[0]\n",
    "                if isinstance(sample_value, list):\n",
    "                    list_features.append(feature)\n",
    "                    data = self.df[feature]\n",
    "                    \n",
    "                    # è®¡ç®—é•¿åº¦ç»Ÿè®¡\n",
    "                    lengths = [len(x) if isinstance(x, list) else 0 for x in data]\n",
    "                    \n",
    "                    # è®¡ç®—å…ƒç´ ç»Ÿè®¡\n",
    "                    all_elements = []\n",
    "                    for item in data:\n",
    "                        if isinstance(item, list):\n",
    "                            all_elements.extend(item)\n",
    "                    \n",
    "                    element_counter = Counter(all_elements)\n",
    "                    \n",
    "                    list_stats_data.append({\n",
    "                        'ç‰¹å¾å': feature,\n",
    "                        'å¹³å‡é•¿åº¦': f\"{np.mean(lengths):.2f}\",\n",
    "                        'æœ€å°é•¿åº¦': min(lengths),\n",
    "                        'æœ€å¤§é•¿åº¦': max(lengths),\n",
    "                        'æ€»å…ƒç´ æ•°': len(all_elements),\n",
    "                        'å”¯ä¸€å…ƒç´ æ•°': len(element_counter),\n",
    "                        'å…ƒç´ é‡å¤ç‡': f\"{(1-len(element_counter)/len(all_elements))*100:.2f}%\" if all_elements else \"0%\"\n",
    "                    })\n",
    "        \n",
    "        if list_stats_data:\n",
    "            list_stats_df = pd.DataFrame(list_stats_data)\n",
    "            print(list_stats_df.to_string(index=False))\n",
    "            \n",
    "            # åˆ—è¡¨é•¿åº¦åˆ†å¸ƒè¡¨\n",
    "            print(\"\\nğŸ“ åˆ—è¡¨é•¿åº¦åˆ†å¸ƒè¡¨\")\n",
    "            print(\"-\"*80)\n",
    "            for feature in list_features:\n",
    "                data = self.df[feature]\n",
    "                lengths = [len(x) if isinstance(x, list) else 0 for x in data]\n",
    "                length_dist = Counter(lengths)\n",
    "                \n",
    "                print(f\"\\n{feature}:\")\n",
    "                length_data = []\n",
    "                for length, count in sorted(length_dist.items()):\n",
    "                    length_data.append({\n",
    "                        'é•¿åº¦': length,\n",
    "                        'æ ·æœ¬æ•°': count,\n",
    "                        'å æ¯”': f\"{count/len(data)*100:.2f}%\"\n",
    "                    })\n",
    "                length_df = pd.DataFrame(length_data)\n",
    "                print(length_df.to_string(index=False))\n",
    "                \n",
    "                # é«˜é¢‘å…ƒç´ è¡¨\n",
    "                print(f\"\\n{feature} - é«˜é¢‘å…ƒç´  (å‰8ä¸ª):\")\n",
    "                all_elements = []\n",
    "                for item in data:\n",
    "                    if isinstance(item, list):\n",
    "                        all_elements.extend(item)\n",
    "                \n",
    "                if all_elements:\n",
    "                    element_counter = Counter(all_elements)\n",
    "                    element_data = []\n",
    "                    for element, count in element_counter.most_common(8):\n",
    "                        element_data.append({\n",
    "                            'å…ƒç´ ': element,\n",
    "                            'å‡ºç°æ¬¡æ•°': count,\n",
    "                            'å æ¯”': f\"{count/len(all_elements)*100:.2f}%\"\n",
    "                        })\n",
    "                    element_df = pd.DataFrame(element_data)\n",
    "                    print(element_df.to_string(index=False))\n",
    "        else:\n",
    "            print(\"æœªæ‰¾åˆ°åˆ—è¡¨å‹ç‰¹å¾\")\n",
    "    \n",
    "    def create_data_quality_table(self) -> None:\n",
    "        \"\"\"åˆ›å»ºæ•°æ®è´¨é‡æŠ¥å‘Šè¡¨\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ğŸ” æ•°æ®è´¨é‡æŠ¥å‘Š\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        quality_data = []\n",
    "        for feature in self.features:\n",
    "            if feature in self.df.columns:\n",
    "                data = self.df[feature]\n",
    "                missing_count = data.isnull().sum()\n",
    "                \n",
    "                # æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥\n",
    "                sample_value = data.iloc[0]\n",
    "                if isinstance(sample_value, list):\n",
    "                    # æ£€æŸ¥åˆ—è¡¨ç‰¹å¾çš„ä¸€è‡´æ€§\n",
    "                    lengths = [len(x) if isinstance(x, list) else 0 for x in data]\n",
    "                    length_variance = np.var(lengths)\n",
    "                    consistency = \"é«˜\" if length_variance < 1 else \"ä¸­\" if length_variance < 4 else \"ä½\"\n",
    "                else:\n",
    "                    # æ£€æŸ¥æ•°å€¼ç‰¹å¾çš„åˆ†å¸ƒ\n",
    "                    cv = data.std() / data.mean() if data.mean() != 0 else 0\n",
    "                    consistency = \"é«˜\" if cv < 0.1 else \"ä¸­\" if cv < 0.5 else \"ä½\"\n",
    "                \n",
    "                quality_data.append({\n",
    "                    'ç‰¹å¾å': feature,\n",
    "                    'å®Œæ•´æ€§': f\"{(1-missing_count/len(data))*100:.1f}%\",\n",
    "                    'ç¼ºå¤±å€¼æ•°é‡': missing_count,\n",
    "                    'æ•°æ®ä¸€è‡´æ€§': consistency,\n",
    "                    'æ•°æ®ç±»å‹': \"åˆ—è¡¨å‹\" if isinstance(sample_value, list) else \"æ•°å€¼å‹\",\n",
    "                    'æ˜¯å¦å¯ç”¨': \"âœ…\" if missing_count < len(data) * 0.5 else \"âš ï¸\"\n",
    "                })\n",
    "        \n",
    "        quality_df = pd.DataFrame(quality_data)\n",
    "        print(quality_df.to_string(index=False))\n",
    "        \n",
    "        # æ€»ä½“è´¨é‡è¯„ä¼°\n",
    "        total_features = len(quality_data)\n",
    "        usable_features = sum(1 for item in quality_data if item['æ˜¯å¦å¯ç”¨'] == \"âœ…\")\n",
    "        avg_completeness = np.mean([float(item['å®Œæ•´æ€§'].strip('%')) for item in quality_data])\n",
    "        \n",
    "        print(f\"\\nğŸ“‹ æ€»ä½“è¯„ä¼°:\")\n",
    "        print(f\"   â€¢ ç‰¹å¾æ€»æ•°: {total_features}\")\n",
    "        print(f\"   â€¢ å¯ç”¨ç‰¹å¾: {usable_features} ({usable_features/total_features*100:.1f}%)\")\n",
    "        print(f\"   â€¢ å¹³å‡å®Œæ•´æ€§: {avg_completeness:.1f}%\")\n",
    "        print(f\"   â€¢ æ•°æ®è´¨é‡: {'ä¼˜ç§€' if avg_completeness > 95 else 'è‰¯å¥½' if avg_completeness > 85 else 'éœ€æ”¹è¿›'}\")\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹å‡½æ•°\n",
    "def run_feature_analysis(df_processed: pd.DataFrame, processed_features: List[str]) -> None:\n",
    "    \"\"\"è¿è¡Œç‰¹å¾åˆ†æçš„ä¾¿æ·å‡½æ•°\"\"\"\n",
    "    analyzer = FeatureTableAnalyzer(df_processed, processed_features)\n",
    "    analyzer.analyze_all_features()\n",
    "\n",
    "print(\"ğŸ“Š ç‰¹å¾è¡¨æ ¼åˆ†ææ¨¡å—å·²åŠ è½½!\")\n",
    "print(\"ä½¿ç”¨æ–¹æ³•: run_feature_analysis(df_processed, processed_features)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ ç‰¹å¾åˆ†æä½¿ç”¨è¯´æ˜:\n",
      "   åœ¨å®Œæˆç‰¹å¾å·¥ç¨‹åï¼Œè¿è¡Œ: run_comprehensive_feature_analysis()\n",
      "   æˆ–è€…ç›´æ¥è¿è¡Œ: run_feature_analysis(df_processed, processed_features)\n",
      "\n",
      "ğŸ’¡ æç¤º: å»ºè®®åœ¨notebookæœ€åæ‰§è¡Œç‰¹å¾å·¥ç¨‹åå†è¿è¡Œæ­¤åˆ†æ\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# ğŸš€ è¿è¡Œå®Œæ•´ç‰¹å¾åˆ†æ \n",
    "# ===============================\n",
    "# æ³¨æ„ï¼šæ­¤ä»£ç å—éœ€è¦åœ¨ç‰¹å¾å·¥ç¨‹å®Œæˆåè¿è¡Œ\n",
    "# å°†åœ¨notebookæœ«å°¾æ‰§è¡Œç‰¹å¾å·¥ç¨‹åï¼Œå†å›åˆ°æ­¤å¤„è¿è¡Œåˆ†æ\n",
    "\n",
    "def run_comprehensive_feature_analysis():\n",
    "    \"\"\"è¿è¡Œå®Œæ•´çš„ç‰¹å¾åˆ†æ - åœ¨ç‰¹å¾å·¥ç¨‹å®Œæˆåè°ƒç”¨\"\"\"\n",
    "    try:\n",
    "        # æ£€æŸ¥æ˜¯å¦å·²ç»å®Œæˆç‰¹å¾å·¥ç¨‹\n",
    "        if 'df_processed' in globals() and 'processed_features' in globals():\n",
    "            print(\"ğŸ¯ å¼€å§‹è¿è¡Œå®Œæ•´ç‰¹å¾åˆ†æ...\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            # è¿è¡Œè¡¨æ ¼åŒ–ç‰¹å¾åˆ†æ\n",
    "            run_feature_analysis(df_processed, processed_features)\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"ğŸ‰ ç‰¹å¾åˆ†æå®Œæˆï¼ä»¥ä¸ŠæŠ¥å‘Šå±•ç¤ºäº†æ‰€æœ‰ç‰¹å¾çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯\")\n",
    "            \n",
    "        else:\n",
    "            print(\"âš ï¸  è¯·å…ˆè¿è¡Œç‰¹å¾å·¥ç¨‹æµæ°´çº¿ï¼Œç”Ÿæˆ df_processed å’Œ processed_features\")\n",
    "            print(\"   å»ºè®®æŒ‰é¡ºåºæ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š\")\n",
    "            print(\"   1. è¿è¡Œæ•°æ®åŠ è½½ä»£ç å—\")\n",
    "            print(\"   2. è¿è¡ŒYAMLé…ç½®è§£æä»£ç å—\") \n",
    "            print(\"   3. è¿è¡Œç‰¹å¾å·¥ç¨‹æ‰§è¡Œä»£ç å—\")\n",
    "            print(\"   4. å†å›åˆ°æ­¤å¤„è¿è¡Œç‰¹å¾åˆ†æ\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}\")\n",
    "        print(\"   è¯·ç¡®ä¿å·²ç»æ­£ç¡®æ‰§è¡Œäº†å‰é¢çš„ç‰¹å¾å·¥ç¨‹æ­¥éª¤\")\n",
    "\n",
    "# æ˜¾ç¤ºä½¿ç”¨è¯´æ˜\n",
    "print(\"ğŸ“‹ ç‰¹å¾åˆ†æä½¿ç”¨è¯´æ˜:\")\n",
    "print(\"   åœ¨å®Œæˆç‰¹å¾å·¥ç¨‹åï¼Œè¿è¡Œ: run_comprehensive_feature_analysis()\")\n",
    "print(\"   æˆ–è€…ç›´æ¥è¿è¡Œ: run_feature_analysis(df_processed, processed_features)\")\n",
    "print(\"\\nğŸ’¡ æç¤º: å»ºè®®åœ¨notebookæœ€åæ‰§è¡Œç‰¹å¾å·¥ç¨‹åå†è¿è¡Œæ­¤åˆ†æ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ å¼€å§‹è¿è¡Œå®Œæ•´ç‰¹å¾åˆ†æ...\n",
      "================================================================================\n",
      "ğŸ” å¼€å§‹ç‰¹å¾åˆ†æ...\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š ç‰¹å¾æ¦‚è§ˆè¡¨\n",
      "================================================================================\n",
      "                     ç‰¹å¾å  ç±»å‹   æ ·æœ¬æ•° å”¯ä¸€å€¼æ•° ç¼ºå¤±å€¼                   æ ·ä¾‹å€¼\n",
      "                    hour æ•°å€¼å‹ 50000   24  0%                     8\n",
      "                 weekday æ•°å€¼å‹ 50000    6  0%                     5\n",
      "user_watch_stk_code_hash åˆ—è¡¨å‹ 50000  N/A  0% [8381, 8381, 8381...]\n",
      "            country_hash æ•°å€¼å‹ 50000  120  0%                    71\n",
      "    prefer_bid_code_hash åˆ—è¡¨å‹ 50000  N/A  0% [8381, 8381, 8381...]\n",
      "      hold_bid_code_hash åˆ—è¡¨å‹ 50000  N/A  0% [8381, 8381, 8381...]\n",
      "    user_propernoun_hash åˆ—è¡¨å‹ 50000  N/A  0%   [178, 417, 8381...]\n",
      "         push_title_hash æ•°å€¼å‹ 50000    8  0%                     7\n",
      "               title_len æ•°å€¼å‹ 50000   27  0%                    12\n",
      "          item_code_hash åˆ—è¡¨å‹ 50000  N/A  0% [6837, 3491, 8381...]\n",
      "        submit_type_hash æ•°å€¼å‹ 50000    3  0%                     6\n",
      "             tag_id_hash åˆ—è¡¨å‹ 50000  N/A  0%     [8139, 8993, 880]\n",
      "\n",
      "================================================================================\n",
      "ğŸ“ˆ æ•°å€¼å‹ç‰¹å¾ç»Ÿè®¡è¡¨\n",
      "================================================================================\n",
      "             ç‰¹å¾å  æœ€å°å€¼  æœ€å¤§å€¼       å‡å€¼   ä¸­ä½æ•°     æ ‡å‡†å·®  25%åˆ†ä½  75%åˆ†ä½      ååº¦\n",
      "            hour    0   23  11.3420  10.0  6.6374    6.0   17.0  0.1431\n",
      "         weekday    0    6   3.2000   4.0  2.3150    1.0    5.0 -0.2436\n",
      "    country_hash    0  198 124.6826 145.0 47.3443   99.0  145.0 -1.0634\n",
      " push_title_hash    0    7   5.6323   7.0  2.2084    5.0    7.0 -1.3758\n",
      "       title_len    4   31  13.8898  14.0  3.9751   11.0   16.0  0.7432\n",
      "submit_type_hash    1    6   3.2370   4.0  1.9639    1.0    4.0  0.0338\n",
      "\n",
      "ğŸ“Š æ•°å€¼å‹ç‰¹å¾åˆ†å¸ƒè¡¨ (å‰5ä¸ªé«˜é¢‘å€¼)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "hour:\n",
      " å€¼   é¢‘æ¬¡    å æ¯”\n",
      " 9 4108 8.22%\n",
      " 8 3697 7.39%\n",
      "10 2940 5.88%\n",
      "22 2751 5.50%\n",
      "21 2421 4.84%\n",
      "\n",
      "weekday:\n",
      " å€¼    é¢‘æ¬¡     å æ¯”\n",
      " 1 10005 20.01%\n",
      " 4 10001 20.00%\n",
      " 6  9999 20.00%\n",
      " 5  9998 20.00%\n",
      " 0  9995 19.99%\n",
      "\n",
      "country_hash:\n",
      "  å€¼    é¢‘æ¬¡     å æ¯”\n",
      "145 17772 35.54%\n",
      "181  7335 14.67%\n",
      " 99  5182 10.36%\n",
      "106  3149  6.30%\n",
      "127  2342  4.68%\n",
      "\n",
      "push_title_hash:\n",
      " å€¼    é¢‘æ¬¡     å æ¯”\n",
      " 7 32539 65.08%\n",
      " 2  2981  5.96%\n",
      " 5  2979  5.96%\n",
      " 3  2738  5.48%\n",
      " 6  2639  5.28%\n",
      "\n",
      "title_len:\n",
      " å€¼   é¢‘æ¬¡     å æ¯”\n",
      "14 5890 11.78%\n",
      "15 5730 11.46%\n",
      "16 4850  9.70%\n",
      "10 4806  9.61%\n",
      "13 4450  8.90%\n",
      "\n",
      "================================================================================\n",
      "ğŸ“‹ åˆ—è¡¨å‹ç‰¹å¾ç»Ÿè®¡è¡¨\n",
      "================================================================================\n",
      "                     ç‰¹å¾å å¹³å‡é•¿åº¦  æœ€å°é•¿åº¦  æœ€å¤§é•¿åº¦   æ€»å…ƒç´ æ•°  å”¯ä¸€å…ƒç´ æ•°  å…ƒç´ é‡å¤ç‡\n",
      "user_watch_stk_code_hash 5.00     5     5 250000   3612 98.56%\n",
      "    prefer_bid_code_hash 5.00     5     5 250000   1515 99.39%\n",
      "      hold_bid_code_hash 5.00     5     5 250000    829 99.67%\n",
      "    user_propernoun_hash 5.00     5     5 250000    876 99.65%\n",
      "          item_code_hash 5.00     5     5 250000    928 99.63%\n",
      "             tag_id_hash 3.00     3     3 150000     35 99.98%\n",
      "\n",
      "ğŸ“ åˆ—è¡¨é•¿åº¦åˆ†å¸ƒè¡¨\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "user_watch_stk_code_hash:\n",
      " é•¿åº¦   æ ·æœ¬æ•°      å æ¯”\n",
      "  5 50000 100.00%\n",
      "\n",
      "user_watch_stk_code_hash - é«˜é¢‘å…ƒç´  (å‰8ä¸ª):\n",
      "  å…ƒç´    å‡ºç°æ¬¡æ•°     å æ¯”\n",
      "8381 190585 76.23%\n",
      " 304   3009  1.20%\n",
      "1645    929  0.37%\n",
      "2299    898  0.36%\n",
      "6050    817  0.33%\n",
      "2290    749  0.30%\n",
      "5377    692  0.28%\n",
      "4996    683  0.27%\n",
      "\n",
      "prefer_bid_code_hash:\n",
      " é•¿åº¦   æ ·æœ¬æ•°      å æ¯”\n",
      "  5 50000 100.00%\n",
      "\n",
      "prefer_bid_code_hash - é«˜é¢‘å…ƒç´  (å‰8ä¸ª):\n",
      "  å…ƒç´    å‡ºç°æ¬¡æ•°     å æ¯”\n",
      "8381 234507 93.80%\n",
      " 304    474  0.19%\n",
      "1863    379  0.15%\n",
      "9210    274  0.11%\n",
      "8853    220  0.09%\n",
      "5377    213  0.09%\n",
      "9919    195  0.08%\n",
      "5291    181  0.07%\n",
      "\n",
      "hold_bid_code_hash:\n",
      " é•¿åº¦   æ ·æœ¬æ•°      å æ¯”\n",
      "  5 50000 100.00%\n",
      "\n",
      "hold_bid_code_hash - é«˜é¢‘å…ƒç´  (å‰8ä¸ª):\n",
      "  å…ƒç´    å‡ºç°æ¬¡æ•°     å æ¯”\n",
      "8381 243816 97.53%\n",
      " 304    183  0.07%\n",
      "9210     97  0.04%\n",
      "9830     87  0.03%\n",
      "1863     83  0.03%\n",
      "2291     76  0.03%\n",
      "4996     75  0.03%\n",
      "4693     69  0.03%\n",
      "\n",
      "user_propernoun_hash:\n",
      " é•¿åº¦   æ ·æœ¬æ•°      å æ¯”\n",
      "  5 50000 100.00%\n",
      "\n",
      "user_propernoun_hash - é«˜é¢‘å…ƒç´  (å‰8ä¸ª):\n",
      "  å…ƒç´    å‡ºç°æ¬¡æ•°     å æ¯”\n",
      "8381 212013 84.81%\n",
      "1018   3121  1.25%\n",
      "9489   2588  1.04%\n",
      "7480   2531  1.01%\n",
      "1778   1788  0.72%\n",
      "6272   1703  0.68%\n",
      "2035   1222  0.49%\n",
      "2015    952  0.38%\n",
      "\n",
      "item_code_hash:\n",
      " é•¿åº¦   æ ·æœ¬æ•°      å æ¯”\n",
      "  5 50000 100.00%\n",
      "\n",
      "item_code_hash - é«˜é¢‘å…ƒç´  (å‰8ä¸ª):\n",
      "  å…ƒç´    å‡ºç°æ¬¡æ•°     å æ¯”\n",
      "8381 208950 83.58%\n",
      "1863   2534  1.01%\n",
      "9210   2397  0.96%\n",
      "8853   1162  0.46%\n",
      "2299   1024  0.41%\n",
      " 304    891  0.36%\n",
      "1645    758  0.30%\n",
      "7995    568  0.23%\n",
      "\n",
      "tag_id_hash:\n",
      " é•¿åº¦   æ ·æœ¬æ•°      å æ¯”\n",
      "  3 50000 100.00%\n",
      "\n",
      "tag_id_hash - é«˜é¢‘å…ƒç´  (å‰8ä¸ª):\n",
      "  å…ƒç´   å‡ºç°æ¬¡æ•°     å æ¯”\n",
      " 880 26034 17.36%\n",
      "8381 21313 14.21%\n",
      "4634 16513 11.01%\n",
      "4797 11601  7.73%\n",
      "2601 10150  6.77%\n",
      "8993  9951  6.63%\n",
      " 542  9722  6.48%\n",
      "8139  6013  4.01%\n",
      "\n",
      "================================================================================\n",
      "ğŸ” æ•°æ®è´¨é‡æŠ¥å‘Š\n",
      "================================================================================\n",
      "                     ç‰¹å¾å    å®Œæ•´æ€§  ç¼ºå¤±å€¼æ•°é‡ æ•°æ®ä¸€è‡´æ€§ æ•°æ®ç±»å‹ æ˜¯å¦å¯ç”¨\n",
      "                    hour 100.0%      0     ä½  æ•°å€¼å‹    âœ…\n",
      "                 weekday 100.0%      0     ä½  æ•°å€¼å‹    âœ…\n",
      "user_watch_stk_code_hash 100.0%      0     é«˜  åˆ—è¡¨å‹    âœ…\n",
      "            country_hash 100.0%      0     ä¸­  æ•°å€¼å‹    âœ…\n",
      "    prefer_bid_code_hash 100.0%      0     é«˜  åˆ—è¡¨å‹    âœ…\n",
      "      hold_bid_code_hash 100.0%      0     é«˜  åˆ—è¡¨å‹    âœ…\n",
      "    user_propernoun_hash 100.0%      0     é«˜  åˆ—è¡¨å‹    âœ…\n",
      "         push_title_hash 100.0%      0     ä¸­  æ•°å€¼å‹    âœ…\n",
      "               title_len 100.0%      0     ä¸­  æ•°å€¼å‹    âœ…\n",
      "          item_code_hash 100.0%      0     é«˜  åˆ—è¡¨å‹    âœ…\n",
      "        submit_type_hash 100.0%      0     ä½  æ•°å€¼å‹    âœ…\n",
      "             tag_id_hash 100.0%      0     é«˜  åˆ—è¡¨å‹    âœ…\n",
      "\n",
      "ğŸ“‹ æ€»ä½“è¯„ä¼°:\n",
      "   â€¢ ç‰¹å¾æ€»æ•°: 12\n",
      "   â€¢ å¯ç”¨ç‰¹å¾: 12 (100.0%)\n",
      "   â€¢ å¹³å‡å®Œæ•´æ€§: 100.0%\n",
      "   â€¢ æ•°æ®è´¨é‡: ä¼˜ç§€\n",
      "\n",
      "âœ… ç‰¹å¾åˆ†æå®Œæˆ!\n",
      "\n",
      "================================================================================\n",
      "ğŸ‰ ç‰¹å¾åˆ†æå®Œæˆï¼ä»¥ä¸ŠæŠ¥å‘Šå±•ç¤ºäº†æ‰€æœ‰ç‰¹å¾çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯\n"
     ]
    }
   ],
   "source": [
    "run_comprehensive_feature_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08 æ ‘æ¨¡å‹å®šä¹‰ä¸è®­ç»ƒè¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def prepare_features(df_processed, processed_features, max_list_length=5):\n",
    "    \"\"\"å±•å¼€åˆ—è¡¨ç‰¹å¾\"\"\"\n",
    "    df_tree = df_processed[processed_features].copy()\n",
    "    \n",
    "    for feat in processed_features:\n",
    "        if isinstance(df_tree[feat].iloc[0], list):\n",
    "            expanded = df_tree[feat].apply(pd.Series).iloc[:, :max_list_length]\n",
    "            expanded.columns = [f\"{feat}_{i}\" for i in range(expanded.shape[1])]\n",
    "            df_tree = df_tree.drop(columns=[feat]).join(expanded)\n",
    "    \n",
    "    return df_tree\n",
    "\n",
    "def train_model(X, y, train_params):\n",
    "    \"\"\"è®­ç»ƒLightGBMæ¨¡å‹\"\"\"\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train, y_train)\n",
    "    val_data = lgb.Dataset(X_val, y_val, reference=train_data)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        train_params,\n",
    "        train_data,\n",
    "        num_boost_round=train_params.pop('num_iterations', 1000),\n",
    "        callbacks=[lgb.early_stopping(train_params.pop('early_stopping_rounds', 100))],\n",
    "        valid_sets=[train_data, val_data],\n",
    "        valid_names=['train', 'valid']\n",
    "    )\n",
    "    \n",
    "    return model, X_train, X_val, y_train, y_val\n",
    "\n",
    "def evaluate_model(model, X_train, X_val, y_train, y_val):\n",
    "    \"\"\"è¯„ä¼°æ¨¡å‹æ€§èƒ½\"\"\"\n",
    "    y_train_pred = model.predict(X_train, num_iteration=model.best_iteration)\n",
    "    y_val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    \n",
    "    train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "    val_auc = roc_auc_score(y_val, y_val_pred)\n",
    "    \n",
    "    print(f\"è®­ç»ƒé›† AUC: {train_auc:.4f}\")\n",
    "    print(f\"éªŒè¯é›† AUC: {val_auc:.4f}\")\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'feature': model.feature_name(),\n",
    "        'importance': model.feature_importance(importance_type='gain')\n",
    "    }).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[300]\ttrain's auc: 0.863989\tvalid's auc: 0.847059\n",
      "è®­ç»ƒé›† AUC: 0.8640\n",
      "éªŒè¯é›† AUC: 0.8471\n",
      "\n",
      "ç‰¹å¾é‡è¦æ€§ (Top 20):\n",
      "                       feature     importance\n",
      "23      user_propernoun_hash_2  346906.059675\n",
      "21      user_propernoun_hash_0  116131.237506\n",
      "2                 country_hash   60277.225478\n",
      "22      user_propernoun_hash_1   55774.542730\n",
      "1                      weekday   24687.482483\n",
      "0                         hour    9143.383609\n",
      "5             submit_type_hash    5356.176477\n",
      "10  user_watch_stk_code_hash_4    4607.537248\n",
      "3              push_title_hash    3455.920555\n",
      "6   user_watch_stk_code_hash_0    3359.792433\n",
      "4                    title_len    3198.852174\n",
      "7   user_watch_stk_code_hash_1    2804.459620\n",
      "8   user_watch_stk_code_hash_2    2645.200194\n",
      "33               tag_id_hash_2    2603.419945\n",
      "9   user_watch_stk_code_hash_3    2591.612250\n",
      "32               tag_id_hash_1    2223.176195\n",
      "11      prefer_bid_code_hash_0    2154.012811\n",
      "31               tag_id_hash_0    2150.902098\n",
      "27            item_code_hash_1    2011.556514\n",
      "26            item_code_hash_0    1682.390932\n"
     ]
    }
   ],
   "source": [
    "# ä¸»æµç¨‹\n",
    "with open('config/config.yml', 'r', encoding='utf-8') as f:\n",
    "    train_config = yaml.safe_load(f)\n",
    "\n",
    "if 'log_type' in df_processed.columns:\n",
    "    # å‡†å¤‡æ•°æ®\n",
    "    df_processed['label'] = df_processed['log_type'].apply(lambda x: 1 if x == 'PC' else 0)\n",
    "    X = prepare_features(df_processed, processed_features)\n",
    "    y = df_processed['label']\n",
    "    \n",
    "    # è®­ç»ƒæ¨¡å‹\n",
    "    train_params = {**train_config['train'], 'verbose': -1, 'n_jobs': -1, 'seed': 42}\n",
    "    model, X_train, X_val, y_train, y_val = train_model(X, y, train_params)\n",
    "    \n",
    "    # è¯„ä¼°å¹¶è¾“å‡ºç»“æœ\n",
    "    feature_importance = evaluate_model(model, X_train, X_val, y_train, y_val)\n",
    "    print(\"\\nç‰¹å¾é‡è¦æ€§ (Top 20):\")\n",
    "    print(feature_importance.head(20))\n",
    "else:\n",
    "    print(\"é”™è¯¯: æ‰¾ä¸åˆ° 'log_type' åˆ—ï¼Œæ— æ³•è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09 æ·±åº¦æ¨¡å‹å…¨æµç¨‹ï¼šç±»ä¼¼Huggig Faceçš„æ¡†æ¶ è¿™é‡Œç”¨çš„æ˜¯tensorflowçš„æ¡†æ¶"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

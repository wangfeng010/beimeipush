# æ¶æ„æ¦‚è¿°
# åŸå§‹æ•°æ®å­˜æ”¾
#     çº¿ä¸Šï¼šdata/train/*txt
#     çº¿ä¸‹ï¼šdata/train/*csv
# ç‰¹å¾å·¥ç¨‹å¤„ç†æµç¨‹çš„å®šä¹‰
#     æ ‘æ¨¡å‹ï¼šconfig.yml
#     æ·±åº¦æ¨¡å‹ï¼šfeat.yml

# 01 ç¯å¢ƒå¯¼å…¥ â€”â€” ç¡®å®šé¡¹ç›®çš„ æ ¸å¿ƒæ¶æ„ å’Œ å·¥å…·
import os
import sys
import json
from functools import partial
from datetime import datetime
from hashlib import md5
from typing import Any, Dict, List, Union
import pandas as pd
import yaml
import numpy as np
from pprint import pprint
from glob import glob  # æ·»åŠ globæ¨¡å—å¯¼å…¥
import time


# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.getcwd()
env_path = os.path.join(project_root, 'env')
if env_path not in sys.path:
    sys.path.insert(0, env_path)

# 02 åŠ è½½åŸå§‹æ•°æ®
data_path = 'data/train/*.csv'
# ä½¿ç”¨globè·å–æ‰€æœ‰åŒ¹é…çš„CSVæ–‡ä»¶è·¯å¾„
csv_files = glob(data_path)
if not csv_files:
    raise ValueError(f"No CSV files found in {data_path}")

# è¯»å–å¹¶åˆå¹¶æ‰€æœ‰CSVæ–‡ä»¶
df_raw = pd.concat([pd.read_csv(f) for f in csv_files], ignore_index=True)
for col in df_raw.columns: print(f"{col}: {df_raw[col].iloc[0]}") # ç›´è§‚å±•ç¤ºå„åˆ—çš„å…·ä½“æ•°æ®çš„æ ·å¼
# æˆ‘ä»¬å¯ä»¥çœ‹åˆ°åŸå§‹æ•°æ®çš„æ ·å­ ç”¨pdè¯»å»å
# user_id	create_time	log_type	watchlists	holdings	country	prefer_bid	user_propernoun	push_title	push_content	item_code	item_tags	submit_type
# 0	1800000316	2025-05-30 09:30:14	PR	VIXY_171 & UPST_185 & AMD_185 & ASTS_185 & SHO...	RKLB,186|NFLX,185|ISRG,185|NVDA,185|OKLO,169|A...	China	TWCUX#0.54|ACFOX#0.53721|ALAB#0.38253|SPOT#0.3...	NaN	Ainvest Newswire	Nvidia Corporation shares rise 3.25% intraday ...	[{"market":"185","score":1,"code":"NVDA","tagI...	[{"score":0.9049434065818787,"tagId":"51510","...	autoFlash
# 1	1800000316	2025-05-30 09:30:14.995	PR	VIXY_171 & UPST_185 & AMD_185 & ASTS_185 & SHO...	RKLB,186|NFLX,185|ISRG,185|NVDA,185|OKLO,169|A...	China	TWCUX#0.54|ACFOX#0.53721|ALAB#0.38253|SPOT#0.3...	NaN	Ainvest Newswire	Nvidia Corporation shares rise 3.25% intraday ...	[{"market":"185","score":1,"code":"NVDA","tagI...	[{"score":0.9049434065818787,"tagId":"51510","...	autoFlash
# 2	1800001318	2025-05-30 19:49:03	PR	IBIT_185 & TEM_185 & DLR_169 & HOOD_185 & ETH_...	BRK.B,169|HBAN,185	China	BRK.B#0.54|SQQQ#0.28932|IBIT#0.24075|HBAN#0.17851	NaN	Breaking News	Fed's Daly Says Still Comfortable With Two Rat...	NaN	[{"score":0,"tagId":"56127","name":"us_stock",...	flash
# 3	1800001318	2025-05-30 05:09:09	PR	IBIT_185 & TEM_185 & DLR_169 & HOOD_185 & ETH_...	BRK.B,169|HBAN,185	China	BRK.B#0.54|SQQQ#0.28932|IBIT#0.24075|HBAN#0.17851	NaN	UP Fintech's Q1 Surge: A C...	The financial markets are always searching for...	[{"market":"185","score":0,"code":"TIGR","tagI...	[{"score":0.4888981282711029,"code":"us_low_im...	NaN
# 4	1800001324	2025-05-30 13:57:50	PR	NaN	NaN	United States	NaN	blackwell nvl72 ai#3.06|nvidia#3.06	Trump Media's $2.32 Billio...	Trump Media's recent acquisition of $2.32 bill...	[{"market":"185","score":0,"code":"DJT","tagId...	[{"score":0.6,"code":"Stock","tagId":"1000147"...	NaN

# æˆ‘ä»¬å¯ä»¥åšåŸºç¡€çš„ç»Ÿè®¡åˆ†æ
# åŸå§‹æ•°æ®pdçš„å½¢çŠ¶ï¼ˆå¤šå°‘è¡Œ å¤šå°‘åˆ—ï¼‰ï¼šdf_raw.shape
# æ¯ä¸€åˆ—çš„åˆ—åï¼šlist(df_raw.columns)
# ç›´è§‚å±•ç¤ºä¸€ä¸ªæ•°æ®ï¼šdf_raw.head()
# ç›´è§‚å±•ç¤ºå„åˆ—çš„å…·ä½“æ•°æ®çš„æ ·å¼ï¼šfor col in df_raw.columns: print(f"{col}: {df_raw[col].iloc[0]}")
#                          ä¹Ÿå°±æ˜¯col å’Œ df_raw[col].iloc[0]
# user_id: 1800000316
# create_time: 2025-05-30 09:30:14
# log_type: PR
# watchlists: VIXY_171 & UPST_185 & AMD_185 & ASTS_185 & SHOP_185 & RKLB_186 & IBKR_185 & PLTR_185 & ISRG_185 & APP_185 & VIXM_171 & NFLX_185 & NVDA_185 & ORCL_169 & QCOM_185 & SMCI_185 & SONY_169 & SOUN_185 & TSLA_185 & TSM_169 & U_169 & UBER_169 & AAPL_185 & ALAB_185 & AMT_169 & AMZN_185 & ARM_185 & BABA_169 & BRK.A_169 & COST_185 & DELL_169 & DJT_185 & DOCU_185 & EQIX_185 & EWBC_185 & GME_169 & GOOGL_185 & HOOD_185 & INTC_185 & LULU_185 & META_185 & MSFT_185 & OKLO_169 & IONQ_169 & NNE_186 & SPOT_169
# holdings: RKLB,186|NFLX,185|ISRG,185|NVDA,185|OKLO,169|ALAB,185|CSCO,185|MSFT,185|SPOT,169|TSLA,185|AAPL,185|AMZN,185|GOOGL,185|HOOD,185|PLTR,185|ASML,185|AVGO,185|COST,185|META,185
# country: China
# prefer_bid: TWCUX#0.54|ACFOX#0.53721|ALAB#0.38253|SPOT#0.34926|NVDA#0.34051
# user_propernoun: nan
# push_title: Ainvest Newswire
# push_content: Nvidia Corporation shares rise 3.25% intraday after Q1 2026 revenue beats expectations at $44.1B.
# item_code: [{"market":"185","score":1,"code":"NVDA","tagId":"U000017465","name":"è‹±ä¼Ÿè¾¾","type":0,"parentId":"US_ROBOT7140bd7a307047b2"}]
# item_tags: [{"score":0.9049434065818787,"tagId":"51510","name":"us_high_importance","type":4,"parentId":"US_ROBOT7140bd7a307047b2"},{"score":0.9049434065818787,"tagId":"53111","name":"aigc_today_mover","type":4,"parentId":"US_ROBOT7140bd7a307047b2"},{"tagId":"1002","name":"no_penny_stock","type":4,"parentId":"US_ROBOT7140bd7a307047b2"}]
# submit_type: autoFlash

# 03 åŠ è½½å¹¶è§£æYAMLé…ç½® â€”â€” "è“å›¾"æ–‡ä»¶
print("ğŸ“‹ é…ç½®æ–‡ä»¶ä½¿ç”¨è¯´æ˜:")
print("  ğŸŒ³ æ ‘æ¨¡å‹: config.yml (åŒ…å«ç‰¹å¾å·¥ç¨‹ + è®­ç»ƒå‚æ•°)")
print("  ğŸ§  æ·±åº¦æ¨¡å‹: feat.yml (ç‰¹å¾å·¥ç¨‹) + train.yml (è®­ç»ƒå‚æ•°)")

# åŠ è½½æ ‘æ¨¡å‹é…ç½® (config.yml åŒ…å«ç‰¹å¾å’Œè®­ç»ƒé…ç½®)
with open('config/config.yml', 'r', encoding='utf-8') as f:
    config_yml = yaml.safe_load(f)

# ä» config.yml æå–æ ‘æ¨¡å‹çš„ç‰¹å¾é…ç½®
tree_feat_config = config_yml['features']

# åŠ è½½æ·±åº¦æ¨¡å‹çš„ç‰¹å¾é…ç½® (feat.yml)
with open('config/feat.yml', 'r', encoding='utf-8') as f:
    deep_feat_config = yaml.safe_load(f)

# å½“å‰ä½¿ç”¨æ ‘æ¨¡å‹çš„ç‰¹å¾é…ç½®è¿›è¡Œæ¼”ç¤º
feat_config = tree_feat_config

print(feat_config)
# æ ¸å¿ƒå°±æ˜¯ç†è§£è¿™ä¸ªPyYAMLåº“æä¾›çš„yaml.safe_load()å·¥å…·çš„ä½¿ç”¨
# è¯»å–ä¸€ä¸ª YAML æ ¼å¼çš„æ–‡ä»¶æˆ–å­—ç¬¦ä¸²ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸º Python å¯¹è±¡ï¼ˆé€šå¸¸æ˜¯å­—å…¸æˆ–åˆ—è¡¨ï¼‰
# è¿™é‡Œçš„feat_configå°±å˜æˆäº†è¿™æ ·
# è¿™æ˜¯ä¸€ä¸ªä¸¤å±‚ç»“æ„çš„å­—å…¸ï¼š
# é¡¶å±‚æœ‰ä¸¤ä¸ªé”®ï¼šexclude_features å’Œ pipelines
# pipelines æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªåŒ…å«ç‰¹å¾å¤„ç†é…ç½®çš„å­—å…¸
# æ‰€ä»¥é’ˆå¯¹è¿™ä¸ªå¤„ç†å‡ºæ¥çš„æ•°æ®ç»“æ„ len(feat_config['pipelines']) æ˜¯å¯ä»¥çœ‹ä¸€ä¸‹æœ‰å¤šå°‘ä¸ªåœ¨è“å›¾ä¸­å†™ç€çš„éœ€è¦å¤„ç†çš„ç‰¹å¾
# {
#     # ç¬¬ä¸€éƒ¨åˆ†ï¼šç‰¹å¾æ’é™¤é…ç½®
#     'exclude_features': {
#         'current': 'default',
#         'default': [],
#         'exclude_user_behavior': [
#             'user_watch_stk_code',
#             'prefer_bid_code',
#             'hold_bid_code',
#             'user_propernoun'
#         ],
#         'exclude_user_propernoun': [
#             'user_propernoun'
#         ]
#     },

#     # ç¬¬äºŒéƒ¨åˆ†ï¼šç‰¹å¾å¤„ç†æµæ°´çº¿åˆ—è¡¨
#     'pipelines': [
#         # æ¯ä¸ªæµæ°´çº¿çš„åŸºæœ¬ç»“æ„
#         {
#             'embedding_dim': <int>,          # åµŒå…¥ç»´åº¦
#             'feat_name': <str>,              # ç‰¹å¾åç§°
#             'feat_type': <str>,              # ç‰¹å¾ç±»å‹
#             'input_sample': <str>,           # è¾“å…¥æ ·ä¾‹
#             'vocabulary_size': <int>,        # è¯æ±‡è¡¨å¤§å°
#             'operations': [                  # æ“ä½œåˆ—è¡¨
#                 {
#                     'col_in': <str>,         # è¾“å…¥åˆ—
#                     'col_out': <str>,        # è¾“å‡ºåˆ—
#                     'func_name': <str>,      # å‡½æ•°å
#                     'func_parameters': {     # å‡½æ•°å‚æ•°
#                         <param_name>: <value>
#                     }
#                 },
#                 # ... å¯ä»¥æœ‰å¤šä¸ªæ“ä½œ
#             ]
#         },
#         # ... å¯ä»¥æœ‰å¤šä¸ªæµæ°´çº¿
#     ]
# }

# è¿™é‡Œéœ€è¦è¡¥å……ä»‹ç»çš„å°±æ˜¯yaml.safe_load(f)å‡½æ•°æ˜¯æ€ä¹ˆè§£æçš„æµç¨‹
# 1. è¯æ³•åˆ†æ
#     è¯†åˆ«
#         ç¼©è¿›çº§åˆ«
#         æ ‡ç‚¹ç¬¦å·ï¼ˆï¼šï¼Œ-ï¼Œç­‰ï¼‰
#         æ ‡é‡å€¼ï¼ˆstrï¼Œæ•°å­—ï¼Œboolï¼‰
#         æ¢è¡Œç¬¦
# 2. è¯­æ³•åˆ†æ
#     è¯†åˆ«
#         Mappingï¼ˆä½¿ç”¨ï¼šç»“æ„ï¼‰
#             number: 123        # è½¬æ¢ä¸º int
#             float: 123.45     # è½¬æ¢ä¸º float
#             boolean: true     # è½¬æ¢ä¸º bool
#             string: "hello"   # è½¬æ¢ä¸º str
#             null_value: null  # è½¬æ¢ä¸º None
#         Sequenceï¼ˆä½¿ç”¨ - å¼€å¤´çš„åˆ—è¡¨é¡¹ï¼‰
#         åµŒå¥—ç»“æ„ï¼ˆé€šè¿‡ç¼©è¿›è¡¨ç¤ºå±‚çº§å…³ç³»ï¼‰

# æˆ‘ä»¬æ ¸å¿ƒæ˜¯æƒ³çœ‹ä¸€ä¸‹ä¸€ä¸ªpipelineä¸­çš„ä¸€ä¸ªç‰¹å¾ ä»–è¢«è§£ææˆäº†ä»€ä¹ˆç»“æ„
# é€‚é… config.yml å’Œ feat.yml çš„ä¸åŒç»“æ„
if 'pipelines' in feat_config:
    # feat.yml æ ¼å¼
    example_pipeline = feat_config['pipelines'][0]
elif 'process' in feat_config and 'pipelines' in feat_config['process']:
    # config.yml æ ¼å¼
    example_pipeline = feat_config['process']['pipelines'][0]
else:
    print("âš ï¸ æ— æ³•æ‰¾åˆ°ç‰¹å¾é…ç½®æ ¼å¼")
    example_pipeline = None

if example_pipeline:
    pprint(example_pipeline)
# {'embedding_dim': 8,
#  'feat_name': 'hour',
#  'feat_type': 'sparse',
#  'input_sample': '2024-08-02 00:44:05',
#  'operations': [{'col_in': 'create_time',
#                  'col_out': 'create_time',
#                  'func_name': 'fillna',
#                  'func_parameters': {'na_value': '2024-08-02 00:16:34'}},
#                 {'col_in': 'create_time',
#                  'col_out': 'hour',
#                  'func_name': 'to_hour',
#                  'func_parameters': {}}],
#  'vocabulary_size': 24}

# 04 å®šä¹‰ç‰¹å¾æ“ä½œå‡½æ•°
# å®šä¹‰ä¸€ä¸ªå•ç‹¬çš„pyæ–‡ä»¶æ¥å®ç°è¿™ä¸ªæ“ä½œ å¹¶æ„å»ºæ“ä½œä¸­å¿ƒ
# è¿™æ­¥éª¤çš„æµç¨‹å°±æ˜¯è§£è€¦æˆåŸå­æ“ä½œç‰ˆæœ¬çš„Hugging Faceæ¡†æ¶é‡Œçš„
# - ğŸŒŸæ‰¹é‡å¤„ç†æ‰€æœ‰åˆ†å‰²
#     def preprocess(examples):
#         return tokenizer(examples['text'], truncation=True) # æ˜¯å¦æˆªæ–­

#     processed_datasets = datasets.map(preprocess, batched=True)
# - ğŸŒŸå½“ç„¶ä¹Ÿå¯ä»¥å•æ ·æœ¬å¤„ç†
#     datasets = datasets.map(lambda x: {'length': len(x['text'])})
# åªæ˜¯è¿™é‡Œä¸ºäº†ç‰¹å¾å¤„ç†çš„é€šç”¨æ€§è§£è€¦æˆäº†ä¸€ä¸ªä¸€å †åŸå­æ“ä½œå‡½æ•°é¡ºåºæ‰§è¡Œçš„pipelineå½¢å¼æ¥æ‹¼æˆè¿™ä¸ªpreprocesså‡½æ•°
# å¹¶ä¸”è¿™ä¸€å †å‡½æ•°çš„æ‹¼æ¥é¡ºåº å’Œ æ¯ä¸ªå‡½æ•°ä¼ å…¥ä»€ä¹ˆå‚æ•° éƒ½ç”¨yamlæ–‡ä»¶ å½“ä½œè®¾è®¡è“å›¾è§£è€¦çš„å®šä¹‰åœ¨å¤–é¢
# é€šè¿‡transform_func = partial(OP_HUB[func_name], **parameters) çš„æ–¹å¼åšæˆåŸå­æ“ä½œå‡½æ•°
# ç„¶å
# if isinstance(col_in, list):
#     df[col_out] = df[col_in].apply(lambda row: transform_func(*row), axis=1)
# else:
#     df[col_out] = df[col_in].apply(transform_func)
# çš„æ–¹å¼æ‰§è¡Œä¸‹å» ç›¸å½“äº datasets.mapä¸‹å»
# åŒºåˆ«æ˜¯åŸæ¥æ˜¯ä¸€ä¸ªå‡½æ•° mapä¸‹å»å®Œæˆå¤„ç† è¿™é‡Œæ˜¯ä¸€å †æŒ‰è“å›¾ä¼ å‚å’Œæ‹¼æ¥åŸå­å‡½æ•° ä¸€ä¸ªä¸€ä¸ªmapä¸‹å»

# å®šä¹‰ç¼ºå¤±å€¼å¸¸é‡
MISSING_VALUE = [None, '', 'null', 'NULL', 'None', np.nan]

def fillna(x: Union[float, int, str], na_value: Union[float, int, str]) -> Union[float, int, str]:
    """å¡«å……ç¼ºå¤±å€¼"""
    if x in MISSING_VALUE or (isinstance(x, float) and pd.isna(x)):
        return na_value
    return x

def split(x: str, sep: str) -> List[str]:
    """å­—ç¬¦ä¸²åˆ†å‰²"""
    return str(x).split(sep)

def seperation(x: List[str], sep: str) -> List[List[str]]:
    """åˆ—è¡¨å…ƒç´ äºŒæ¬¡åˆ†å‰²"""
    if not isinstance(x, list):
        return []
    return [item.split(sep) for item in x]

def list_get(x: List[List[Any]], item_index: int) -> List[Any]:
    """è·å–åµŒå¥—åˆ—è¡¨ä¸­æŒ‡å®šä½ç½®çš„å…ƒç´ """
    if not isinstance(x, list):
        return []
    result = []
    for sublist in x:
        if isinstance(sublist, list) and len(sublist) > item_index:
            result.append(sublist[item_index])
        else:
            result.append('null')
    return result

def remove_items(x: List[str], target_values: List[str]) -> List[str]:
    """ç§»é™¤åˆ—è¡¨ä¸­çš„æŒ‡å®šå…ƒç´ """
    if not isinstance(x, list):
        return []
    return [item for item in x if item not in target_values]

def padding(x: List[Any], pad_value: Union[str, float, int], max_len: int) -> List[Any]:
    """åˆ—è¡¨å¡«å……åˆ°æŒ‡å®šé•¿åº¦"""
    if not isinstance(x, list):
        x = []
    if len(x) >= max_len:
        return x[:max_len]
    else:
        return x + [pad_value] * (max_len - len(x))

def list_hash(x: List[str], vocabulary_size: int) -> List[int]:
    """å¯¹åˆ—è¡¨ä¸­æ¯ä¸ªå…ƒç´ è¿›è¡Œå“ˆå¸Œ"""
    if not isinstance(x, list):
        return []
    result = []
    for item in x:
        hash_val = int(md5(str(item).encode()).hexdigest(), 16) % vocabulary_size
        result.append(hash_val)
    return result

def str_hash(x: str, vocabulary_size: int) -> int:
    """å­—ç¬¦ä¸²å“ˆå¸Œ"""
    return int(md5(str(x).encode()).hexdigest(), 16) % vocabulary_size

def to_hour(x: str) -> int:
    """æå–æ—¶é—´ä¸­çš„å°æ—¶"""
    try:
        dt = pd.to_datetime(x)
        return dt.hour
    except:
        return 0

def to_weekday(x: str) -> int:
    """æå–æ—¶é—´ä¸­çš„æ˜ŸæœŸ"""
    try:
        dt = pd.to_datetime(x)
        return dt.weekday()
    except:
        return 0

def list_len(x: List) -> int:
    """åˆ—è¡¨é•¿åº¦"""
    if isinstance(x, list):
        return len(x)
    return 0

def int_max(x: int, max_value: int) -> int:
    """é™åˆ¶æ•´æ•°æœ€å¤§å€¼"""
    return min(int(x), max_value)

def json_object_to_list(x: str, key: str) -> List[str]:
    """ä»JSONå¯¹è±¡åˆ—è¡¨ä¸­æå–æŒ‡å®šé”®çš„å€¼"""
    try:
        data = json.loads(x)
        if isinstance(data, list):
            return [item.get(key, 'null') for item in data if isinstance(item, dict)]
        return ['null']
    except:
        return ['null']

def map_to_int(x: Union[str, List], map_dict: Dict[str, int], default_code: int = 0) -> Union[List[int], int]:
    """æ˜ å°„åˆ°æ•´æ•°"""
    if isinstance(x, list):
        return [map_dict.get(item, default_code) for item in x]
    else:
        return map_dict.get(str(x), default_code)

# æ„å»ºæ“ä½œä¸­å¿ƒ (OP_HUB)
OP_HUB = {
    'fillna': fillna,
    'split': split,
    'seperation': seperation,
    'list_get': list_get,
    'remove_items': remove_items,
    'padding': padding,
    'list_hash': list_hash,
    'str_hash': str_hash,
    'to_hour': to_hour,
    'to_weekday': to_weekday,
    'list_len': list_len,
    'int_max': int_max,
    'json_object_to_list': json_object_to_list,
    'map_to_int': map_to_int
}

print(f"OP_HUB æ„å»ºå®Œæˆï¼ŒåŒ…å« {len(OP_HUB)} ä¸ªæ“ä½œå‡½æ•°")
print(f"å¯ç”¨å‡½æ•°: {list(OP_HUB.keys())}")

# 05 å®ç°åŸå­æ“ä½œä½œç”¨äºdf
def run_one_op(df: pd.DataFrame, operation: dict) -> pd.DataFrame:
    """æ‰§è¡Œå•ä¸ªç‰¹å¾æ“ä½œ"""
    # è·å–æ“ä½œé…ç½®
    col_in = operation['col_in']
    col_out = operation['col_out']
    func_name = operation['func_name']
    parameters = operation.get('func_parameters', {})
    
    # æ£€æŸ¥å‡½æ•°æ˜¯å¦å­˜åœ¨
    if func_name not in OP_HUB:
        return df
    
    # æ£€æŸ¥è¾“å…¥åˆ—æ˜¯å¦å­˜åœ¨
    input_cols = [col_in] if isinstance(col_in, str) else col_in
    if not all(col in df.columns for col in input_cols):
        return df
    
    # å‡†å¤‡ç‰¹å¾è½¬æ¢å‡½æ•°
    transform_func = partial(OP_HUB[func_name], **parameters)
    
    # æ‰§è¡Œç‰¹å¾è½¬æ¢
    if isinstance(col_in, list):
        df[col_out] = df[col_in].apply(lambda row: transform_func(*row), axis=1)
    else:
        df[col_out] = df[col_in].apply(transform_func)
    
    return df
#   1. ç»™ä»–ä¼ å…¥çš„æ˜¯ operation
#   operationsæ˜¯list operationæ˜¯dict
#  'operations': [{'col_in': 'create_time',
#                  'col_out': 'create_time',
#                  'func_name': 'fillna',
#                  'func_parameters': {'na_value': '2024-08-02 00:16:34'}},
#                 {'col_in': 'create_time',
#                  'col_out': 'hour',
#                  'func_name': 'to_hour',
#                  'func_parameters': {}}],
#  'operation' ï¼š {'col_in': 'create_time',
#                  'col_out': 'create_time',
#                  'func_name': 'fillna',
#                  'func_parameters': {'na_value': '2024-08-02 00:16:34'}}

# 2. å…ˆå®šä¹‰å¥½æ•°æ®æ£€æŸ¥
# input_cols = [col_in] if isinstance(col_in, str) else col_in
# è¿™å¥è¯çš„ä½œç”¨æ˜¯
# æƒ…å†µ1ï¼šå•åˆ—è¾“å…¥
# col_in = "user_id"
# input_cols = ["user_id"]  # è½¬æˆåˆ—è¡¨
# # æƒ…å†µ2ï¼šå¤šåˆ—è¾“å…¥
# col_in = ["first_name", "last_name"]
# input_cols = ["first_name", "last_name"]  # ä¿æŒä¸å˜

# 3. å®šä¹‰å¤„ç†å‡½æ•°
# transform_func = partial(OP_HUB[func_name], **parameters)
# ä» OP_HUB å­—å…¸ä¸­è·å–åä¸º func_name çš„å‡½æ•°
# ä½¿ç”¨ partial æŠŠå‡½æ•°å‚æ•° parameters å›ºå®šè¿›å»
# # å‡è®¾æœ‰è¿™æ ·ä¸€ä¸ªå‡½æ•°åœ¨ OP_HUB ä¸­
# def fillna(value, na_value="null"):
#     return value if value else na_value
# # é…ç½®æ˜¯è¿™æ ·çš„
# func_name = "fillna"
# parameters = {"na_value": "unknown"}
# # partial åç›¸å½“äº
# transform_func = lambda value: fillna(value, na_value="unknown")

# 4. å®šä¹‰å¤„ç†å‡½æ•°æ€ä¹ˆä½œç”¨äºæ•°æ®é›†
# if isinstance(col_in, list): # æ”¯æŒå¤šåˆ—æ“ä½œ
#     df[col_out] = df[col_in].apply(lambda row: transform_func(*row), axis=1)
# else: # å•åˆ—æ“ä½œ
#     df[col_out] = df[col_in].apply(transform_func)

# å•åˆ—è¾“å…¥ï¼šç›´æ¥å¯¹è¿™ä¸€åˆ—çš„æ¯ä¸ªå€¼åº”ç”¨è½¬æ¢å‡½æ•°
# å¤šåˆ—è¾“å…¥ï¼šæŠŠå¤šåˆ—çš„å€¼ä½œä¸ºå‚æ•°ä¼ ç»™è½¬æ¢å‡½æ•°
# *row çš„ä½œç”¨æ˜¯æŠŠè¡Œçš„å€¼å±•å¼€ä½œä¸ºå‡½æ•°å‚æ•°
# axis=1 è¡¨ç¤ºæŒ‰è¡Œå¤„ç†è€Œä¸æ˜¯æŒ‰åˆ—

# å‡è®¾æ•°æ®æ˜¯ï¼š
# å•åˆ—æ“ä½œ
# df = pd.DataFrame({
#     'name': ['Alice', None, 'Bob']
# })
# # é…ç½®æ˜¯ï¼š
# col_in = 'name'
# col_out = 'name_filled'
# func_name = 'fillna'
# parameters = {'na_value': 'unknown'}
# # æ‰§è¡Œåç›¸å½“äºï¼š
# df['name_filled'] = df['name'].apply(lambda x: x if x else 'unknown')
# # ç»“æœï¼š['Alice', 'unknown', 'Bob']
# å¤šåˆ—æ“ä½œ
# # å‡è®¾æ•°æ®æ˜¯ï¼š
# df = pd.DataFrame({
#     'first_name': ['Alice', 'Bob', None],
#     'last_name': ['Smith', None, 'Johnson']
# })
# # é…ç½®æ˜¯ï¼š
# col_in = ['first_name', 'last_name']
# col_out = 'full_name'
# func_name = 'concat_names'
# parameters = {'sep': ' '}
# # æ‰§è¡Œåç›¸å½“äºï¼š
# df['full_name'] = df[['first_name', 'last_name']].apply(
#     lambda row: f"{row['first_name']} {row['last_name']}", 
#     axis=1
# )
# # ç»“æœï¼š['Alice Smith', 'Bob None', 'None Johnson']

# 06 å®ç°åŸå­æ“ä½œæ‹¼æˆçš„å®Œæ•´processå‡½æ•°ä½œç”¨äºdf
def process_feature_pipelines(df_raw: pd.DataFrame, feat_config: dict) -> tuple[pd.DataFrame, list]:
    """æ‰§è¡Œç‰¹å¾å·¥ç¨‹æµæ°´çº¿ - é€‚é…ä¸åŒçš„é…ç½®æ–‡ä»¶æ ¼å¼"""
    # åˆ›å»ºæ•°æ®å‰¯æœ¬
    df = df_raw.copy()
    
    # è·å–éœ€è¦å¤„ç†çš„æµæ°´çº¿ - é€‚é…ä¸åŒæ ¼å¼
    if 'pipelines' in feat_config:
        # feat.yml æ ¼å¼: { pipelines: [...] }
        pipelines = feat_config['pipelines']
    elif 'process' in feat_config and 'pipelines' in feat_config['process']:
        # config.yml æ ¼å¼: { process: { pipelines: [...] } }
        pipelines = feat_config['process']['pipelines']
    else:
        print("âš ï¸ æ— æ³•æ‰¾åˆ°ç‰¹å¾é…ç½®ä¸­çš„ pipelines")
        return df, []

    # è®°å½•æˆåŠŸå¤„ç†çš„ç‰¹å¾
    processed_features = []
    
    # æ‰§è¡Œæ¯ä¸ªç‰¹å¾å¤„ç†æµæ°´çº¿
    for pipeline in pipelines:
        feat_name = pipeline['feat_name']
        operations = pipeline['operations']
        
        # æ‰§è¡Œæµæ°´çº¿ä¸­çš„æ¯ä¸ªæ“ä½œ
        for operation in operations:
            df = run_one_op(df, operation)

        # è®°å½•å¤„ç†æˆåŠŸçš„ç‰¹å¾
        processed_features.append(feat_name)
    
    return df, processed_features

df_processed, processed_features = process_feature_pipelines(df_raw, feat_config)

# 07 åˆ†æå¤„ç†ç»“æœ
# æ¯”è¾ƒå¤„ç†å‰åçš„æ•°æ®ç»“æ„
print("æ•°æ®ç»“æ„å¯¹æ¯”:")
print(f"åŸå§‹åˆ—æ•°: {len(df_raw.columns)}")
print(f"å¤„ç†ååˆ—æ•°: {len(df_processed.columns)}")
print(f"æ–°å¢åˆ—æ•°: {len(df_processed.columns) - len(df_raw.columns)}")

print("\nåŸå§‹åˆ—å:")
print(list(df_raw.columns))

print("\næ–°å¢åˆ—å:")
new_columns = [col for col in df_processed.columns if col not in df_raw.columns]
print(new_columns)
# æ•°æ®ç»“æ„å¯¹æ¯”ï¼ˆåŒ…å«æœ‰å¾ˆå¤šä¸­é—´ç‰¹å¾åˆ—ï¼‰
# åŸå§‹åˆ—æ•°: 13
# å¤„ç†ååˆ—æ•°: 30
# æ–°å¢åˆ—æ•°: 17

# åŸå§‹åˆ—å:
# ['user_id', 'create_time', 'log_type', 'watchlists', 'holdings', 'country', 'prefer_bid', 'user_propernoun', 'push_title', 'push_content', 'item_code', 'item_tags', 'submit_type']

# æ–°å¢åˆ—å:
# ['hour', 'weekday', 'user_watch_stk_code', 'user_watch_stk_code_hash', 'country_hash', 'prefer_bid_code', 'prefer_bid_code_hash', 'hold_bid_code', 'hold_bid_code_hash', 'user_propernoun_code', 'user_propernoun_hash', 'push_title_hash', 'title_len', 'item_code_hash', 'submit_type_hash', 'tagIds', 'tag_id_hash']
# æŸ¥çœ‹æˆåŠŸç”Ÿæˆçš„ç‰¹å¾
print("æˆåŠŸç”Ÿæˆçš„ç‰¹å¾è¯¦æƒ…:")
for feat_name in processed_features:
    if feat_name in df_processed.columns:
        sample_data = df_processed[feat_name].iloc[0]
        data_type = type(sample_data).__name__
        print(f"  {feat_name}: {data_type} = {sample_data}")
# æˆåŠŸç”Ÿæˆçš„ç‰¹å¾è¯¦æƒ…:
#   hour: int64 = 9
#   weekday: int64 = 4
#   user_watch_stk_code_hash: list = [5353, 3385, 2290, 7019, 6324]
#   country_hash: int64 = 106
#   prefer_bid_code_hash: list = [2449, 9133, 9212, 4180, 304]
#   hold_bid_code_hash: list = [1766, 9287, 1277, 304, 777]
#   user_propernoun_hash: list = [8381, 8381, 8381, 8381, 8381]
#   push_title_hash: int64 = 7
#   title_len: int64 = 14
#   item_code_hash: list = [304, 8381, 8381, 8381, 8381]
#   submit_type_hash: int64 = 6
#   tag_id_hash: list = [8139, 798, 880]
# æ˜¾ç¤ºæœ€ç»ˆå¤„ç†ç»“æœçš„éƒ¨åˆ†æ•°æ®
print("æœ€ç»ˆå¤„ç†ç»“æœé¢„è§ˆ:")
display_cols = ['user_id', 'log_type'] + processed_features
display_cols = [col for col in display_cols if col in df_processed.columns]

print(df_processed[display_cols])
# user_id	log_type	hour	weekday	user_watch_stk_code_hash	country_hash	prefer_bid_code_hash	hold_bid_code_hash	user_propernoun_hash	push_title_hash	title_len	item_code_hash	submit_type_hash	tag_id_hash
# 0	1800000316	PR	9	4	[5353, 3385, 2290, 7019, 6324]	106	[2449, 9133, 9212, 4180, 304]	[1766, 9287, 1277, 304, 777]	[8381, 8381, 8381, 8381, 8381]	7	14	[304, 8381, 8381, 8381, 8381]	6	[8139, 798, 880]
# 1	1800000316	PR	9	4	[5353, 3385, 2290, 7019, 6324]	106	[2449, 9133, 9212, 4180, 304]	[1766, 9287, 1277, 304, 777]	[8381, 8381, 8381, 8381, 8381]	7	14	[304, 8381, 8381, 8381, 8381]	6	[8139, 798, 880]
# 2	1800001318	PR	19	4	[898, 5602, 5053, 8877, 5282]	106	[9153, 3036, 898, 6687, 8381]	[9153, 6687, 8381, 8381, 8381]	[8381, 8381, 8381, 8381, 8381]	7	13	[8381, 8381, 8381, 8381, 8381]	4	[4634, 880, 8381]
# 3	1800001318	PR	5	4	[898, 5602, 5053, 8877, 5282]	106	[9153, 3036, 898, 6687, 8381]	[9153, 6687, 8381, 8381, 8381]	[8381, 8381, 8381, 8381, 8381]	0	16	[5687, 8381, 8381, 8381, 8381]	1	[9593, 542, 4797]
# 4	1800001324	PR	13	4	[8381, 8381, 8381, 8381, 8381]	145	[8381, 8381, 8381, 8381, 8381]	[8381, 8381, 8381, 8381, 8381]	[7053, 6272, 8381, 8381, 8381]	2	17	[7759, 8381, 8381, 8381, 8381]	1	[542, 4530, 4797]

# 08 æ ‘æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°
import yaml
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

def prepare_features(df_processed, processed_features, max_list_length=5):
    """å±•å¼€åˆ—è¡¨ç‰¹å¾"""
    df_tree = df_processed[processed_features].copy()
    
    for feat in processed_features:
        if isinstance(df_tree[feat].iloc[0], list):
            expanded = df_tree[feat].apply(pd.Series).iloc[:, :max_list_length]
            expanded.columns = [f"{feat}_{i}" for i in range(expanded.shape[1])]
            df_tree = df_tree.drop(columns=[feat]).join(expanded)
    
    return df_tree

def train_model(X, y, train_params):
    """è®­ç»ƒLightGBMæ¨¡å‹"""
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    
    train_data = lgb.Dataset(X_train, y_train)
    val_data = lgb.Dataset(X_val, y_val, reference=train_data)
    
    model = lgb.train(
        train_params,
        train_data,
        num_boost_round=train_params.pop('num_iterations', 1000),
        callbacks=[lgb.early_stopping(train_params.pop('early_stopping_rounds', 100))],
        valid_sets=[train_data, val_data],
        valid_names=['train', 'valid']
    )
    
    return model, X_train, X_val, y_train, y_val

def evaluate_model(model, X_train, X_val, y_train, y_val):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    y_train_pred = model.predict(X_train, num_iteration=model.best_iteration)
    y_val_pred = model.predict(X_val, num_iteration=model.best_iteration)
    
    train_auc = roc_auc_score(y_train, y_train_pred)
    val_auc = roc_auc_score(y_val, y_val_pred)
    
    print(f"è®­ç»ƒé›† AUC: {train_auc:.4f}")
    print(f"éªŒè¯é›† AUC: {val_auc:.4f}")
    
    return pd.DataFrame({
        'feature': model.feature_name(),
        'importance': model.feature_importance(importance_type='gain')
    }).sort_values('importance', ascending=False)

# æ ‘æ¨¡å‹ä½¿ç”¨config.ymlé…ç½®
with open('config/config.yml', 'r', encoding='utf-8') as f:
    tree_config = yaml.safe_load(f)

if 'log_type' in df_processed.columns:
    # å‡†å¤‡æ•°æ®
    df_processed['label'] = df_processed['log_type'].apply(lambda x: 1 if x == 'PC' else 0)
    X = prepare_features(df_processed, processed_features)
    y = df_processed['label']
    
    # è®­ç»ƒæ¨¡å‹
    train_params = {**tree_config['train'], 'verbose': -1, 'n_jobs': -1, 'seed': 42}
    model, X_train, X_val, y_train, y_val = train_model(X, y, train_params)
    
    # è¯„ä¼°å¹¶è¾“å‡ºç»“æœ
    feature_importance = evaluate_model(model, X_train, X_val, y_train, y_val)
    print("\nç‰¹å¾é‡è¦æ€§ (Top 20):")
    print(feature_importance.head(20))
else:
    print("é”™è¯¯: æ‰¾ä¸åˆ° 'log_type' åˆ—ï¼Œæ— æ³•è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚")

# 09 æ·±åº¦æ¨¡å‹å…¨æµç¨‹ï¼šç±»ä¼¼Huggig Faceçš„æ¡†æ¶ è¿™é‡Œç”¨çš„æ˜¯tensorflowçš„æ¡†æ¶
# åŸé¡¹ç›®æ·±åº¦æ¨¡å‹çš„æ ¸å¿ƒç‰¹ç‚¹ï¼š
# 1. ä½¿ç”¨ tf.data.Dataset è¿›è¡Œæ•°æ®æµå¤„ç†
# 2. é€šè¿‡ FeaturePipelineBuilder æ„å»ºç‰¹å¾å¤„ç†ç®¡é“
# 3. æ”¯æŒ sparseã€varlen_sparseã€dense ä¸‰ç§ç‰¹å¾ç±»å‹
# 4. ä½¿ç”¨ Embedding + BatchNorm + Dense + Dropout çš„ç»å…¸æ¶æ„

import tensorflow as tf
from keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout, BatchNormalization, Concatenate
from keras.models import Model
from keras.regularizers import l2

# ---------------------------------------------------------------------------- #
# 1. åŸºäºåŸé¡¹ç›®æ¶æ„çš„ç‰¹å¾å¤„ç†å™¨
#    å‚è€ƒ src/models/deep/feature_pipeline.py
# ---------------------------------------------------------------------------- #

class FeaturePipelineBuilder:
    """ç‰¹å¾å¤„ç†ç®¡é“æ„å»ºå™¨ - åŸºäºåŸé¡¹ç›®å®ç°"""
    
    def __init__(self, feat_configs: list, verbose: bool = True):
        self.feat_configs = feat_configs
        self.verbose = verbose
        self.embedding_layers = {}
        self.pooling_layers = {}
    
    def build_feature_pipelines(self) -> list:
        """æ„å»ºç‰¹å¾å¤„ç†ç®¡é“"""
        pipelines = []
        
        for config in self.feat_configs:
            feat_name = config.get('feat_name')
            feat_type = config.get('feat_type')
            
            if not feat_name or not feat_type:
                continue
                
            # åˆ›å»ºå¤„ç†å™¨åºåˆ—
            processors = self._create_processors(config)
            if processors:
                pipelines.append((feat_name, processors))
        
        if self.verbose:
            print(f"æˆåŠŸæ„å»º {len(pipelines)} ä¸ªç‰¹å¾å¤„ç†ç®¡é“")
            for feat_name, processors in pipelines:
                processor_names = [p.__class__.__name__ for p in processors]
                print(f"  {feat_name}: {' -> '.join(processor_names)}")
        
        return pipelines
    
    def _create_processors(self, config: dict) -> list:
        """æ ¹æ®ç‰¹å¾ç±»å‹åˆ›å»ºå¤„ç†å™¨"""
        feat_name = config['feat_name']
        feat_type = config['feat_type']
        vocab_size = config.get('vocabulary_size', 1000)
        embed_dim = config.get('embedding_dim', 8)
        
        if feat_type == 'sparse':
            embedding = Embedding(
                input_dim=vocab_size,
                output_dim=embed_dim,
                name=f'{feat_name}_embedding',
                mask_zero=False
            )
            return [embedding]
        
        elif feat_type == 'varlen_sparse':
            embedding = Embedding(
                input_dim=vocab_size,
                output_dim=embed_dim,
                name=f'{feat_name}_embedding',
                mask_zero=True  # å˜é•¿ç‰¹å¾éœ€è¦masking
            )
            pooling = GlobalAveragePooling1D(name=f'{feat_name}_pooling')
            return [embedding, pooling]
        
        elif feat_type == 'dense':
            # å¯†é›†ç‰¹å¾ç›´æ¥é€šè¿‡ï¼Œå¯ä»¥åŠ BN
            identity = tf.keras.layers.Lambda(lambda x: x, name=f'{feat_name}_identity')
            return [identity]
        
        return []

def process_feature_batch(features_dict: dict, pipelines: list) -> list:
    """å¤„ç†ç‰¹å¾æ‰¹æ¬¡æ•°æ®"""
    outputs = []
    
    for feat_name, processors in pipelines:
        if feat_name not in features_dict:
            continue
        
        # ä¾æ¬¡åº”ç”¨å¤„ç†å™¨
        feature_input = features_dict[feat_name]
        for processor in processors:
            feature_input = processor(feature_input)
        
        outputs.append(feature_input)
    
    return outputs

# ---------------------------------------------------------------------------- #
# 2. MLP æ¨¡å‹å®šä¹‰ï¼ˆåŸºäºåŸé¡¹ç›®æ¶æ„ï¼‰
#    å‚è€ƒ src/models/deep/mlp.py
# ---------------------------------------------------------------------------- #

class DeepMLP(tf.keras.Model):
    """æ·±åº¦MLPæ¨¡å‹ - åŸºäºåŸé¡¹ç›®å®ç°"""
    
    def __init__(self, feat_configs: list, train_config: dict = None, verbose: bool = True):
        super(DeepMLP, self).__init__()
        
        # æ„å»ºç‰¹å¾å¤„ç†ç®¡é“
        pipeline_builder = FeaturePipelineBuilder(feat_configs, verbose=verbose)
        self.feature_pipelines = pipeline_builder.build_feature_pipelines()
        
        # ç‰¹å¾è¿æ¥å±‚
        self.concat_layer = Concatenate(axis=1)
        
        # è·å–æ¨¡å‹å‚æ•° - ä»train.ymlåŠ¨æ€è¯»å–
        model_params = (train_config or {}).get('model', {})
        hidden_layers = model_params.get('layers', [128, 64, 32])  # ä½¿ç”¨train.ymlçš„é»˜è®¤å€¼
        dropout_rates = model_params.get('dropout_rates', [0.3, 0.3, 0.2])  # ä½¿ç”¨train.ymlçš„é»˜è®¤å€¼
        l2_reg = model_params.get('l2_regularization', 0.001)
        
        # æ„å»ºåˆ†ç±»å™¨
        self.classifier = self._build_classifier(hidden_layers, dropout_rates, l2_reg)
    
    def _build_classifier(self, hidden_layers: list, dropout_rates: list, l2_reg: float):
        """æ„å»ºåˆ†ç±»å™¨ç½‘ç»œ"""
        layers = []
        
        # æ·»åŠ BatchNorm
        layers.append(BatchNormalization())
        
        # æ·»åŠ éšè—å±‚
        for i, units in enumerate(hidden_layers):
            layers.append(Dense(
                units, 
                activation='relu',
                kernel_regularizer=l2(l2_reg)
            ))
            if i < len(dropout_rates):
                layers.append(Dropout(dropout_rates[i]))
        
        # è¾“å‡ºå±‚
        layers.append(Dense(1, activation='sigmoid', kernel_regularizer=l2(l2_reg)))
        
        return tf.keras.Sequential(layers)
    
    def call(self, features, training=None):
        """å‰å‘ä¼ æ’­"""
        # å¤„ç†æ‰€æœ‰ç‰¹å¾
        processed_outputs = process_feature_batch(features, self.feature_pipelines)
        
        if not processed_outputs:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„ç‰¹å¾è¾“å‡º")
        
        # åˆå¹¶ç‰¹å¾
        if len(processed_outputs) > 1:
            concat_features = self.concat_layer(processed_outputs)
        else:
            concat_features = processed_outputs[0]
        
        # åº”ç”¨åˆ†ç±»å™¨
        predictions = self.classifier(concat_features, training=training)
        return predictions

# ---------------------------------------------------------------------------- #
# 3. æ•°æ®å‡†å¤‡å‡½æ•°ï¼ˆåŸºäºåŸé¡¹ç›®çš„æ•°æ®æ ¼å¼ï¼‰
# ---------------------------------------------------------------------------- #

def prepare_tf_dataset_for_deep_model(df: pd.DataFrame, feat_config: dict, batch_size: int = 512) -> tf.data.Dataset:
    """å‡†å¤‡TensorFlowæ•°æ®é›†ç”¨äºæ·±åº¦æ¨¡å‹è®­ç»ƒ"""
    
    # è·å–éœ€è¦çš„ç‰¹å¾ - é€‚é…ä¸åŒçš„é…ç½®æ–‡ä»¶æ ¼å¼
    if 'pipelines' in feat_config:
        # feat.yml æ ¼å¼
        pipelines = feat_config['pipelines']
    elif 'process' in feat_config and 'pipelines' in feat_config['process']:
        # config.yml æ ¼å¼
        pipelines = feat_config['process']['pipelines']
    else:
        raise ValueError("æ— æ³•æ‰¾åˆ°ç‰¹å¾é…ç½®ä¸­çš„ pipelines")
    
    pipeline_feats = {p['feat_name']: p for p in pipelines}
    features_dict = {}
    
    for feat_name, config in pipeline_feats.items():
        if feat_name not in df.columns:
            continue
        
        feat_type = config.get('feat_type', 'sparse')
        
        if feat_type == 'sparse':
            # å•å€¼ç‰¹å¾ï¼Œshapeä¸º (batch_size,) - è¿™æ˜¯å…³é”®ï¼
            values = df[feat_name].values.astype(np.int32)
            features_dict[feat_name] = values
            
        elif feat_type == 'varlen_sparse':
            # å˜é•¿ç‰¹å¾ï¼Œéœ€è¦padding
            sequences = df[feat_name].tolist()
            max_len = max(len(seq) if isinstance(seq, list) else 1 for seq in sequences)
            
            padded_sequences = []
            for seq in sequences:
                if isinstance(seq, list):
                    padded = seq + [0] * (max_len - len(seq))
                else:
                    padded = [int(seq)] + [0] * (max_len - 1)
                padded_sequences.append(padded[:max_len])
            
            features_dict[feat_name] = np.array(padded_sequences, dtype=np.int32)
            
        elif feat_type == 'dense':
            # å¯†é›†ç‰¹å¾
            values = df[feat_name].values.astype(np.float32).reshape(-1, 1)
            features_dict[feat_name] = values
    
    # å‡†å¤‡æ ‡ç­¾
    labels = df['label'].values.astype(np.int32)
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = tf.data.Dataset.from_tensor_slices((features_dict, labels))
    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)
    
    return dataset

# ---------------------------------------------------------------------------- #
# 4. è®­ç»ƒæµç¨‹
# ---------------------------------------------------------------------------- #

print("\n--- å¼€å§‹åŸé¡¹ç›®é£æ ¼çš„æ·±åº¦æ¨¡å‹è®­ç»ƒ ---")

# æ·±åº¦æ¨¡å‹è¯»å–train.ymlé…ç½®æ–‡ä»¶
print("ğŸ”„ è¯»å–æ·±åº¦æ¨¡å‹é…ç½®æ–‡ä»¶ (train.yml)...")
with open('config/train.yml', 'r', encoding='utf-8') as f:
    train_config = yaml.safe_load(f)

# æ˜¾ç¤ºè¯»å–çš„é…ç½®ä¿¡æ¯
print(f"ğŸ“‹ ä½¿ç”¨çš„è®­ç»ƒé…ç½®:")
training_config = train_config.get('training', {})
print(f"  Batch Size: {training_config.get('batch_size', 'N/A')}")
print(f"  Epochs: {training_config.get('epochs', 'N/A')}")
print(f"  Learning Rate: {training_config.get('lr', 'N/A')}")
print(f"  Validation Split: {training_config.get('validation_split', 'N/A')}")

model_config = train_config.get('model', {})
print(f"  Model Layers: {model_config.get('layers', 'N/A')}")
print(f"  Dropout Rates: {model_config.get('dropout_rates', 'N/A')}")
print(f"  L2 Regularization: {model_config.get('l2_regularization', 'N/A')}")

# ç¡®ä¿æœ‰æ ‡ç­¾ - ä¸ºåŸå§‹æ•°æ®æ·»åŠ æ ‡ç­¾
if 'label' not in df_raw.columns:
    df_raw['label'] = df_raw['log_type'].apply(lambda x: 1 if x == 'PC' else 0)

# 1. å‡†å¤‡æ•°æ®é›† - æ·±åº¦æ¨¡å‹åº”è¯¥ç‹¬ç«‹å¤„ç†åŸå§‹æ•°æ®
print("ğŸ”„ é‡æ–°å¤„ç†åŸå§‹æ•°æ®ç”¨äºæ·±åº¦æ¨¡å‹...")
print("ğŸ“‹ åŸé¡¹ç›®æ¶æ„è¯´æ˜:")
print("  ğŸŒ³ æ ‘æ¨¡å‹: config.yml (ç‰¹å¾å·¥ç¨‹) + config.yml (è®­ç»ƒå‚æ•°)")
print("  ğŸ§  æ·±åº¦æ¨¡å‹: feat.yml (ç‰¹å¾å·¥ç¨‹) + train.yml (è®­ç»ƒå‚æ•°)")
print("  ğŸ“Š ä¸¤ä¸ªæ¨¡å‹ç‹¬ç«‹å¤„ç†ç›¸åŒçš„åŸå§‹æ•°æ®ï¼Œå„è‡ªä¼˜åŒ–")

# é‡æ–°ä»åŸå§‹æ•°æ®å¼€å§‹ï¼Œåº”ç”¨feat.ymlé…ç½®è¿›è¡Œç‰¹å¾å·¥ç¨‹
df_deep_processed, _ = process_feature_pipelines(df_raw, deep_feat_config)

batch_size = train_config.get('training', {}).get('batch_size', 256)  # ä½¿ç”¨train.ymlçš„é»˜è®¤å€¼
full_dataset = prepare_tf_dataset_for_deep_model(df_deep_processed, deep_feat_config, batch_size)

print(f"å·²åˆ›å»ºTensorFlowæ•°æ®é›†ï¼Œbatch_size={batch_size}")

# éªŒè¯æ•°æ®é›†æ ¼å¼
for features, labels in full_dataset.take(1):
    print("æ•°æ®é›†æ ¼å¼éªŒè¯:")
    print(f"  ç‰¹å¾æ•°é‡: {len(features)}")
    for name, tensor in features.items():
        print(f"  - {name}: shape={tensor.shape}, dtype={tensor.dtype}")
    print(f"  æ ‡ç­¾: shape={labels.shape}, dtype={labels.dtype}")
    break

# 2. åˆ›å»ºæ¨¡å‹
print("\næ„å»ºæ·±åº¦MLPæ¨¡å‹...")
# è·å–æ·±åº¦æ¨¡å‹çš„ç‰¹å¾é…ç½®
if 'pipelines' in deep_feat_config:
    deep_pipelines = deep_feat_config['pipelines']
elif 'process' in deep_feat_config and 'pipelines' in deep_feat_config['process']:
    deep_pipelines = deep_feat_config['process']['pipelines']
else:
    raise ValueError("æ— æ³•æ‰¾åˆ°æ·±åº¦æ¨¡å‹ç‰¹å¾é…ç½®ä¸­çš„ pipelines")

deep_model = DeepMLP(deep_pipelines, train_config, verbose=True)

# 3. ç¼–è¯‘æ¨¡å‹
learning_rate = train_config.get('training', {}).get('lr', 0.0005)  # ä½¿ç”¨train.ymlçš„é»˜è®¤å€¼
deep_model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
    loss='binary_crossentropy',
    metrics=[
        tf.keras.metrics.AUC(name='auc'),
        tf.keras.metrics.BinaryAccuracy(name='accuracy')
    ]
)

print(f"æ¨¡å‹ç¼–è¯‘å®Œæˆï¼Œå­¦ä¹ ç‡: {learning_rate}")

# 4. æ•°æ®åˆ†å‰²ï¼ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„validation_splitï¼‰
validation_split = train_config.get('training', {}).get('validation_split', 0.2)
train_size = int((1 - validation_split) * len(df_deep_processed))

print(f"\nğŸ“Š æ•°æ®ä½¿ç”¨æƒ…å†µ:")
print(f"  æ€»æ•°æ®é‡: {len(df_deep_processed):,}")
print(f"  è®­ç»ƒé›†: {train_size:,} ({(1-validation_split)*100:.1f}%)")
print(f"  éªŒè¯é›†: {len(df_deep_processed)-train_size:,} ({validation_split*100:.1f}%)")
print(f"  è®­ç»ƒæ‰¹æ¬¡æ•°: {train_size // batch_size}")
print(f"  éªŒè¯æ‰¹æ¬¡æ•°: {(len(df_deep_processed)-train_size) // batch_size}")

train_dataset = full_dataset.take(train_size // batch_size)
val_dataset = full_dataset.skip(train_size // batch_size)

# 5. è®­ç»ƒæ¨¡å‹
epochs = train_config.get('training', {}).get('epochs', 2)  # ä½¿ç”¨train.ymlçš„é»˜è®¤å€¼
print(f"\nå¼€å§‹è®­ç»ƒæ·±åº¦æ¨¡å‹... (epochs={epochs})")

history = deep_model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=epochs,
    verbose=1
)

# 6. è¾“å‡ºç»“æœ
print("\n--- æ·±åº¦æ¨¡å‹è®­ç»ƒå®Œæˆ ---")
if 'val_auc' in history.history:
    final_train_auc = history.history['auc'][-1]
    final_val_auc = history.history['val_auc'][-1]
    print(f"æœ€ç»ˆè®­ç»ƒé›† AUC: {final_train_auc:.4f}")
    print(f"æœ€ç»ˆéªŒè¯é›† AUC: {final_val_auc:.4f}")
    print(f"AUCå·®å¼‚: {final_train_auc - final_val_auc:.4f}")
else:
    final_auc = history.history['auc'][-1]
    print(f"æœ€ç»ˆ AUC: {final_auc:.4f}")

print(f"\nğŸ“‹ æ·±åº¦æ¨¡å‹æ¶æ„æ€»ç»“:")
print(f"  ğŸ”§ ç‰¹å¾å¤„ç†ç®¡é“: {len(deep_model.feature_pipelines)} ä¸ª")
print(f"  ğŸ—ï¸  åˆ†ç±»å™¨å±‚æ•°: {len([l for l in deep_model.classifier.layers if isinstance(l, Dense)])}")
print(f"  ğŸ“Š æ€»å‚æ•°é‡: {deep_model.count_params():,}")

# æ˜¾ç¤ºæ·±åº¦æ¨¡å‹è®­ç»ƒç»“æœ
print(f"\nğŸ”„ æ·±åº¦æ¨¡å‹è®­ç»ƒç»“æœ:")
if 'val_auc' in history.history:
    print(f"  éªŒè¯é›†AUC: {final_val_auc:.4f}")
    print(f"  è®­ç»ƒé›†AUC: {history.history['auc'][-1]:.4f}")
else:
    print(f"  AUC: {final_auc:.4f}")

# å¦‚æœéœ€è¦å’Œæ ‘æ¨¡å‹å¯¹æ¯”ï¼Œè¯·é‡æ–°è¿è¡Œå®Œæ•´çš„æµç¨‹

print("\nâœ… å®Œæ•´çš„ç‰¹å¾å·¥ç¨‹ -> æ ‘æ¨¡å‹ -> æ·±åº¦æ¨¡å‹æµæ°´çº¿å·²å®Œæˆï¼")

# 10AFMæ¨¡å‹è®­ç»ƒ

# ---------------------------------------------------------------------------- #
# 5. AFMæ¨¡å‹å®ç° (Attentional Factorization Machines)
# ---------------------------------------------------------------------------- #

import torch
import torch.nn as nn
from torch import Tensor
from itertools import combinations
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import roc_auc_score, accuracy_score, log_loss
import numpy as np

class AttentionalFactorizationMachine(nn.Module):
    """
    æ³¨æ„åŠ›å› å­åˆ†è§£æœº (AFM)
    
    å‚æ•°:
    - num_fields: int. ç‰¹å¾å­—æ®µæ•°é‡
    - emb_dim: int. åµŒå…¥ç»´åº¦
    - attn_dim: int. æ³¨æ„åŠ›ç»´åº¦
    - num_classes: int. ç±»åˆ«æ•°é‡. é»˜è®¤: 1
    - bias: bool. æ˜¯å¦ä½¿ç”¨åç½®. é»˜è®¤: True
    - dropout_rate: float. Dropoutç‡. é»˜è®¤: 0.2
    
    è¾“å…¥:
    - x: Tensor. å½¢çŠ¶: (batch_size, num_fields, emb_dim)
    
    è¿”å›:
    - y: Tensor. å½¢çŠ¶: (batch_size, num_classes)
    """

    def __init__(
        self,
        num_fields: int,
        emb_dim: int,
        attn_dim: int,
        num_classes: int = 1,
        bias: bool = True,
        dropout_rate: float = 0.2,
    ) -> None:
        super().__init__()
        
        # ç‰¹å¾å­—æ®µç»„åˆ
        combs = list(combinations(range(num_fields), 2))
        self.comb_i = [c[0] for c in combs]
        self.comb_j = [c[1] for c in combs]
        
        # æ³¨æ„åŠ›ç½‘ç»œ
        self.linear_attn = nn.Sequential(
            nn.Linear(emb_dim, attn_dim, bias),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate),
            nn.Linear(attn_dim, 1, bias=False),
        )
        
        # è¾“å‡ºå±‚
        self.fc = nn.Linear(emb_dim, num_classes)
        
        # Dropout
        self.dropout = nn.Dropout(dropout_rate)
        
        # å‚æ•°åˆå§‹åŒ–
        self._init_weights()
    
    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

    def forward(self, x: Tensor) -> Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Arguments:
            x -- Tensor. shape: (batch_size, num_fields, emb_dim)
        
        Returns:
            y -- Tensor. shape: (batch_size, num_classes)
        """
        print(f"    ğŸ” AFMå‰å‘ä¼ æ’­å¼€å§‹ï¼Œè¾“å…¥å½¢çŠ¶: {x.shape}")
        
        # è·å–ç‰¹å¾äº¤äº’å¯¹
        print(f"    ğŸ“Š è®¡ç®—ç‰¹å¾äº¤äº’å¯¹ï¼Œç»„åˆæ•°: {len(self.comb_i)}")
        x_i = x[:, self.comb_i]  # (batch_size, num_pairs, emb_dim)
        x_j = x[:, self.comb_j]  # (batch_size, num_pairs, emb_dim)
        print(f"    âœ… ç‰¹å¾äº¤äº’å¯¹è®¡ç®—å®Œæˆï¼Œx_i: {x_i.shape}, x_j: {x_j.shape}")
        
        # å…ƒç´ çº§ä¹˜ç§¯å¾—åˆ°äº¤äº’ç‰¹å¾
        x_cross = x_i * x_j  # (batch_size, num_pairs, emb_dim)
        print(f"    âœ… äº¤äº’ç‰¹å¾è®¡ç®—å®Œæˆï¼Œx_cross: {x_cross.shape}")
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        print(f"    ğŸ§  å¼€å§‹è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°...")
        attn_score = self.linear_attn(x_cross)  # (batch_size, num_pairs, 1)
        print(f"    âœ… æ³¨æ„åŠ›åˆ†æ•°è®¡ç®—å®Œæˆï¼Œattn_score: {attn_score.shape}")
        
        attn_score = torch.softmax(attn_score, dim=1)  # æ³¨æ„åŠ›æƒé‡å½’ä¸€åŒ–
        print(f"    âœ… æ³¨æ„åŠ›æƒé‡å½’ä¸€åŒ–å®Œæˆ")
        
        # åŠ æƒæ±‚å’Œå¾—åˆ°æœ€ç»ˆç‰¹å¾è¡¨ç¤º
        f = torch.sum(attn_score * x_cross, dim=1)  # (batch_size, emb_dim)
        print(f"    âœ… åŠ æƒæ±‚å’Œå®Œæˆï¼Œf: {f.shape}")
        
        # åº”ç”¨dropout
        f = self.dropout(f)
        print(f"    âœ… Dropoutå®Œæˆ")
        
        # è¾“å‡ºé¢„æµ‹
        y = self.fc(f)  # (batch_size, num_classes)
        print(f"    âœ… æœ€ç»ˆè¾“å‡ºå®Œæˆï¼Œy: {y.shape}")
        
        return y


class EmbeddingLayer(nn.Module):
    """
    åµŒå…¥å±‚ - å¤„ç†ç¨€ç–ç‰¹å¾å’Œå˜é•¿ç¨€ç–ç‰¹å¾
    """
    
    def __init__(self, feat_configs: list, verbose: bool = True):
        super().__init__()
        self.feat_configs = feat_configs
        self.embeddings = nn.ModuleDict()
        self.feat_info = {}
        
        if verbose:
            print("ğŸ”§ æ„å»ºåµŒå…¥å±‚...")
        
        for config in feat_configs:
            feat_name = config['feat_name']
            feat_type = config['feat_type']
            vocab_size = config['vocabulary_size']
            emb_dim = config['embedding_dim']
            
            # åˆ›å»ºåµŒå…¥å±‚
            self.embeddings[feat_name] = nn.Embedding(vocab_size + 1, emb_dim, padding_idx=0)
            
            # å­˜å‚¨ç‰¹å¾ä¿¡æ¯
            self.feat_info[feat_name] = {
                'type': feat_type,
                'vocab_size': vocab_size,
                'emb_dim': emb_dim
            }
            
            if verbose:
                print(f"  - {feat_name}: {feat_type}, vocab={vocab_size}, emb_dim={emb_dim}")
        
        # åˆå§‹åŒ–åµŒå…¥æƒé‡
        self._init_embeddings()
    
    def _init_embeddings(self):
        """åˆå§‹åŒ–åµŒå…¥æƒé‡"""
        for emb in self.embeddings.values():
            nn.init.xavier_uniform_(emb.weight)
            # padding idxè®¾ä¸º0
            with torch.no_grad():
                emb.weight[0].fill_(0)
    
    def forward(self, features_dict):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            features_dict: dict, ç‰¹å¾å­—å…¸ {feat_name: tensor}
        
        Returns:
            embedded_features: list of tensors, æ¯ä¸ªtensorå½¢çŠ¶ä¸º (batch_size, emb_dim)
        """
        print(f"  ğŸ”§ åµŒå…¥å±‚å¼€å§‹å¤„ç† {len(features_dict)} ä¸ªç‰¹å¾...")
        embedded_features = []
        
        for i, config in enumerate(self.feat_configs):
            feat_name = config['feat_name']
            feat_type = config['feat_type']
            
            if feat_name not in features_dict:
                print(f"    âš ï¸  ç‰¹å¾ {feat_name} ä¸åœ¨è¾“å…¥ä¸­ï¼Œè·³è¿‡")
                continue
            
            feat_tensor = features_dict[feat_name]
            print(f"    ğŸ“Š å¤„ç†ç‰¹å¾ {i+1}/{len(self.feat_configs)}: {feat_name} ({feat_type}), è¾“å…¥å½¢çŠ¶: {feat_tensor.shape}")
            
            if feat_type == 'sparse':
                # ç¨€ç–ç‰¹å¾: (batch_size,) -> (batch_size, emb_dim)
                emb = self.embeddings[feat_name](feat_tensor)
                print(f"      âœ… ç¨€ç–ç‰¹å¾åµŒå…¥å®Œæˆï¼Œè¾“å‡ºå½¢çŠ¶: {emb.shape}")
                embedded_features.append(emb)
                
            elif feat_type == 'varlen_sparse':
                # å˜é•¿ç¨€ç–ç‰¹å¾: (batch_size, max_len) -> (batch_size, emb_dim)
                emb = self.embeddings[feat_name](feat_tensor)  # (batch_size, max_len, emb_dim)
                print(f"      ğŸ“Š å˜é•¿ç‰¹å¾åµŒå…¥: {emb.shape}")
                
                # å¹³å‡æ± åŒ–
                mask = (feat_tensor != 0).float().unsqueeze(-1)  # (batch_size, max_len, 1)
                print(f"      ğŸ“Š æ©ç å½¢çŠ¶: {mask.shape}")
                
                emb_masked = emb * mask  # (batch_size, max_len, emb_dim)
                emb_pooled = emb_masked.sum(dim=1) / (mask.sum(dim=1) + 1e-8)  # (batch_size, emb_dim)
                print(f"      âœ… å˜é•¿ç‰¹å¾æ± åŒ–å®Œæˆï¼Œè¾“å‡ºå½¢çŠ¶: {emb_pooled.shape}")
                embedded_features.append(emb_pooled)
        
        print(f"  âœ… åµŒå…¥å±‚å¤„ç†å®Œæˆï¼Œç”Ÿæˆ {len(embedded_features)} ä¸ªç‰¹å¾åµŒå…¥")
        return embedded_features


class PushAFMModel(nn.Module):
    """
    å®Œæ•´çš„AFMæ¨¡å‹ - ä¸“é—¨ç”¨äºæ¨é€åˆ†ç±»ä»»åŠ¡
    """
    
    def __init__(self, feat_configs: list, train_config: dict = None, verbose: bool = True):
        super().__init__()
        
        self.feat_configs = feat_configs
        self.train_config = train_config or {}
        
        # è·å–åµŒå…¥ç»´åº¦
        emb_dims = [config['embedding_dim'] for config in feat_configs]
        if len(set(emb_dims)) != 1:
            raise ValueError("æ‰€æœ‰ç‰¹å¾çš„åµŒå…¥ç»´åº¦å¿…é¡»ç›¸åŒ")
        self.emb_dim = emb_dims[0]
        
        # ç‰¹å¾æ•°é‡
        self.num_fields = len(feat_configs)
        
        if verbose:
            print(f"ğŸ—ï¸  æ„å»ºAFMæ¨¡å‹...")
            print(f"  ç‰¹å¾å­—æ®µæ•°: {self.num_fields}")
            print(f"  åµŒå…¥ç»´åº¦: {self.emb_dim}")
        
        # åµŒå…¥å±‚
        self.embedding_layer = EmbeddingLayer(feat_configs, verbose=verbose)
        
        # AFMå‚æ•°
        model_config = self.train_config.get('model', {})
        attn_dim = model_config.get('attention_dim', 64)  # æ³¨æ„åŠ›ç»´åº¦
        dropout_rate = model_config.get('dropout_rate', 0.2)
        
        if verbose:
            print(f"  æ³¨æ„åŠ›ç»´åº¦: {attn_dim}")
            print(f"  Dropoutç‡: {dropout_rate}")
        
        # AFMæ ¸å¿ƒ
        self.afm = AttentionalFactorizationMachine(
            num_fields=self.num_fields,
            emb_dim=self.emb_dim,
            attn_dim=attn_dim,
            num_classes=1,
            dropout_rate=dropout_rate
        )
        
        # Sigmoidæ¿€æ´»å‡½æ•°ç”¨äºäºŒåˆ†ç±»
        self.sigmoid = nn.Sigmoid()
        
        if verbose:
            total_params = sum(p.numel() for p in self.parameters())
            print(f"  æ€»å‚æ•°é‡: {total_params:,}")
    
    def forward(self, features_dict):
        """
        å‰å‘ä¼ æ’­
        """
        # 1. åµŒå…¥å±‚å¤„ç†
        embedded_features = self.embedding_layer(features_dict)  # list of (batch_size, emb_dim)
        
        # 2. å †å æˆAFMè¾“å…¥æ ¼å¼
        x = torch.stack(embedded_features, dim=1)  # (batch_size, num_fields, emb_dim)
        
        # 3. AFMè®¡ç®—
        logits = self.afm(x)  # (batch_size, 1)
        
        # 4. Sigmoidæ¿€æ´»
        probs = self.sigmoid(logits)  # (batch_size, 1)
        
        return probs.squeeze(-1)  # (batch_size,)


class PushDataset(Dataset):
    """
    æ¨é€æ•°æ®çš„PyTorch Dataset
    """
    
    def __init__(self, features_dict: dict, labels: np.ndarray):
        print(f"  ğŸ”§ å¼€å§‹åˆ›å»ºPushDataset...")
        print(f"    ğŸ“Š è¾“å…¥ç‰¹å¾æ•°é‡: {len(features_dict)}")
        print(f"    ğŸ“Š æ ‡ç­¾æ•°é‡: {len(labels)}")
        
        self.features_dict = features_dict
        self.labels = labels
        self.length = len(labels)
        
        # è½¬æ¢ä¸ºtensor
        print(f"  ğŸ”„ å¼€å§‹è½¬æ¢ç‰¹å¾ä¸ºtensor...")
        self.features_tensor = {}
        
        for i, (name, values) in enumerate(features_dict.items()):
            print(f"    ğŸ“Š è½¬æ¢ç‰¹å¾ {i+1}/{len(features_dict)}: {name}")
            print(f"      è¾“å…¥ç±»å‹: {type(values)}, å½¢çŠ¶: {values.shape}, dtype: {values.dtype}")
            
            try:
                if isinstance(values, np.ndarray):
                    if values.dtype in [np.int32, np.int64]:
                        tensor = torch.from_numpy(values).long()
                    elif values.dtype in [np.float32, np.float64]:
                        tensor = torch.from_numpy(values.astype(np.int64)).long()
                    else:
                        tensor = torch.from_numpy(values.astype(np.int64)).long()
                else:
                    tensor = torch.tensor(values, dtype=torch.long)
                
                self.features_tensor[name] = tensor
                print(f"      âœ… è½¬æ¢æˆåŠŸï¼Œtensorå½¢çŠ¶: {tensor.shape}, dtype: {tensor.dtype}")
                
            except Exception as e:
                print(f"      âŒ è½¬æ¢å¤±è´¥: {e}")
                # å°è¯•å¤‡ç”¨æ–¹æ¡ˆ
                try:
                    if isinstance(values, np.ndarray):
                        tensor = torch.from_numpy(values.astype(np.int64)).long()
                    else:
                        tensor = torch.tensor(values, dtype=torch.long)
                    self.features_tensor[name] = tensor
                    print(f"      âœ… å¤‡ç”¨è½¬æ¢æˆåŠŸï¼Œtensorå½¢çŠ¶: {tensor.shape}")
                except Exception as e2:
                    print(f"      âŒ å¤‡ç”¨è½¬æ¢ä¹Ÿå¤±è´¥: {e2}")
                    raise e2
        
        print(f"  ğŸ”„ è½¬æ¢æ ‡ç­¾ä¸ºtensor...")
        try:
            self.labels_tensor = torch.from_numpy(labels).float()
            print(f"    âœ… æ ‡ç­¾è½¬æ¢æˆåŠŸï¼Œå½¢çŠ¶: {self.labels_tensor.shape}")
        except Exception as e:
            print(f"    âŒ æ ‡ç­¾è½¬æ¢å¤±è´¥: {e}")
            raise e
        
        print(f"  âœ… PushDatasetåˆ›å»ºå®Œæˆï¼")
    
    def __len__(self):
        return self.length
    
    def __getitem__(self, idx):
        features = {name: tensor[idx] for name, tensor in self.features_tensor.items()}
        label = self.labels_tensor[idx]
        return features, label


def prepare_afm_dataset(df: pd.DataFrame, feat_config: dict, batch_size: int = 256) -> tuple:
    """
    å‡†å¤‡AFMè®­ç»ƒæ•°æ®
    """
    # è·å–ç‰¹å¾é…ç½®
    if 'pipelines' in feat_config:
        pipelines = feat_config['pipelines']
    elif 'process' in feat_config and 'pipelines' in feat_config['process']:
        pipelines = feat_config['process']['pipelines']
    else:
        raise ValueError("æ— æ³•æ‰¾åˆ°ç‰¹å¾é…ç½®ä¸­çš„ pipelines")
    
    pipeline_feats = {p['feat_name']: p for p in pipelines}
    features_dict = {}
    
    for feat_name, config in pipeline_feats.items():
        if feat_name not in df.columns:
            continue
        
        feat_type = config.get('feat_type', 'sparse')
        
        if feat_type == 'sparse':
            # å•å€¼ç‰¹å¾
            values = df[feat_name].values.astype(np.int32)
            features_dict[feat_name] = values
            
        elif feat_type == 'varlen_sparse':
            # å˜é•¿ç‰¹å¾ï¼Œéœ€è¦padding
            sequences = df[feat_name].tolist()
            max_len = max(len(seq) if isinstance(seq, list) else 1 for seq in sequences)
            
            padded_sequences = []
            for seq in sequences:
                if isinstance(seq, list):
                    padded = seq + [0] * (max_len - len(seq))
                else:
                    padded = [int(seq)] + [0] * (max_len - 1)
                padded_sequences.append(padded[:max_len])
            
            features_dict[feat_name] = np.array(padded_sequences, dtype=np.int32)
    
    # å‡†å¤‡æ ‡ç­¾
    labels = df['label'].values.astype(np.float32)
    
    return features_dict, labels


def train_afm_model(model, train_loader, val_loader, train_config: dict, device):
    """
    è®­ç»ƒAFMæ¨¡å‹
    """
    # ä¼˜åŒ–å™¨é…ç½®
    training_config = train_config.get('training', {})
    lr = training_config.get('lr', 0.001)
    weight_decay = training_config.get('weight_decay', 0.001)
    epochs = training_config.get('epochs', 2)
    
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    criterion = nn.BCELoss()
    
    print(f"\nğŸš€ å¼€å§‹AFMæ¨¡å‹è®­ç»ƒ...")
    print(f"  å­¦ä¹ ç‡: {lr}")
    print(f"  æƒé‡è¡°å‡: {weight_decay}")
    print(f"  è®­ç»ƒè½®æ•°: {epochs}")
    print(f"  è®¾å¤‡: {device}")
    
    # æ·»åŠ è°ƒè¯•ä¿¡æ¯
    print(f"\nğŸ”§ è®­ç»ƒæ•°æ®æ£€æŸ¥:")
    print(f"  è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
    print(f"  éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}")
    
    # æµ‹è¯•ç¬¬ä¸€ä¸ªbatchä»¥æ£€æŸ¥æ•°æ®æ ¼å¼
    print(f"\nğŸ§ª æµ‹è¯•ç¬¬ä¸€ä¸ªbatch...")
    try:
        first_batch = next(iter(train_loader))
        features, labels = first_batch
        print(f"  âœ… æ•°æ®åŠ è½½æˆåŠŸ")
        print(f"  ç‰¹å¾æ•°é‡: {len(features)}")
        print(f"  æ ‡ç­¾å½¢çŠ¶: {labels.shape}")
        
        # æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­
        print(f"  ğŸ§  æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­...")
        model.eval()
        with torch.no_grad():
            # ç§»åŠ¨åˆ°è®¾å¤‡
            features_device = {name: tensor.to(device) for name, tensor in features.items()}
            labels_device = labels.to(device)
            
            start_time = time.time()
            predictions = model(features_device)
            forward_time = time.time() - start_time
            
            print(f"  âœ… å‰å‘ä¼ æ’­æˆåŠŸï¼Œè€—æ—¶: {forward_time:.2f}ç§’")
            print(f"  é¢„æµ‹å½¢çŠ¶: {predictions.shape}")
            print(f"  é¢„æµ‹èŒƒå›´: [{predictions.min():.4f}, {predictions.max():.4f}]")
            
    except Exception as e:
        print(f"  âŒ æ•°æ®æˆ–æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        return None
    
    best_val_auc = 0.0
    train_losses = []
    val_aucs = []
    
    for epoch in range(epochs):
        print(f"\nğŸ”„ å¼€å§‹ç¬¬ {epoch+1}/{epochs} è½®è®­ç»ƒ...")
        
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        total_loss = 0.0
        train_predictions = []
        train_targets = []
        
        # æ·»åŠ æ—¶é—´ç»Ÿè®¡
        epoch_start_time = time.time()
        batch_times = []
        
        for batch_idx, (features, labels) in enumerate(train_loader):
            batch_start_time = time.time()
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            features = {name: tensor.to(device) for name, tensor in features.items()}
            labels = labels.to(device)
            
            # å‰å‘ä¼ æ’­
            optimizer.zero_grad()
            predictions = model(features)
            loss = criterion(predictions, labels)
            
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            train_predictions.extend(predictions.detach().cpu().numpy())
            train_targets.extend(labels.detach().cpu().numpy())
            
            batch_time = time.time() - batch_start_time
            batch_times.append(batch_time)
            
            # æ›´é¢‘ç¹çš„è¿›åº¦è¾“å‡º
            if batch_idx % 10 == 0:  # æ¯10ä¸ªbatchè¾“å‡ºä¸€æ¬¡
                avg_batch_time = np.mean(batch_times[-10:])  # æœ€è¿‘10ä¸ªbatchçš„å¹³å‡æ—¶é—´
                eta = avg_batch_time * (len(train_loader) - batch_idx - 1)  # é¢„ä¼°å‰©ä½™æ—¶é—´
                print(f"    Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}, "
                      f"Time: {batch_time:.2f}s, ETA: {eta:.1f}s")
        
        # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
        train_auc = roc_auc_score(train_targets, train_predictions)
        avg_train_loss = total_loss / len(train_loader)
        train_losses.append(avg_train_loss)
        epoch_time = time.time() - epoch_start_time
        
        print(f"\nâ±ï¸  ç¬¬{epoch+1}è½®è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {epoch_time:.1f}ç§’")
        print(f"    å¹³å‡batchæ—¶é—´: {np.mean(batch_times):.2f}ç§’")
        
        # éªŒè¯é˜¶æ®µ
        print(f"ğŸ” å¼€å§‹éªŒè¯...")
        model.eval()
        val_predictions = []
        val_targets = []
        val_loss = 0.0
        
        val_start_time = time.time()
        with torch.no_grad():
            for batch_idx, (features, labels) in enumerate(val_loader):
                features = {name: tensor.to(device) for name, tensor in features.items()}
                labels = labels.to(device)
                
                predictions = model(features)
                loss = criterion(predictions, labels)
                
                val_loss += loss.item()
                val_predictions.extend(predictions.cpu().numpy())
                val_targets.extend(labels.cpu().numpy())
                
                if batch_idx % 20 == 0:
                    print(f"    éªŒè¯Batch {batch_idx}/{len(val_loader)}")
        
        val_time = time.time() - val_start_time
        print(f"âœ… éªŒè¯å®Œæˆï¼Œè€—æ—¶: {val_time:.1f}ç§’")
        
        # è®¡ç®—éªŒè¯æŒ‡æ ‡
        val_auc = roc_auc_score(val_targets, val_predictions)
        avg_val_loss = val_loss / len(val_loader)
        val_aucs.append(val_auc)
        
        print(f"\nğŸ“Š Epoch {epoch+1}/{epochs} ç»“æœ:")
        print(f"  è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}")
        print(f"  è®­ç»ƒAUC: {train_auc:.4f}")
        print(f"  éªŒè¯æŸå¤±: {avg_val_loss:.4f}")
        print(f"  éªŒè¯AUC: {val_auc:.4f}")
        print(f"  æ€»è€—æ—¶: {epoch_time + val_time:.1f}ç§’")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_auc > best_val_auc:
            best_val_auc = val_auc
            print(f"  ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯AUC: {best_val_auc:.4f}")
    
    return {
        'train_losses': train_losses,
        'val_aucs': val_aucs,
        'best_val_auc': best_val_auc
    }


# ---------------------------------------------------------------------------- #
# 6. AFMæ¨¡å‹è®­ç»ƒä¸»æµç¨‹
# ---------------------------------------------------------------------------- #

print("\n" + "="*80)
print("ğŸ”¥ å¼€å§‹AFMæ¨¡å‹è®­ç»ƒ")
print("="*80)

# æ£€æŸ¥è®¾å¤‡
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")

# ç¡®ä¿æœ‰æ ‡ç­¾
if 'label' not in df_raw.columns:
    df_raw['label'] = df_raw['log_type'].apply(lambda x: 1 if x == 'PC' else 0)

# 1. é‡æ–°å¤„ç†æ•°æ® (ä½¿ç”¨feat.ymlé…ç½®)
print("\nğŸ”„ ä¸ºAFMæ¨¡å‹å¤„ç†æ•°æ®...")
df_afm_processed, _ = process_feature_pipelines(df_raw, deep_feat_config)

print(f"ğŸ“Š AFMæ•°æ®ç»Ÿè®¡:")
print(f"  æ€»æ ·æœ¬æ•°: {len(df_afm_processed):,}")
print(f"  æ­£æ ·æœ¬æ•°: {df_afm_processed['label'].sum():,}")
print(f"  è´Ÿæ ·æœ¬æ•°: {(df_afm_processed['label'] == 0).sum():,}")
print(f"  æ­£æ ·æœ¬æ¯”ä¾‹: {df_afm_processed['label'].mean():.3f}")

# 2. å‡†å¤‡AFMæ•°æ®é›†
print("\nğŸ”§ å‡†å¤‡AFMæ•°æ®é›†...")
features_dict, labels = prepare_afm_dataset(df_afm_processed, deep_feat_config)

print(f"ç‰¹å¾å­—æ®µ: {list(features_dict.keys())}")
for name, values in features_dict.items():
    print(f"  - {name}: shape={values.shape}, dtype={values.dtype}")

# 3. åˆ›å»ºè®­ç»ƒ/éªŒè¯åˆ†å‰²
validation_split = train_config.get('training', {}).get('validation_split', 0.2)
train_size = int((1 - validation_split) * len(labels))

print(f"\nğŸ“Š æ•°æ®åˆ†å‰²:")
print(f"  è®­ç»ƒé›†: {train_size:,} ({(1-validation_split)*100:.1f}%)")
print(f"  éªŒè¯é›†: {len(labels)-train_size:,} ({validation_split*100:.1f}%)")

# åˆ†å‰²æ•°æ®
train_features = {name: values[:train_size] for name, values in features_dict.items()}
val_features = {name: values[train_size:] for name, values in features_dict.items()}
train_labels = labels[:train_size]
val_labels = labels[train_size:]

# 4. åˆ›å»ºPyTorchæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
batch_size = train_config.get('training', {}).get('batch_size', 256)

# å¦‚æœæ˜¯CPUè®­ç»ƒï¼Œå‡å°‘batch_sizeä»¥æé«˜é€Ÿåº¦
if device.type == 'cpu':
    batch_size = min(batch_size, 16)  # CPUä¸Šä½¿ç”¨æ›´å°çš„batch_size
    print(f"âš ï¸  æ£€æµ‹åˆ°CPUè®­ç»ƒï¼Œå‡å°‘batch_sizeåˆ°: {batch_size}")

print(f"\nğŸ“¦ åˆ›å»ºæ•°æ®é›†...")
train_dataset = PushDataset(train_features, train_labels)
val_dataset = PushDataset(val_features, val_labels)

print(f"  è®­ç»ƒæ•°æ®é›†å¤§å°: {len(train_dataset):,}")
print(f"  éªŒè¯æ•°æ®é›†å¤§å°: {len(val_dataset):,}")

# æµ‹è¯•æ•°æ®é›†
print(f"ğŸ§ª æµ‹è¯•æ•°æ®é›†...")
try:
    sample_features, sample_label = train_dataset[0]
    print(f"  âœ… æ•°æ®é›†åˆ›å»ºæˆåŠŸ")
    print(f"  æ ·æœ¬ç‰¹å¾æ•°é‡: {len(sample_features)}")
    print(f"  æ ·æœ¬æ ‡ç­¾: {sample_label}")
except Exception as e:
    print(f"  âŒ æ•°æ®é›†æµ‹è¯•å¤±è´¥: {e}")

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)

print(f"  è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
print(f"  éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}")
print(f"  ä½¿ç”¨batch_size: {batch_size}")

# æµ‹è¯•æ•°æ®åŠ è½½å™¨
print(f"ğŸ§ª æµ‹è¯•æ•°æ®åŠ è½½å™¨...")
try:
    start_time = time.time()
    first_batch = next(iter(train_loader))
    load_time = time.time() - start_time
    print(f"  âœ… æ•°æ®åŠ è½½æˆåŠŸï¼Œè€—æ—¶: {load_time:.2f}ç§’")
except Exception as e:
    print(f"  âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")

# 5. åˆ›å»ºAFMæ¨¡å‹
print("\nğŸ—ï¸  æ„å»ºAFMæ¨¡å‹...")

# è·å–ç‰¹å¾é…ç½®
if 'pipelines' in deep_feat_config:
    afm_pipelines = deep_feat_config['pipelines']
elif 'process' in deep_feat_config and 'pipelines' in deep_feat_config['process']:
    afm_pipelines = deep_feat_config['process']['pipelines']
else:
    raise ValueError("æ— æ³•æ‰¾åˆ°AFMç‰¹å¾é…ç½®ä¸­çš„ pipelines")

# æ·»åŠ AFMç‰¹å®šé…ç½®åˆ°train_config
afm_train_config = train_config.copy()
if 'model' not in afm_train_config:
    afm_train_config['model'] = {}
afm_train_config['model']['attention_dim'] = 32  # AFMæ³¨æ„åŠ›ç»´åº¦ï¼ˆå‡å°‘è®¡ç®—é‡ï¼‰
afm_train_config['model']['dropout_rate'] = 0.2  # AFMçš„dropoutç‡

# åˆ›å»ºæ¨¡å‹
afm_model = PushAFMModel(afm_pipelines, afm_train_config, verbose=True)
afm_model.to(device)

print(f"\nğŸ“‹ AFMæ¨¡å‹æ¶æ„:")
print(f"  ğŸ”§ ç‰¹å¾å­—æ®µæ•°: {afm_model.num_fields}")
print(f"  ğŸ“Š åµŒå…¥ç»´åº¦: {afm_model.emb_dim}")
print(f"  ğŸ§  æ³¨æ„åŠ›ç»´åº¦: {afm_train_config['model']['attention_dim']}")
print(f"  ğŸ’§ Dropoutç‡: {afm_train_config['model']['dropout_rate']}")
print(f"  ğŸ“Š æ€»å‚æ•°é‡: {sum(p.numel() for p in afm_model.parameters()):,}")

# 6. è®­ç»ƒæ¨¡å‹
training_results = train_afm_model(afm_model, train_loader, val_loader, afm_train_config, device)

# 7. è¾“å‡ºæœ€ç»ˆç»“æœ
print("\n" + "="*80)
print("ğŸ‰ AFMæ¨¡å‹è®­ç»ƒå®Œæˆï¼")
print("="*80)

print(f"\nğŸ“Š æœ€ç»ˆç»“æœ:")
print(f"  æœ€ä½³éªŒè¯AUC: {training_results['best_val_auc']:.4f}")
print(f"  æœ€ç»ˆè®­ç»ƒæŸå¤±: {training_results['train_losses'][-1]:.4f}")
print(f"  AUCæå‡æ›²çº¿: {[f'{auc:.4f}' for auc in training_results['val_aucs']]}")

print(f"\nğŸ”„ æ¨¡å‹å¯¹æ¯”:")
print(f"  ğŸŒ³ æ ‘æ¨¡å‹(LightGBM): åŸºäºconfig.ymlç‰¹å¾å·¥ç¨‹")
print(f"  ğŸ§  MLPæ¨¡å‹: åŸºäºfeat.ymlç‰¹å¾å·¥ç¨‹")
print(f"  ğŸ”¥ AFMæ¨¡å‹: åŸºäºfeat.ymlç‰¹å¾å·¥ç¨‹ + æ³¨æ„åŠ›æœºåˆ¶")
print(f"     â””â”€ æœ€ä½³éªŒè¯AUC: {training_results['best_val_auc']:.4f}")

print(f"\nâœ… AFMæ¨¡å‹è®­ç»ƒæµæ°´çº¿å®Œæˆï¼")
print(f"ğŸ’¡ AFMé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶è‡ªåŠ¨å­¦ä¹ ç‰¹å¾äº¤äº’çš„é‡è¦æ€§ï¼Œ")
print(f"   ç›¸æ¯”ä¼ ç»ŸFMæ¨¡å‹èƒ½æ›´å¥½åœ°å¤„ç†ç‰¹å¾äº¤äº’ï¼")

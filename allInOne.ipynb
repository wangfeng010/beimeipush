{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 完整版本项目梳理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 树模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 环境设置与导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "from hashlib import md5\n",
    "from typing import Any, Dict, List, Union\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from glob import glob  # 添加glob模块导入\n",
    "import time\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加项目路径\n",
    "project_root = os.getcwd()\n",
    "env_path = os.path.join(project_root, 'env')\n",
    "if env_path not in sys.path:\n",
    "    sys.path.insert(0, env_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 加载原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检测到CSV文件，使用本地环境配置\n",
      "分隔符: ',', 表头行: 0, 文件数量: 5\n",
      "形状(50000, 13), 列名: ['user_id', 'create_time', 'log_type', 'watchlists', 'holdings', 'country', 'prefer_bid', 'user_propernoun', 'push_title', 'push_content', 'item_code', 'item_tags', 'submit_type']\n",
      "{'user_id': 1800001088, 'create_time': '2025-05-31 08:39:07', 'log_type': 'PR', 'watchlists': nan, 'holdings': nan, 'country': 'Germany', 'prefer_bid': nan, 'user_propernoun': 'germany#3.06|mid-america#1.02', 'push_title': 'Ainvest Newswire', 'push_content': 'Hims & Hers Health Lays Off 4% of Staff Amid Strategy Shift', 'item_code': '[{\"market\":\"169\",\"score\":0,\"code\":\"HIMS\",\"tagId\":\"U000012934\",\"name\":\"Hims & Hers Health\",\"type\":0,\"parentId\":\"US_ROBOT0f37d7fd3fca6a41\"},{\"market\":\"169\",\"score\":0,\"code\":\"NVO\",\"tagId\":\"U000002999\",\"name\":\"Novo Nordisk\",\"type\":0,\"parentId\":\"US_ROBOT0f37d7fd3fca6a41\"}]', 'item_tags': '[{\"score\":0.7803922295570374,\"tagId\":\"51510\",\"name\":\"us_high_importance\",\"type\":4,\"parentId\":\"US_ROBOT0f37d7fd3fca6a41\"},{\"score\":0.7803922295570374,\"tagId\":\"57967\",\"name\":\"Fusion\",\"type\":4,\"parentId\":\"US_ROBOT0f37d7fd3fca6a41\"},{\"tagId\":\"1002\",\"name\":\"no_penny_stock\",\"type\":4,\"parentId\":\"US_ROBOT0f37d7fd3fca6a41\"}]', 'submit_type': 'autoFlash'}\n"
     ]
    }
   ],
   "source": [
    "# 02 根据配置文件加载原始数据\n",
    "def load_raw_data_from_config():\n",
    "    \"\"\"根据data.yml配置文件加载原始数据，支持本地和线上环境\"\"\"\n",
    "    \n",
    "    # 加载数据配置\n",
    "    with open('config/data.yml', 'r', encoding='utf-8') as f:\n",
    "        data_config = yaml.safe_load(f)\n",
    "    \n",
    "    # 检测文件类型和环境\n",
    "    train_dir = data_config['train_dir']\n",
    "    \n",
    "    # 检查CSV和TXT文件\n",
    "    csv_files = glob(os.path.join(train_dir, '*.csv'))\n",
    "    txt_files = glob(os.path.join(train_dir, '*.txt'))\n",
    "    \n",
    "    if csv_files:\n",
    "        # 本地环境 - 使用CSV格式\n",
    "        print(\"检测到CSV文件，使用本地环境配置\")\n",
    "        csv_config = data_config['csv_format']\n",
    "        separator, header = csv_config['separator'], csv_config['header']\n",
    "        \n",
    "        print(f\"分隔符: '{separator}', 表头行: {header}, 文件数量: {len(csv_files)}\")\n",
    "        \n",
    "        # 读取CSV文件\n",
    "        dfs = [pd.read_csv(f, sep=separator, header=header) for f in csv_files]\n",
    "        \n",
    "    elif txt_files:\n",
    "        # 线上环境 - 使用TXT格式\n",
    "        print(\"检测到TXT文件，使用线上环境配置\")\n",
    "        txt_config = data_config.get('txt_format', {'separator': '\\t', 'header': None})\n",
    "        separator, header = txt_config['separator'], txt_config['header']\n",
    "        \n",
    "        # 从列表中提取列名\n",
    "        raw_columns = [list(item.keys())[0] for item in data_config['raw_data_columns']]\n",
    "        \n",
    "        print(f\"分隔符: '{separator}', 表头行: {header}, 文件数量: {len(txt_files)}\")\n",
    "        print(f\"预定义列名: {raw_columns}\")\n",
    "        \n",
    "        # 读取TXT文件\n",
    "        dfs = [pd.read_csv(f, sep=separator, header=header, names=raw_columns) for f in txt_files]\n",
    "    else:\n",
    "        raise ValueError(f\"在目录 {train_dir} 中未找到CSV或TXT文件\")\n",
    "    \n",
    "    df_raw = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"形状{df_raw.shape}, 列名: {list(df_raw.columns)}\")\n",
    "    \n",
    "    return df_raw\n",
    "\n",
    "# 加载原始数据\n",
    "df_raw = load_raw_data_from_config()\n",
    "\n",
    "# 显示数据样例（简化版）\n",
    "print(df_raw.iloc[0].to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 加载并解析YAML配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'process': {'embedding_dim': 8, 'pooling_type': 'sum', 'pipelines': [{'feat_name': 'hour', 'feat_type': 'sparse', 'vocabulary_size': 24, 'embedding_dim': 8, 'input_sample': '2024-08-02 00:44:05', 'operations': [{'col_in': 'create_time', 'col_out': 'create_time', 'func_name': 'fillna', 'func_parameters': {'na_value': '2024-08-02 00:16:34'}}, {'col_in': 'create_time', 'col_out': 'hour', 'func_name': 'to_hour', 'func_parameters': {}}]}, {'feat_name': 'weekday', 'feat_type': 'sparse', 'vocabulary_size': 7, 'embedding_dim': 8, 'input_sample': '2024-08-02 00:44:05', 'operations': [{'col_in': 'create_time', 'col_out': 'weekday', 'func_name': 'to_weekday', 'func_parameters': {}}]}, {'feat_name': 'user_watch_stk_code_hash', 'feat_type': 'varlen_sparse', 'vocabulary_size': 10000, 'embedding_dim': 8, 'input_sample': 'AAPL_185 & TSLA_185', 'operations': [{'col_in': 'watchlists', 'col_out': 'watchlists', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null_0 & null_0'}}, {'col_in': 'watchlists', 'col_out': 'watchlists', 'func_name': 'split', 'func_parameters': {'sep': ' & '}}, {'col_in': 'watchlists', 'col_out': 'watchlists', 'func_name': 'seperation', 'func_parameters': {'sep': '_'}}, {'col_in': 'watchlists', 'col_out': 'user_watch_stk_code', 'func_name': 'list_get', 'func_parameters': {'item_index': 0}}, {'col_in': 'user_watch_stk_code', 'col_out': 'user_watch_stk_code', 'func_name': 'remove_items', 'func_parameters': {'target_values': ['AAPL', 'AMZN', 'GOOGL', 'TSLA']}}, {'col_in': 'user_watch_stk_code', 'col_out': 'user_watch_stk_code', 'func_name': 'padding', 'func_parameters': {'max_len': 5, 'pad_value': 'null'}}, {'col_in': 'user_watch_stk_code', 'col_out': 'user_watch_stk_code_hash', 'func_name': 'list_hash', 'func_parameters': {'vocabulary_size': 10000}}]}, {'feat_name': 'country_hash', 'feat_type': 'sparse', 'vocabulary_size': 200, 'embedding_dim': 8, 'input_sample': 'United States', 'operations': [{'col_in': 'country', 'col_out': 'country', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null'}}, {'col_in': 'country', 'col_out': 'country_hash', 'func_name': 'str_hash', 'func_parameters': {'vocabulary_size': 200}}]}, {'feat_name': 'prefer_bid_code_hash', 'feat_type': 'varlen_sparse', 'vocabulary_size': 10000, 'embedding_dim': 8, 'input_sample': 'AAPL#0.24809|AMZN#0.24809|GOOGL#0.24809|TSLA#0.24809', 'operations': [{'col_in': 'prefer_bid', 'col_out': 'prefer_bid', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null#0'}}, {'col_in': 'prefer_bid', 'col_out': 'prefer_bid', 'func_name': 'split', 'func_parameters': {'sep': '|'}}, {'col_in': 'prefer_bid', 'col_out': 'prefer_bid', 'func_name': 'seperation', 'func_parameters': {'sep': '#'}}, {'col_in': 'prefer_bid', 'col_out': 'prefer_bid_code', 'func_name': 'list_get', 'func_parameters': {'item_index': 0}}, {'col_in': 'prefer_bid_code', 'col_out': 'prefer_bid_code', 'func_name': 'padding', 'func_parameters': {'max_len': 5, 'pad_value': 'null'}}, {'col_in': 'prefer_bid_code', 'col_out': 'prefer_bid_code_hash', 'func_name': 'list_hash', 'func_parameters': {'vocabulary_size': 10000}}]}, {'feat_name': 'hold_bid_code_hash', 'feat_type': 'varlen_sparse', 'vocabulary_size': 10000, 'embedding_dim': 8, 'input_sample': 'JD,185|BAC,169|TSLA,185', 'operations': [{'col_in': 'holdings', 'col_out': 'holdings', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null,0'}}, {'col_in': 'holdings', 'col_out': 'holdings', 'func_name': 'split', 'func_parameters': {'sep': '|'}}, {'col_in': 'holdings', 'col_out': 'holdings', 'func_name': 'seperation', 'func_parameters': {'sep': ','}}, {'col_in': 'holdings', 'col_out': 'hold_bid_code', 'func_name': 'list_get', 'func_parameters': {'item_index': 0}}, {'col_in': 'hold_bid_code', 'col_out': 'hold_bid_code', 'func_name': 'padding', 'func_parameters': {'max_len': 5, 'pad_value': 'null'}}, {'col_in': 'hold_bid_code', 'col_out': 'hold_bid_code_hash', 'func_name': 'list_hash', 'func_parameters': {'vocabulary_size': 10000}}]}, {'feat_name': 'user_propernoun_hash', 'feat_type': 'varlen_sparse', 'vocabulary_size': 10000, 'embedding_dim': 8, 'input_sample': 'apple#1.02|nike#1.02', 'operations': [{'col_in': 'user_propernoun', 'col_out': 'user_propernoun', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null#0'}}, {'col_in': 'user_propernoun', 'col_out': 'user_propernoun', 'func_name': 'split', 'func_parameters': {'sep': '|'}}, {'col_in': 'user_propernoun', 'col_out': 'user_propernoun', 'func_name': 'seperation', 'func_parameters': {'sep': '#'}}, {'col_in': 'user_propernoun', 'col_out': 'user_propernoun_code', 'func_name': 'list_get', 'func_parameters': {'item_index': 0}}, {'col_in': 'user_propernoun_code', 'col_out': 'user_propernoun_code', 'func_name': 'padding', 'func_parameters': {'max_len': 5, 'pad_value': 'null'}}, {'col_in': 'user_propernoun_code', 'col_out': 'user_propernoun_hash', 'func_name': 'list_hash', 'func_parameters': {'vocabulary_size': 10000}}]}, {'feat_name': 'push_title_hash', 'feat_type': 'sparse', 'vocabulary_size': 8, 'embedding_dim': 8, 'input_sample': 'Breaking News', 'operations': [{'col_in': 'push_title', 'col_out': 'push_title', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null'}}, {'col_in': 'push_title', 'col_out': 'push_title_hash', 'func_name': 'str_hash', 'func_parameters': {'vocabulary_size': 8}}]}, {'feat_name': 'title_len', 'feat_type': 'sparse', 'vocabulary_size': 32, 'embedding_dim': 8, 'input_sample': \"Hong Kong's Leading Broker Futu Securities Introduces Bitcoin and XRP Trading with Lucrative Incentives\", 'operations': [{'col_in': 'push_content', 'col_out': 'push_content', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null'}}, {'col_in': 'push_content', 'col_out': 'push_content', 'func_name': 'split', 'func_parameters': {'sep': ' '}}, {'col_in': 'push_content', 'col_out': 'title_len', 'func_name': 'list_len', 'func_parameters': {}}, {'col_in': 'title_len', 'col_out': 'title_len', 'func_name': 'int_max', 'func_parameters': {'max_value': 31}}]}, {'feat_name': 'item_code_hash', 'feat_type': 'varlen_sparse', 'vocabulary_size': 10000, 'embedding_dim': 8, 'input_sample': '[{\"market\":\"185\",\"score\":1,\"code\":\"META\",\"name\":\"Meta\",\"type\":0,\"parentId\":\"0339437d07195361\"}]', 'operations': [{'col_in': 'item_code', 'col_out': 'item_code', 'func_name': 'fillna', 'func_parameters': {'na_value': '[{\"market\":\"null\",\"score\":0,\"code\":\"null\",\"name\":\"null\",\"type\":\"null\",\"parentId\":\"null\"}]'}}, {'col_in': 'item_code', 'col_out': 'item_code', 'func_name': 'json_object_to_list', 'func_parameters': {'key': 'code'}}, {'col_in': 'item_code', 'col_out': 'item_code', 'func_name': 'padding', 'func_parameters': {'max_len': 5, 'pad_value': 'null'}}, {'col_in': 'item_code', 'col_out': 'item_code_hash', 'func_name': 'list_hash', 'func_parameters': {'vocabulary_size': 10000}}]}, {'feat_name': 'submit_type_hash', 'feat_type': 'sparse', 'vocabulary_size': 10, 'embedding_dim': 8, 'input_sample': 'auto_flash', 'operations': [{'col_in': 'submit_type', 'col_out': 'submit_type', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null'}}, {'col_in': 'submit_type', 'col_out': 'submit_type_hash', 'func_name': 'str_hash', 'func_parameters': {'vocabulary_size': 10}}]}, {'feat_name': 'tag_id_hash', 'feat_type': 'varlen_sparse', 'vocabulary_size': 10000, 'embedding_dim': 8, 'input_sample': '[{\"score\":0,\"tagId\":\"57967\",\"name\":\"Fusion\",\"type\":4,\"parentId\":\"0339437d07195361\"}]', 'operations': [{'col_in': 'item_tags', 'col_out': 'item_tags', 'func_name': 'fillna', 'func_parameters': {'na_value': '[{\"score\":0,\"tagId\":\"null\",\"name\":\"null\",\"type\":0,\"parentId\":\"null\"}]'}}, {'col_in': 'item_tags', 'col_out': 'tagIds', 'func_name': 'json_object_to_list', 'func_parameters': {'key': 'tagId'}}, {'col_in': 'tagIds', 'col_out': 'tagIds', 'func_name': 'padding', 'func_parameters': {'max_len': 3, 'pad_value': 'null'}}, {'col_in': 'tagIds', 'col_out': 'tag_id_hash', 'func_name': 'list_hash', 'func_parameters': {'vocabulary_size': 100}}]}]}, 'interactions': {'embedding_dim': 8, 'pooling_type': 'sum', 'pipelines': [{'feat_name': 'preder_bid_cross', 'feat_type': 'sparse', 'vocabulary_size': 2, 'embedding_dim': 8, 'operations': [{'col_in': ['item_code', 'prefer_bid_code'], 'col_out': 'preder_bid_cross', 'func_name': 'has_intersection', 'func_parameters': {'exclude': ['null', '0']}}]}, {'feat_name': 'watch_bid_cross', 'feat_type': 'sparse', 'vocabulary_size': 2, 'embedding_dim': 8, 'operations': [{'col_in': ['item_code', 'user_watch_stk_code'], 'col_out': 'watch_bid_cross', 'func_name': 'has_intersection', 'func_parameters': {'exclude': ['null', '0']}}]}, {'feat_name': 'hold_bid_cross', 'feat_type': 'sparse', 'vocabulary_size': 2, 'embedding_dim': 8, 'operations': [{'col_in': ['item_code', 'hold_bid_code'], 'col_out': 'hold_bid_cross', 'func_name': 'has_intersection', 'func_parameters': {'exclude': ['null', '0']}}]}]}, 'label_process': {'pipelines': [{'feat_name': 'log_type', 'feat_type': 'sparse', 'vocabulary_size': 2, 'embedding_dim': 8, 'input_sample': '0', 'operations': [{'col_in': 'log_type', 'col_out': 'log_type', 'func_name': 'fillna', 'func_parameters': {'na_value': 'PR'}}, {'col_in': 'log_type', 'col_out': 'log_type', 'func_name': 'map_to_int', 'func_parameters': {'map_dict': {'PR': 0, 'PC': 1}, 'default_code': 0}}]}]}}\n"
     ]
    }
   ],
   "source": [
    "# 加载树模型配置 (config.yml 包含特征和训练配置)\n",
    "with open('config/config.yml', 'r', encoding='utf-8') as f:\n",
    "    config_yml = yaml.safe_load(f)\n",
    "# 从 config.yml 提取树模型的特征配置\n",
    "tree_feat_config = config_yml['features']\n",
    "# 加载深度模型的特征配置 (feat.yml)\n",
    "with open('config/feat.yml', 'r', encoding='utf-8') as f:\n",
    "    deep_feat_config = yaml.safe_load(f)\n",
    "\n",
    "# 当前使用树模型的特征配置进行演示\n",
    "feat_config = tree_feat_config\n",
    "\n",
    "print(feat_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_dim': 8,\n",
      " 'feat_name': 'hour',\n",
      " 'feat_type': 'sparse',\n",
      " 'input_sample': '2024-08-02 00:44:05',\n",
      " 'operations': [{'col_in': 'create_time',\n",
      "                 'col_out': 'create_time',\n",
      "                 'func_name': 'fillna',\n",
      "                 'func_parameters': {'na_value': '2024-08-02 00:16:34'}},\n",
      "                {'col_in': 'create_time',\n",
      "                 'col_out': 'hour',\n",
      "                 'func_name': 'to_hour',\n",
      "                 'func_parameters': {}}],\n",
      " 'vocabulary_size': 24}\n"
     ]
    }
   ],
   "source": [
    "# 看一下一个pipeline中对一个特征的操作 他被解析成了什么结构\n",
    "# 适配 config.yml 和 feat.yml 的不同结构\n",
    "if 'pipelines' in feat_config:\n",
    "    # feat.yml 格式\n",
    "    example_pipeline = feat_config['pipelines'][0]\n",
    "elif 'process' in feat_config and 'pipelines' in feat_config['process']:\n",
    "    # config.yml 格式\n",
    "    example_pipeline = feat_config['process']['pipelines'][0]\n",
    "else:\n",
    "    print(\"无法找到特征配置格式\")\n",
    "    example_pipeline = None\n",
    "\n",
    "if example_pipeline:\n",
    "    pprint(example_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 进行数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1 定义原子操作函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OP_HUB 构建完成，包含 14 个操作函数\n",
      "可用函数: ['fillna', 'split', 'seperation', 'list_get', 'remove_items', 'padding', 'list_hash', 'str_hash', 'to_hour', 'to_weekday', 'list_len', 'int_max', 'json_object_to_list', 'map_to_int']\n"
     ]
    }
   ],
   "source": [
    "MISSING_VALUE = [None, '', 'null', 'NULL', 'None', np.nan]\n",
    "\n",
    "def fillna(x: Union[float, int, str], na_value: Union[float, int, str]) -> Union[float, int, str]:\n",
    "    \"\"\"填充缺失值\"\"\"\n",
    "    if x in MISSING_VALUE or (isinstance(x, float) and pd.isna(x)):\n",
    "        return na_value\n",
    "    return x\n",
    "\n",
    "def split(x: str, sep: str) -> List[str]:\n",
    "    \"\"\"字符串分割\"\"\"\n",
    "    return str(x).split(sep)\n",
    "\n",
    "def seperation(x: List[str], sep: str) -> List[List[str]]:\n",
    "    \"\"\"列表元素二次分割\"\"\"\n",
    "    if not isinstance(x, list):\n",
    "        return []\n",
    "    return [item.split(sep) for item in x]\n",
    "\n",
    "def list_get(x: List[List[Any]], item_index: int) -> List[Any]:\n",
    "    \"\"\"获取嵌套列表中指定位置的元素\"\"\"\n",
    "    if not isinstance(x, list):\n",
    "        return []\n",
    "    result = []\n",
    "    for sublist in x:\n",
    "        if isinstance(sublist, list) and len(sublist) > item_index:\n",
    "            result.append(sublist[item_index])\n",
    "        else:\n",
    "            result.append('null')\n",
    "    return result\n",
    "\n",
    "def remove_items(x: List[str], target_values: List[str]) -> List[str]:\n",
    "    \"\"\"移除列表中的指定元素\"\"\"\n",
    "    if not isinstance(x, list):\n",
    "        return []\n",
    "    return [item for item in x if item not in target_values]\n",
    "\n",
    "def padding(x: List[Any], pad_value: Union[str, float, int], max_len: int) -> List[Any]:\n",
    "    \"\"\"列表填充到指定长度\"\"\"\n",
    "    if not isinstance(x, list):\n",
    "        x = []\n",
    "    if len(x) >= max_len:\n",
    "        return x[:max_len]\n",
    "    else:\n",
    "        return x + [pad_value] * (max_len - len(x))\n",
    "\n",
    "def list_hash(x: List[str], vocabulary_size: int) -> List[int]:\n",
    "    \"\"\"对列表中每个元素进行哈希\"\"\"\n",
    "    if not isinstance(x, list):\n",
    "        return []\n",
    "    result = []\n",
    "    for item in x:\n",
    "        hash_val = int(md5(str(item).encode()).hexdigest(), 16) % vocabulary_size\n",
    "        result.append(hash_val)\n",
    "    return result\n",
    "\n",
    "def str_hash(x: str, vocabulary_size: int) -> int:\n",
    "    \"\"\"字符串哈希\"\"\"\n",
    "    return int(md5(str(x).encode()).hexdigest(), 16) % vocabulary_size\n",
    "\n",
    "def to_hour(x: str) -> int:\n",
    "    \"\"\"提取时间中的小时\"\"\"\n",
    "    try:\n",
    "        dt = pd.to_datetime(x)\n",
    "        return dt.hour\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def to_weekday(x: str) -> int:\n",
    "    \"\"\"提取时间中的星期\"\"\"\n",
    "    try:\n",
    "        dt = pd.to_datetime(x)\n",
    "        return dt.weekday()\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def list_len(x: List) -> int:\n",
    "    \"\"\"列表长度\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return len(x)\n",
    "    return 0\n",
    "\n",
    "def int_max(x: int, max_value: int) -> int:\n",
    "    \"\"\"限制整数最大值\"\"\"\n",
    "    return min(int(x), max_value)\n",
    "\n",
    "def json_object_to_list(x: str, key: str) -> List[str]:\n",
    "    \"\"\"从JSON对象列表中提取指定键的值\"\"\"\n",
    "    try:\n",
    "        data = json.loads(x)\n",
    "        if isinstance(data, list):\n",
    "            return [item.get(key, 'null') for item in data if isinstance(item, dict)]\n",
    "        return ['null']\n",
    "    except:\n",
    "        return ['null']\n",
    "\n",
    "def map_to_int(x: Union[str, List], map_dict: Dict[str, int], default_code: int = 0) -> Union[List[int], int]:\n",
    "    \"\"\"映射到整数\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return [map_dict.get(item, default_code) for item in x]\n",
    "    else:\n",
    "        return map_dict.get(str(x), default_code)\n",
    "\n",
    "# 构建操作中心 (OP_HUB)\n",
    "OP_HUB = {\n",
    "    'fillna': fillna,\n",
    "    'split': split,\n",
    "    'seperation': seperation,\n",
    "    'list_get': list_get,\n",
    "    'remove_items': remove_items,\n",
    "    'padding': padding,\n",
    "    'list_hash': list_hash,\n",
    "    'str_hash': str_hash,\n",
    "    'to_hour': to_hour,\n",
    "    'to_weekday': to_weekday,\n",
    "    'list_len': list_len,\n",
    "    'int_max': int_max,\n",
    "    'json_object_to_list': json_object_to_list,\n",
    "    'map_to_int': map_to_int\n",
    "}\n",
    "\n",
    "print(f\"OP_HUB 构建完成，包含 {len(OP_HUB)} 个操作函数\")\n",
    "print(f\"可用函数: {list(OP_HUB.keys())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 实现原子操作作用与df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_op(df: pd.DataFrame, operation: dict) -> pd.DataFrame:\n",
    "    \"\"\"执行单个特征操作\"\"\"\n",
    "    # 获取操作配置\n",
    "    col_in = operation['col_in']\n",
    "    col_out = operation['col_out']\n",
    "    func_name = operation['func_name']\n",
    "    parameters = operation.get('func_parameters', {})\n",
    "    \n",
    "    # 检查函数是否存在\n",
    "    if func_name not in OP_HUB:\n",
    "        return df\n",
    "    \n",
    "    # 检查输入列是否存在\n",
    "    input_cols = [col_in] if isinstance(col_in, str) else col_in\n",
    "    if not all(col in df.columns for col in input_cols):\n",
    "        return df\n",
    "    \n",
    "    # 准备特征转换函数\n",
    "    transform_func = partial(OP_HUB[func_name], **parameters)\n",
    "    \n",
    "    # 执行特征转换\n",
    "    if isinstance(col_in, list):\n",
    "        df[col_out] = df[col_in].apply(lambda row: transform_func(*row), axis=1)\n",
    "    else:\n",
    "        df[col_out] = df[col_in].apply(transform_func)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.3 实现原子操作拼接成完整的process函数作用与df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_feature_pipelines(df_raw: pd.DataFrame, feat_config: dict) -> tuple[pd.DataFrame, list]:\n",
    "    \"\"\"执行特征工程流水线 - 适配不同的配置文件格式\"\"\"\n",
    "    # 创建数据副本\n",
    "    df = df_raw.copy()\n",
    "    \n",
    "    # 获取需要处理的流水线 - 适配不同格式\n",
    "    if 'pipelines' in feat_config:\n",
    "        # feat.yml 格式: { pipelines: [...] }\n",
    "        pipelines = feat_config['pipelines']\n",
    "    elif 'process' in feat_config and 'pipelines' in feat_config['process']:\n",
    "        # config.yml 格式: { process: { pipelines: [...] } }\n",
    "        pipelines = feat_config['process']['pipelines']\n",
    "    else:\n",
    "        print(\"⚠️ 无法找到特征配置中的 pipelines\")\n",
    "        return df, []\n",
    "\n",
    "    # 记录成功处理的特征\n",
    "    processed_features = []\n",
    "    \n",
    "    # 执行每个特征处理流水线\n",
    "    for pipeline in pipelines:\n",
    "        feat_name = pipeline['feat_name']\n",
    "        operations = pipeline['operations']\n",
    "        \n",
    "        # 执行流水线中的每个操作\n",
    "        for operation in operations:\n",
    "            df = run_one_op(df, operation)\n",
    "\n",
    "        # 记录处理成功的特征\n",
    "        processed_features.append(feat_name)\n",
    "    \n",
    "    return df, processed_features\n",
    "\n",
    "df_processed, processed_features = process_feature_pipelines(df_raw, feat_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 特征处理后数据集分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据结构对比:\n",
      "原始列数: 13\n",
      "处理后列数: 30\n",
      "新增列数: 17\n",
      "\n",
      "原始列名:\n",
      "['user_id', 'create_time', 'log_type', 'watchlists', 'holdings', 'country', 'prefer_bid', 'user_propernoun', 'push_title', 'push_content', 'item_code', 'item_tags', 'submit_type']\n",
      "\n",
      "新增列名:\n",
      "['hour', 'weekday', 'user_watch_stk_code', 'user_watch_stk_code_hash', 'country_hash', 'prefer_bid_code', 'prefer_bid_code_hash', 'hold_bid_code', 'hold_bid_code_hash', 'user_propernoun_code', 'user_propernoun_hash', 'push_title_hash', 'title_len', 'item_code_hash', 'submit_type_hash', 'tagIds', 'tag_id_hash']\n",
      "成功生成的特征详情:\n",
      "  hour: int64 = 8\n",
      "  weekday: int64 = 5\n",
      "  user_watch_stk_code_hash: list = [8381, 8381, 8381, 8381, 8381]\n",
      "  country_hash: int64 = 71\n",
      "  prefer_bid_code_hash: list = [8381, 8381, 8381, 8381, 8381]\n",
      "  hold_bid_code_hash: list = [8381, 8381, 8381, 8381, 8381]\n",
      "  user_propernoun_hash: list = [178, 417, 8381, 8381, 8381]\n",
      "  push_title_hash: int64 = 7\n",
      "  title_len: int64 = 12\n",
      "  item_code_hash: list = [6837, 3491, 8381, 8381, 8381]\n",
      "  submit_type_hash: int64 = 6\n",
      "  tag_id_hash: list = [39, 93, 80]\n",
      "最终处理结果预览:\n",
      "      user_id log_type  hour  weekday        user_watch_stk_code_hash  \\\n",
      "0  1800001088       PR     8        5  [8381, 8381, 8381, 8381, 8381]   \n",
      "1  1800001417       PR    22        5  [8381, 8381, 8381, 8381, 8381]   \n",
      "2  1800001501       PC    10        5  [1895, 8808, 1021, 8381, 8381]   \n",
      "3  1800001501       PR    22        5  [1895, 8808, 1021, 8381, 8381]   \n",
      "4  1800001819       PR    21        5  [8381, 8381, 8381, 8381, 8381]   \n",
      "\n",
      "   country_hash            prefer_bid_code_hash  \\\n",
      "0            71  [8381, 8381, 8381, 8381, 8381]   \n",
      "1           145  [8381, 8381, 8381, 8381, 8381]   \n",
      "2           145  [8381, 8381, 8381, 8381, 8381]   \n",
      "3           145  [8381, 8381, 8381, 8381, 8381]   \n",
      "4           145  [8381, 8381, 8381, 8381, 8381]   \n",
      "\n",
      "               hold_bid_code_hash            user_propernoun_hash  \\\n",
      "0  [8381, 8381, 8381, 8381, 8381]    [178, 417, 8381, 8381, 8381]   \n",
      "1  [8381, 8381, 8381, 8381, 8381]  [8381, 8381, 8381, 8381, 8381]   \n",
      "2  [8381, 8381, 8381, 8381, 8381]   [323, 9351, 3453, 8381, 8381]   \n",
      "3  [8381, 8381, 8381, 8381, 8381]   [323, 9351, 3453, 8381, 8381]   \n",
      "4  [8381, 8381, 8381, 8381, 8381]  [8381, 8381, 8381, 8381, 8381]   \n",
      "\n",
      "   push_title_hash  title_len                  item_code_hash  \\\n",
      "0                7         12  [6837, 3491, 8381, 8381, 8381]   \n",
      "1                7         10  [7762, 6902, 6157, 1986, 5551]   \n",
      "2                5         17  [9724, 8381, 8381, 8381, 8381]   \n",
      "3                7         10  [7762, 6902, 6157, 1986, 5551]   \n",
      "4                4         11   [916, 8381, 8381, 8381, 8381]   \n",
      "\n",
      "   submit_type_hash   tag_id_hash  \n",
      "0                 6  [39, 93, 80]  \n",
      "1                 4   [34, 6, 27]  \n",
      "2                 1   [1, 80, 97]  \n",
      "3                 4   [34, 6, 27]  \n",
      "4                 1  [93, 80, 81]  \n"
     ]
    }
   ],
   "source": [
    "print(\"数据结构对比:\")\n",
    "print(f\"原始列数: {len(df_raw.columns)}\")\n",
    "print(f\"处理后列数: {len(df_processed.columns)}\")\n",
    "print(f\"新增列数: {len(df_processed.columns) - len(df_raw.columns)}\")\n",
    "\n",
    "print(\"\\n原始列名:\")\n",
    "print(list(df_raw.columns))\n",
    "\n",
    "print(\"\\n新增列名:\")\n",
    "new_columns = [col for col in df_processed.columns if col not in df_raw.columns]\n",
    "print(new_columns)\n",
    "\n",
    "# 查看成功生成的特征\n",
    "print(\"成功生成的特征详情:\")\n",
    "for feat_name in processed_features:\n",
    "    if feat_name in df_processed.columns:\n",
    "        sample_data = df_processed[feat_name].iloc[0]\n",
    "        data_type = type(sample_data).__name__\n",
    "        print(f\"  {feat_name}: {data_type} = {sample_data}\")\n",
    "\n",
    "print(\"最终处理结果预览:\")\n",
    "display_cols = ['user_id', 'log_type'] + processed_features\n",
    "display_cols = [col for col in display_cols if col in df_processed.columns]\n",
    "\n",
    "print(df_processed[display_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 模型训练与评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(df_processed, processed_features, max_list_length=5):\n",
    "    \"\"\"展开列表特征 树模型的需求\"\"\"\n",
    "    df_tree = df_processed[processed_features].copy()\n",
    "    \n",
    "    for feat in processed_features:\n",
    "        if isinstance(df_tree[feat].iloc[0], list):\n",
    "            expanded = df_tree[feat].apply(pd.Series).iloc[:, :max_list_length]\n",
    "            expanded.columns = [f\"{feat}_{i}\" for i in range(expanded.shape[1])]\n",
    "            df_tree = df_tree.drop(columns=[feat]).join(expanded)\n",
    "    \n",
    "    return df_tree\n",
    "\n",
    "def train_model(X, y, train_params):\n",
    "    \"\"\"训练LightGBM模型\"\"\"\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train, y_train)\n",
    "    val_data = lgb.Dataset(X_val, y_val, reference=train_data)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        train_params,\n",
    "        train_data,\n",
    "        num_boost_round=train_params.pop('num_iterations', 1000),\n",
    "        callbacks=[lgb.early_stopping(train_params.pop('early_stopping_rounds', 100))],\n",
    "        valid_sets=[train_data, val_data],\n",
    "        valid_names=['train', 'valid']\n",
    "    )\n",
    "    \n",
    "    return model, X_train, X_val, y_train, y_val\n",
    "\n",
    "def evaluate_model(model, X_train, X_val, y_train, y_val):\n",
    "    \"\"\"评估模型性能\"\"\"\n",
    "    y_train_pred = model.predict(X_train, num_iteration=model.best_iteration)\n",
    "    y_val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    \n",
    "    train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "    val_auc = roc_auc_score(y_val, y_val_pred)\n",
    "    \n",
    "    print(f\"训练集 AUC: {train_auc:.4f}\")\n",
    "    print(f\"验证集 AUC: {val_auc:.4f}\")\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'feature': model.feature_name(),\n",
    "        'importance': model.feature_importance(importance_type='gain')\n",
    "    }).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[300]\ttrain's auc: 0.864733\tvalid's auc: 0.847046\n",
      "训练集 AUC: 0.8647\n",
      "验证集 AUC: 0.8470\n",
      "\n",
      "特征重要性 (Top 20):\n",
      "                       feature     importance\n",
      "23      user_propernoun_hash_2  346798.388273\n",
      "21      user_propernoun_hash_0  115543.097794\n",
      "2                 country_hash   60724.277898\n",
      "22      user_propernoun_hash_1   55430.613762\n",
      "1                      weekday   23594.930362\n",
      "0                         hour   10044.143828\n",
      "32               tag_id_hash_1    5167.244439\n",
      "5             submit_type_hash    4476.428500\n",
      "10  user_watch_stk_code_hash_4    4473.375479\n",
      "31               tag_id_hash_0    4001.311887\n",
      "33               tag_id_hash_2    3325.424608\n",
      "4                    title_len    3310.380234\n",
      "6   user_watch_stk_code_hash_0    3213.638852\n",
      "3              push_title_hash    3065.194983\n",
      "7   user_watch_stk_code_hash_1    2862.350930\n",
      "8   user_watch_stk_code_hash_2    2614.035170\n",
      "9   user_watch_stk_code_hash_3    1983.679067\n",
      "11      prefer_bid_code_hash_0    1954.205699\n",
      "13      prefer_bid_code_hash_2    1802.084255\n",
      "26            item_code_hash_0    1788.037595\n"
     ]
    }
   ],
   "source": [
    "# 树模型使用config.yml配置\n",
    "with open('config/config.yml', 'r', encoding='utf-8') as f:\n",
    "    tree_config = yaml.safe_load(f)\n",
    "\n",
    "# 准备数据\n",
    "df_processed['label'] = df_processed['log_type'].apply(lambda x: 1 if x == 'PC' else 0)\n",
    "X = prepare_features(df_processed, processed_features)\n",
    "y = df_processed['label']\n",
    "\n",
    "# 训练模型\n",
    "train_params = {**tree_config['train'], 'verbose': -1, 'n_jobs': -1, 'seed': 42}\n",
    "model, X_train, X_val, y_train, y_val = train_model(X, y, train_params)\n",
    "\n",
    "# 评估并输出结果\n",
    "feature_importance = evaluate_model(model, X_train, X_val, y_train, y_val)\n",
    "print(\"\\n特征重要性 (Top 20):\")\n",
    "print(feature_importance.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 MLP模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 环境设置与导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout, BatchNormalization, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# 树模型时已经引入的\n",
    "# import os\n",
    "# import sys\n",
    "# import json\n",
    "# from functools import partial\n",
    "# from datetime import datetime\n",
    "# from hashlib import md5\n",
    "# from typing import Any, Dict, List, Union\n",
    "# import pandas as pd\n",
    "# import yaml\n",
    "# import numpy as np\n",
    "# from pprint import pprint\n",
    "# from glob import glob  # 添加glob模块导入\n",
    "# import time\n",
    "# import lightgbm as lgb\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 加载原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检测到CSV文件，使用本地环境配置\n",
      "分隔符: ',', 表头行: 0, 文件数量: 5\n",
      "形状(50000, 13), 列名: ['user_id', 'create_time', 'log_type', 'watchlists', 'holdings', 'country', 'prefer_bid', 'user_propernoun', 'push_title', 'push_content', 'item_code', 'item_tags', 'submit_type']\n",
      "{'user_id': 1800001088, 'create_time': '2025-05-31 08:39:07', 'log_type': 'PR', 'watchlists': nan, 'holdings': nan, 'country': 'Germany', 'prefer_bid': nan, 'user_propernoun': 'germany#3.06|mid-america#1.02', 'push_title': 'Ainvest Newswire', 'push_content': 'Hims & Hers Health Lays Off 4% of Staff Amid Strategy Shift', 'item_code': '[{\"market\":\"169\",\"score\":0,\"code\":\"HIMS\",\"tagId\":\"U000012934\",\"name\":\"Hims & Hers Health\",\"type\":0,\"parentId\":\"US_ROBOT0f37d7fd3fca6a41\"},{\"market\":\"169\",\"score\":0,\"code\":\"NVO\",\"tagId\":\"U000002999\",\"name\":\"Novo Nordisk\",\"type\":0,\"parentId\":\"US_ROBOT0f37d7fd3fca6a41\"}]', 'item_tags': '[{\"score\":0.7803922295570374,\"tagId\":\"51510\",\"name\":\"us_high_importance\",\"type\":4,\"parentId\":\"US_ROBOT0f37d7fd3fca6a41\"},{\"score\":0.7803922295570374,\"tagId\":\"57967\",\"name\":\"Fusion\",\"type\":4,\"parentId\":\"US_ROBOT0f37d7fd3fca6a41\"},{\"tagId\":\"1002\",\"name\":\"no_penny_stock\",\"type\":4,\"parentId\":\"US_ROBOT0f37d7fd3fca6a41\"}]', 'submit_type': 'autoFlash'}\n"
     ]
    }
   ],
   "source": [
    "# 02 根据配置文件加载原始数据\n",
    "# 和树模型部分一样\n",
    "def load_raw_data_from_config():\n",
    "    \"\"\"根据data.yml配置文件加载原始数据，支持本地和线上环境\"\"\"\n",
    "    \n",
    "    # 加载数据配置\n",
    "    with open('config/data.yml', 'r', encoding='utf-8') as f:\n",
    "        data_config = yaml.safe_load(f)\n",
    "    \n",
    "    # 检测文件类型和环境\n",
    "    train_dir = data_config['train_dir']\n",
    "    \n",
    "    # 检查CSV和TXT文件\n",
    "    csv_files = glob(os.path.join(train_dir, '*.csv'))\n",
    "    txt_files = glob(os.path.join(train_dir, '*.txt'))\n",
    "    \n",
    "    if csv_files:\n",
    "        # 本地环境 - 使用CSV格式\n",
    "        print(\"检测到CSV文件，使用本地环境配置\")\n",
    "        csv_config = data_config['csv_format']\n",
    "        separator, header = csv_config['separator'], csv_config['header']\n",
    "        \n",
    "        print(f\"分隔符: '{separator}', 表头行: {header}, 文件数量: {len(csv_files)}\")\n",
    "        \n",
    "        # 读取CSV文件\n",
    "        dfs = [pd.read_csv(f, sep=separator, header=header) for f in csv_files]\n",
    "        \n",
    "    elif txt_files:\n",
    "        # 线上环境 - 使用TXT格式\n",
    "        print(\"检测到TXT文件，使用线上环境配置\")\n",
    "        txt_config = data_config.get('txt_format', {'separator': '\\t', 'header': None})\n",
    "        separator, header = txt_config['separator'], txt_config['header']\n",
    "        \n",
    "        # 从列表中提取列名\n",
    "        raw_columns = [list(item.keys())[0] for item in data_config['raw_data_columns']]\n",
    "        \n",
    "        print(f\"分隔符: '{separator}', 表头行: {header}, 文件数量: {len(txt_files)}\")\n",
    "        print(f\"预定义列名: {raw_columns}\")\n",
    "        \n",
    "        # 读取TXT文件\n",
    "        dfs = [pd.read_csv(f, sep=separator, header=header, names=raw_columns) for f in txt_files]\n",
    "    else:\n",
    "        raise ValueError(f\"在目录 {train_dir} 中未找到CSV或TXT文件\")\n",
    "    \n",
    "    df_raw = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"形状{df_raw.shape}, 列名: {list(df_raw.columns)}\")\n",
    "    \n",
    "    return df_raw\n",
    "\n",
    "# 加载原始数据\n",
    "df_raw = load_raw_data_from_config()\n",
    "\n",
    "# 显示数据样例（简化版）\n",
    "print(df_raw.iloc[0].to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 加载并解析YAML配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 256, 'epochs': 2, 'lr': 0.0005, 'weight_decay': 0.001, 'lr_scheduler': {'type': 'ExponentialDecay', 'decay_steps': 100, 'decay_rate': 0.96, 'staircase': True, 'min_lr': 1e-05}, 'early_stopping': {'monitor': 'val_auc', 'patience': 2, 'mode': 'max', 'restore_best_weights': True}, 'reduce_lr': {'monitor': 'val_loss', 'factor': 0.5, 'patience': 1, 'min_lr': 1e-05}, 'shuffle_buffer_size': 20000, 'prefetch': 'auto', 'save_model': True, 'model_path': './models/push_binary_classification_model.keras', 'best_model_path': './models/best_model.keras', 'save_best_only': True, 'validation_split': 0.2}\n",
      "{'layers': [128, 64, 32], 'dropout_rates': [0.3, 0.3, 0.2], 'l2_regularization': 0.001}\n",
      "{'exclude_features': {'current': 'default', 'default': [], 'exclude_user_behavior': ['user_watch_stk_code', 'prefer_bid_code', 'hold_bid_code', 'user_propernoun'], 'exclude_user_propernoun': ['user_propernoun']}, 'pipelines': [{'embedding_dim': 8, 'feat_name': 'hour', 'feat_type': 'sparse', 'input_sample': '2024-08-02 00:44:05', 'operations': [{'col_in': 'create_time', 'col_out': 'create_time', 'func_name': 'fillna', 'func_parameters': {'na_value': '2024-08-02 00:16:34'}}, {'col_in': 'create_time', 'col_out': 'hour', 'func_name': 'to_hour', 'func_parameters': {}}], 'vocabulary_size': 24}, {'embedding_dim': 8, 'feat_name': 'weekday', 'feat_type': 'sparse', 'input_sample': '2024-08-02 00:44:05', 'operations': [{'col_in': 'create_time', 'col_out': 'weekday', 'func_name': 'to_weekday', 'func_parameters': {}}], 'vocabulary_size': 7}, {'embedding_dim': 8, 'feat_name': 'user_watch_stk_code_hash', 'feat_type': 'varlen_sparse', 'input_sample': 'AAPL_185 & TSLA_185', 'operations': [{'col_in': 'watchlists', 'col_out': 'watchlists', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null_0 & null_0'}}, {'col_in': 'watchlists', 'col_out': 'watchlists', 'func_name': 'split', 'func_parameters': {'sep': ' & '}}, {'col_in': 'watchlists', 'col_out': 'watchlists', 'func_name': 'seperation', 'func_parameters': {'sep': '_'}}, {'col_in': 'watchlists', 'col_out': 'user_watch_stk_code', 'func_name': 'list_get', 'func_parameters': {'item_index': 0}}, {'col_in': 'user_watch_stk_code', 'col_out': 'user_watch_stk_code', 'func_name': 'remove_items', 'func_parameters': {'target_values': ['AAPL', 'AMZN', 'GOOGL', 'TSLA']}}, {'col_in': 'user_watch_stk_code', 'col_out': 'user_watch_stk_code', 'func_name': 'padding', 'func_parameters': {'max_len': 5, 'pad_value': 'null'}}, {'col_in': 'user_watch_stk_code', 'col_out': 'user_watch_stk_code_hash', 'func_name': 'list_hash', 'func_parameters': {'vocabulary_size': 10000}}], 'vocabulary_size': 10000}, {'embedding_dim': 8, 'feat_name': 'country_hash', 'feat_type': 'sparse', 'input_sample': 'United States', 'operations': [{'col_in': 'country', 'col_out': 'country', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null'}}, {'col_in': 'country', 'col_out': 'country_hash', 'func_name': 'str_hash', 'func_parameters': {'vocabulary_size': 200}}], 'vocabulary_size': 200}, {'embedding_dim': 8, 'feat_name': 'prefer_bid_code_hash', 'feat_type': 'varlen_sparse', 'input_sample': 'AAPL#0.24809|AMZN#0.24809|GOOGL#0.24809|TSLA#0.24809', 'operations': [{'col_in': 'prefer_bid', 'col_out': 'prefer_bid', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null#0'}}, {'col_in': 'prefer_bid', 'col_out': 'prefer_bid', 'func_name': 'split', 'func_parameters': {'sep': '|'}}, {'col_in': 'prefer_bid', 'col_out': 'prefer_bid', 'func_name': 'seperation', 'func_parameters': {'sep': '#'}}, {'col_in': 'prefer_bid', 'col_out': 'prefer_bid_code', 'func_name': 'list_get', 'func_parameters': {'item_index': 0}}, {'col_in': 'prefer_bid_code', 'col_out': 'prefer_bid_code', 'func_name': 'padding', 'func_parameters': {'max_len': 5, 'pad_value': 'null'}}, {'col_in': 'prefer_bid_code', 'col_out': 'prefer_bid_code_hash', 'func_name': 'list_hash', 'func_parameters': {'vocabulary_size': 10000}}], 'vocabulary_size': 10000}, {'embedding_dim': 8, 'feat_name': 'hold_bid_code_hash', 'feat_type': 'varlen_sparse', 'input_sample': 'JD,185|BAC,169|TSLA,185', 'operations': [{'col_in': 'holdings', 'col_out': 'holdings', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null,0'}}, {'col_in': 'holdings', 'col_out': 'holdings', 'func_name': 'split', 'func_parameters': {'sep': '|'}}, {'col_in': 'holdings', 'col_out': 'holdings', 'func_name': 'seperation', 'func_parameters': {'sep': ','}}, {'col_in': 'holdings', 'col_out': 'hold_bid_code', 'func_name': 'list_get', 'func_parameters': {'item_index': 0}}, {'col_in': 'hold_bid_code', 'col_out': 'hold_bid_code', 'func_name': 'padding', 'func_parameters': {'max_len': 5, 'pad_value': 'null'}}, {'col_in': 'hold_bid_code', 'col_out': 'hold_bid_code_hash', 'func_name': 'list_hash', 'func_parameters': {'vocabulary_size': 10000}}], 'vocabulary_size': 10000}, {'embedding_dim': 8, 'feat_name': 'user_propernoun_hash', 'feat_type': 'varlen_sparse', 'input_sample': 'apple#1.02|nike#1.02', 'operations': [{'col_in': 'user_propernoun', 'col_out': 'user_propernoun', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null#0'}}, {'col_in': 'user_propernoun', 'col_out': 'user_propernoun', 'func_name': 'split', 'func_parameters': {'sep': '|'}}, {'col_in': 'user_propernoun', 'col_out': 'user_propernoun', 'func_name': 'seperation', 'func_parameters': {'sep': '#'}}, {'col_in': 'user_propernoun', 'col_out': 'user_propernoun_code', 'func_name': 'list_get', 'func_parameters': {'item_index': 0}}, {'col_in': 'user_propernoun_code', 'col_out': 'user_propernoun_code', 'func_name': 'padding', 'func_parameters': {'max_len': 5, 'pad_value': 'null'}}, {'col_in': 'user_propernoun_code', 'col_out': 'user_propernoun_hash', 'func_name': 'list_hash', 'func_parameters': {'vocabulary_size': 10000}}], 'vocabulary_size': 10000}, {'embedding_dim': 8, 'feat_name': 'push_title_hash', 'feat_type': 'sparse', 'input_sample': 'Breaking News', 'operations': [{'col_in': 'push_title', 'col_out': 'push_title', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null'}}, {'col_in': 'push_title', 'col_out': 'push_title_hash', 'func_name': 'str_hash', 'func_parameters': {'vocabulary_size': 8}}], 'vocabulary_size': 8}, {'embedding_dim': 8, 'feat_name': 'title_len', 'feat_type': 'sparse', 'input_sample': \"Hong Kong's Leading Broker Futu Securities Introduces Bitcoin and XRP Trading with Lucrative Incentives\", 'operations': [{'col_in': 'push_content', 'col_out': 'push_content', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null'}}, {'col_in': 'push_content', 'col_out': 'push_content', 'func_name': 'split', 'func_parameters': {'sep': ' '}}, {'col_in': 'push_content', 'col_out': 'title_len', 'func_name': 'list_len', 'func_parameters': {}}, {'col_in': 'title_len', 'col_out': 'title_len', 'func_name': 'int_max', 'func_parameters': {'max_value': 31}}], 'vocabulary_size': 32}, {'embedding_dim': 8, 'feat_name': 'item_code_hash', 'feat_type': 'varlen_sparse', 'input_sample': '[{\"market\":\"185\",\"score\":1,\"code\":\"META\",\"name\":\"Meta\",\"type\":0,\"parentId\":\"0339437d07195361\"}]', 'operations': [{'col_in': 'item_code', 'col_out': 'item_code', 'func_name': 'fillna', 'func_parameters': {'na_value': '[{\"market\":\"null\",\"score\":0,\"code\":\"null\",\"name\":\"null\",\"type\":\"null\",\"parentId\":\"null\"}]'}}, {'col_in': 'item_code', 'col_out': 'item_code', 'func_name': 'json_object_to_list', 'func_parameters': {'key': 'code'}}, {'col_in': 'item_code', 'col_out': 'item_code', 'func_name': 'padding', 'func_parameters': {'max_len': 5, 'pad_value': 'null'}}, {'col_in': 'item_code', 'col_out': 'item_code_hash', 'func_name': 'list_hash', 'func_parameters': {'vocabulary_size': 10000}}], 'vocabulary_size': 10000}, {'embedding_dim': 8, 'feat_name': 'submit_type_hash', 'feat_type': 'sparse', 'input_sample': 'auto_flash', 'operations': [{'col_in': 'submit_type', 'col_out': 'submit_type', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null'}}, {'col_in': 'submit_type', 'col_out': 'submit_type_hash', 'func_name': 'str_hash', 'func_parameters': {'vocabulary_size': 10}}], 'vocabulary_size': 10}, {'embedding_dim': 8, 'feat_name': 'tag_id_hash', 'feat_type': 'varlen_sparse', 'input_sample': '[{\"score\":0,\"tagId\":\"57967\",\"name\":\"Fusion\",\"type\":4,\"parentId\":\"0339437d07195361\"}]', 'operations': [{'col_in': 'item_tags', 'col_out': 'item_tags', 'func_name': 'fillna', 'func_parameters': {'na_value': '[{\"score\":0,\"tagId\":\"null\",\"name\":\"null\",\"type\":0,\"parentId\":\"null\"}]'}}, {'col_in': 'item_tags', 'col_out': 'tagIds', 'func_name': 'json_object_to_list', 'func_parameters': {'key': 'tagId'}}, {'col_in': 'tagIds', 'col_out': 'tagIds', 'func_name': 'padding', 'func_parameters': {'max_len': 3, 'pad_value': 'null'}}, {'col_in': 'tagIds', 'col_out': 'tag_id_hash', 'func_name': 'list_hash', 'func_parameters': {'vocabulary_size': 10000}}], 'vocabulary_size': 10000}]}\n"
     ]
    }
   ],
   "source": [
    "# 加载深度模型的训练和模型配置（train.yml）\n",
    "with open('config/train.yml', 'r', encoding='utf-8') as f:\n",
    "    train_config = yaml.safe_load(f)\n",
    "\n",
    "training_config = train_config.get('training', {})\n",
    "model_config = train_config.get('model', {})\n",
    "\n",
    "# 加载深度模型的特征配置 (feat.yml)\n",
    "with open('config/feat.yml', 'r', encoding='utf-8') as f:\n",
    "    deep_feat_config = yaml.safe_load(f)\n",
    "\n",
    "print(training_config)\n",
    "print(model_config)\n",
    "print(deep_feat_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 进行数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1 定义原子操作函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OP_HUB 构建完成，包含 14 个操作函数\n",
      "可用函数: ['fillna', 'split', 'seperation', 'list_get', 'remove_items', 'padding', 'list_hash', 'str_hash', 'to_hour', 'to_weekday', 'list_len', 'int_max', 'json_object_to_list', 'map_to_int']\n"
     ]
    }
   ],
   "source": [
    "MISSING_VALUE = [None, '', 'null', 'NULL', 'None', np.nan]\n",
    "\n",
    "def fillna(x: Union[float, int, str], na_value: Union[float, int, str]) -> Union[float, int, str]:\n",
    "    \"\"\"填充缺失值\"\"\"\n",
    "    if x in MISSING_VALUE or (isinstance(x, float) and pd.isna(x)):\n",
    "        return na_value\n",
    "    return x\n",
    "\n",
    "def split(x: str, sep: str) -> List[str]:\n",
    "    \"\"\"字符串分割\"\"\"\n",
    "    return str(x).split(sep)\n",
    "\n",
    "def seperation(x: List[str], sep: str) -> List[List[str]]:\n",
    "    \"\"\"列表元素二次分割\"\"\"\n",
    "    if not isinstance(x, list):\n",
    "        return []\n",
    "    return [item.split(sep) for item in x]\n",
    "\n",
    "def list_get(x: List[List[Any]], item_index: int) -> List[Any]:\n",
    "    \"\"\"获取嵌套列表中指定位置的元素\"\"\"\n",
    "    if not isinstance(x, list):\n",
    "        return []\n",
    "    result = []\n",
    "    for sublist in x:\n",
    "        if isinstance(sublist, list) and len(sublist) > item_index:\n",
    "            result.append(sublist[item_index])\n",
    "        else:\n",
    "            result.append('null')\n",
    "    return result\n",
    "\n",
    "def remove_items(x: List[str], target_values: List[str]) -> List[str]:\n",
    "    \"\"\"移除列表中的指定元素\"\"\"\n",
    "    if not isinstance(x, list):\n",
    "        return []\n",
    "    return [item for item in x if item not in target_values]\n",
    "\n",
    "def padding(x: List[Any], pad_value: Union[str, float, int], max_len: int) -> List[Any]:\n",
    "    \"\"\"列表填充到指定长度\"\"\"\n",
    "    if not isinstance(x, list):\n",
    "        x = []\n",
    "    if len(x) >= max_len:\n",
    "        return x[:max_len]\n",
    "    else:\n",
    "        return x + [pad_value] * (max_len - len(x))\n",
    "\n",
    "def list_hash(x: List[str], vocabulary_size: int) -> List[int]:\n",
    "    \"\"\"对列表中每个元素进行哈希\"\"\"\n",
    "    if not isinstance(x, list):\n",
    "        return []\n",
    "    result = []\n",
    "    for item in x:\n",
    "        hash_val = int(md5(str(item).encode()).hexdigest(), 16) % vocabulary_size\n",
    "        result.append(hash_val)\n",
    "    return result\n",
    "\n",
    "def str_hash(x: str, vocabulary_size: int) -> int:\n",
    "    \"\"\"字符串哈希\"\"\"\n",
    "    return int(md5(str(x).encode()).hexdigest(), 16) % vocabulary_size\n",
    "\n",
    "def to_hour(x: str) -> int:\n",
    "    \"\"\"提取时间中的小时\"\"\"\n",
    "    try:\n",
    "        dt = pd.to_datetime(x)\n",
    "        return dt.hour\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def to_weekday(x: str) -> int:\n",
    "    \"\"\"提取时间中的星期\"\"\"\n",
    "    try:\n",
    "        dt = pd.to_datetime(x)\n",
    "        return dt.weekday()\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def list_len(x: List) -> int:\n",
    "    \"\"\"列表长度\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return len(x)\n",
    "    return 0\n",
    "\n",
    "def int_max(x: int, max_value: int) -> int:\n",
    "    \"\"\"限制整数最大值\"\"\"\n",
    "    return min(int(x), max_value)\n",
    "\n",
    "def json_object_to_list(x: str, key: str) -> List[str]:\n",
    "    \"\"\"从JSON对象列表中提取指定键的值\"\"\"\n",
    "    try:\n",
    "        data = json.loads(x)\n",
    "        if isinstance(data, list):\n",
    "            return [item.get(key, 'null') for item in data if isinstance(item, dict)]\n",
    "        return ['null']\n",
    "    except:\n",
    "        return ['null']\n",
    "\n",
    "def map_to_int(x: Union[str, List], map_dict: Dict[str, int], default_code: int = 0) -> Union[List[int], int]:\n",
    "    \"\"\"映射到整数\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return [map_dict.get(item, default_code) for item in x]\n",
    "    else:\n",
    "        return map_dict.get(str(x), default_code)\n",
    "\n",
    "# 构建操作中心 (OP_HUB)\n",
    "OP_HUB = {\n",
    "    'fillna': fillna,\n",
    "    'split': split,\n",
    "    'seperation': seperation,\n",
    "    'list_get': list_get,\n",
    "    'remove_items': remove_items,\n",
    "    'padding': padding,\n",
    "    'list_hash': list_hash,\n",
    "    'str_hash': str_hash,\n",
    "    'to_hour': to_hour,\n",
    "    'to_weekday': to_weekday,\n",
    "    'list_len': list_len,\n",
    "    'int_max': int_max,\n",
    "    'json_object_to_list': json_object_to_list,\n",
    "    'map_to_int': map_to_int\n",
    "}\n",
    "\n",
    "print(f\"OP_HUB 构建完成，包含 {len(OP_HUB)} 个操作函数\")\n",
    "print(f\"可用函数: {list(OP_HUB.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2 实现原子操作作用于df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_op(df: pd.DataFrame, operation: dict) -> pd.DataFrame:\n",
    "    \"\"\"执行单个特征操作\"\"\"\n",
    "    # 获取操作配置\n",
    "    col_in = operation['col_in']\n",
    "    col_out = operation['col_out']\n",
    "    func_name = operation['func_name']\n",
    "    parameters = operation.get('func_parameters', {})\n",
    "    \n",
    "    # 检查函数是否存在\n",
    "    if func_name not in OP_HUB:\n",
    "        return df\n",
    "    \n",
    "    # 检查输入列是否存在\n",
    "    input_cols = [col_in] if isinstance(col_in, str) else col_in\n",
    "    if not all(col in df.columns for col in input_cols):\n",
    "        return df\n",
    "    \n",
    "    # 准备特征转换函数\n",
    "    transform_func = partial(OP_HUB[func_name], **parameters)\n",
    "    \n",
    "    # 执行特征转换\n",
    "    if isinstance(col_in, list):\n",
    "        df[col_out] = df[col_in].apply(lambda row: transform_func(*row), axis=1)\n",
    "    else:\n",
    "        df[col_out] = df[col_in].apply(transform_func)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.3 实现原子操作拼接成完整的process函数作用于df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_feature_pipelines(df_raw: pd.DataFrame, feat_config: dict) -> tuple[pd.DataFrame, list]:\n",
    "    \"\"\"执行特征工程流水线 - 适配不同的配置文件格式\"\"\"\n",
    "    # 创建数据副本\n",
    "    df = df_raw.copy()\n",
    "    \n",
    "    # 获取需要处理的流水线 - 适配不同格式\n",
    "    if 'pipelines' in feat_config:\n",
    "        # feat.yml 格式: { pipelines: [...] }\n",
    "        pipelines = feat_config['pipelines']\n",
    "    elif 'process' in feat_config and 'pipelines' in feat_config['process']:\n",
    "        # config.yml 格式: { process: { pipelines: [...] } }\n",
    "        pipelines = feat_config['process']['pipelines']\n",
    "    else:\n",
    "        print(\"⚠️ 无法找到特征配置中的 pipelines\")\n",
    "        return df, []\n",
    "\n",
    "    # 记录成功处理的特征\n",
    "    processed_features = []\n",
    "    \n",
    "    # 执行每个特征处理流水线\n",
    "    for pipeline in pipelines:\n",
    "        feat_name = pipeline['feat_name']\n",
    "        operations = pipeline['operations']\n",
    "        \n",
    "        # 执行流水线中的每个操作\n",
    "        for operation in operations:\n",
    "            df = run_one_op(df, operation)\n",
    "\n",
    "        # 记录处理成功的特征\n",
    "        processed_features.append(feat_name)\n",
    "    \n",
    "    return df, processed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label的处理\n",
    "if 'label' not in df_raw.columns:\n",
    "    df_raw['label'] = df_raw['log_type'].apply(lambda x: 1 if x == 'PC' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4节 只有这里是和1.4节不一样的\n",
    "df_deep_processed, processed_features = process_feature_pipelines(df_raw, deep_feat_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 特征处理后的数据分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据结构对比:\n",
      "原始列数: 14\n",
      "处理后列数: 31\n",
      "新增列数: 17\n",
      "\n",
      "原始列名:\n",
      "['user_id', 'create_time', 'log_type', 'watchlists', 'holdings', 'country', 'prefer_bid', 'user_propernoun', 'push_title', 'push_content', 'item_code', 'item_tags', 'submit_type', 'label']\n",
      "\n",
      "新增列名:\n",
      "['hour', 'weekday', 'user_watch_stk_code', 'user_watch_stk_code_hash', 'country_hash', 'prefer_bid_code', 'prefer_bid_code_hash', 'hold_bid_code', 'hold_bid_code_hash', 'user_propernoun_code', 'user_propernoun_hash', 'push_title_hash', 'title_len', 'item_code_hash', 'submit_type_hash', 'tagIds', 'tag_id_hash']\n",
      "成功生成的特征详情:\n",
      "  hour: int64 = 8\n",
      "  weekday: int64 = 5\n",
      "  user_watch_stk_code_hash: list = [8381, 8381, 8381, 8381, 8381]\n",
      "  country_hash: int64 = 71\n",
      "  prefer_bid_code_hash: list = [8381, 8381, 8381, 8381, 8381]\n",
      "  hold_bid_code_hash: list = [8381, 8381, 8381, 8381, 8381]\n",
      "  user_propernoun_hash: list = [178, 417, 8381, 8381, 8381]\n",
      "  push_title_hash: int64 = 7\n",
      "  title_len: int64 = 12\n",
      "  item_code_hash: list = [6837, 3491, 8381, 8381, 8381]\n",
      "  submit_type_hash: int64 = 6\n",
      "  tag_id_hash: list = [8139, 8993, 880]\n",
      "最终处理结果预览:\n",
      "      user_id log_type  hour  weekday        user_watch_stk_code_hash  \\\n",
      "0  1800001088       PR     8        5  [8381, 8381, 8381, 8381, 8381]   \n",
      "1  1800001417       PR    22        5  [8381, 8381, 8381, 8381, 8381]   \n",
      "2  1800001501       PC    10        5  [1895, 8808, 1021, 8381, 8381]   \n",
      "3  1800001501       PR    22        5  [1895, 8808, 1021, 8381, 8381]   \n",
      "4  1800001819       PR    21        5  [8381, 8381, 8381, 8381, 8381]   \n",
      "\n",
      "   country_hash            prefer_bid_code_hash  \\\n",
      "0            71  [8381, 8381, 8381, 8381, 8381]   \n",
      "1           145  [8381, 8381, 8381, 8381, 8381]   \n",
      "2           145  [8381, 8381, 8381, 8381, 8381]   \n",
      "3           145  [8381, 8381, 8381, 8381, 8381]   \n",
      "4           145  [8381, 8381, 8381, 8381, 8381]   \n",
      "\n",
      "               hold_bid_code_hash            user_propernoun_hash  \\\n",
      "0  [8381, 8381, 8381, 8381, 8381]    [178, 417, 8381, 8381, 8381]   \n",
      "1  [8381, 8381, 8381, 8381, 8381]  [8381, 8381, 8381, 8381, 8381]   \n",
      "2  [8381, 8381, 8381, 8381, 8381]   [323, 9351, 3453, 8381, 8381]   \n",
      "3  [8381, 8381, 8381, 8381, 8381]   [323, 9351, 3453, 8381, 8381]   \n",
      "4  [8381, 8381, 8381, 8381, 8381]  [8381, 8381, 8381, 8381, 8381]   \n",
      "\n",
      "   push_title_hash  title_len                  item_code_hash  \\\n",
      "0                7         12  [6837, 3491, 8381, 8381, 8381]   \n",
      "1                7         10  [7762, 6902, 6157, 1986, 5551]   \n",
      "2                5         17  [9724, 8381, 8381, 8381, 8381]   \n",
      "3                7         10  [7762, 6902, 6157, 1986, 5551]   \n",
      "4                4         11   [916, 8381, 8381, 8381, 8381]   \n",
      "\n",
      "   submit_type_hash         tag_id_hash  \n",
      "0                 6   [8139, 8993, 880]  \n",
      "1                 4  [4634, 2106, 9827]  \n",
      "2                 1  [2601, 4380, 4797]  \n",
      "3                 4  [4634, 2106, 9827]  \n",
      "4                 1  [9593, 4380, 8381]  \n"
     ]
    }
   ],
   "source": [
    "print(\"数据结构对比:\")\n",
    "print(f\"原始列数: {len(df_raw.columns)}\")\n",
    "print(f\"处理后列数: {len(df_deep_processed.columns)}\")\n",
    "print(f\"新增列数: {len(df_deep_processed.columns) - len(df_raw.columns)}\")\n",
    "\n",
    "print(\"\\n原始列名:\")\n",
    "print(list(df_raw.columns))\n",
    "\n",
    "print(\"\\n新增列名:\")\n",
    "new_columns = [col for col in df_deep_processed.columns if col not in df_raw.columns]\n",
    "print(new_columns)\n",
    "\n",
    "# 查看成功生成的特征\n",
    "print(\"成功生成的特征详情:\")\n",
    "for feat_name in processed_features:\n",
    "    if feat_name in df_deep_processed.columns:\n",
    "        sample_data = df_deep_processed[feat_name].iloc[0]\n",
    "        data_type = type(sample_data).__name__\n",
    "        print(f\"  {feat_name}: {data_type} = {sample_data}\")\n",
    "\n",
    "print(\"最终处理结果预览:\")\n",
    "display_cols = ['user_id', 'log_type'] + processed_features\n",
    "display_cols = [col for col in display_cols if col in df_deep_processed.columns]\n",
    "\n",
    "print(df_deep_processed[display_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 将pd数据集转为tf数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tf_dataset_for_deep_model(df: pd.DataFrame, feat_config: dict, batch_size: int = 512) -> tf.data.Dataset:\n",
    "    \"\"\"准备TensorFlow数据集用于深度模型训练\"\"\"\n",
    "    \n",
    "    # 获取需要的特征 - 适配不同的配置文件格式\n",
    "    if 'pipelines' in feat_config:\n",
    "        # feat.yml 格式\n",
    "        pipelines = feat_config['pipelines']\n",
    "    elif 'process' in feat_config and 'pipelines' in feat_config['process']:\n",
    "        # config.yml 格式\n",
    "        pipelines = feat_config['process']['pipelines']\n",
    "    else:\n",
    "        raise ValueError(\"无法找到特征配置中的 pipelines\")\n",
    "    \n",
    "    pipeline_feats = {p['feat_name']: p for p in pipelines}\n",
    "    features_dict = {}\n",
    "    \n",
    "    for feat_name, config in pipeline_feats.items():\n",
    "        if feat_name not in df.columns:\n",
    "            continue\n",
    "        \n",
    "        feat_type = config.get('feat_type', 'sparse')\n",
    "        \n",
    "        if feat_type == 'sparse':\n",
    "            # 单值特征，shape为 (batch_size,) - 这是关键！\n",
    "            values = df[feat_name].values.astype(np.int32)\n",
    "            features_dict[feat_name] = values\n",
    "            \n",
    "        elif feat_type == 'varlen_sparse':\n",
    "            # 变长特征，需要padding\n",
    "            sequences = df[feat_name].tolist()\n",
    "            max_len = max(len(seq) if isinstance(seq, list) else 1 for seq in sequences)\n",
    "            \n",
    "            padded_sequences = []\n",
    "            for seq in sequences:\n",
    "                if isinstance(seq, list):\n",
    "                    padded = seq + [0] * (max_len - len(seq))\n",
    "                else:\n",
    "                    padded = [int(seq)] + [0] * (max_len - 1)\n",
    "                padded_sequences.append(padded[:max_len])\n",
    "            \n",
    "            features_dict[feat_name] = np.array(padded_sequences, dtype=np.int32)\n",
    "            \n",
    "        elif feat_type == 'dense':\n",
    "            # 密集特征\n",
    "            values = df[feat_name].values.astype(np.float32).reshape(-1, 1)\n",
    "            features_dict[feat_name] = values\n",
    "    \n",
    "    # 准备标签\n",
    "    labels = df['label'].values.astype(np.int32)\n",
    "    \n",
    "    # 创建数据集\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features_dict, labels))\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "batch_size = train_config.get('training', {}).get('batch_size', 256)  # 使用train.yml的默认值\n",
    "full_dataset = prepare_tf_dataset_for_deep_model(df_deep_processed, deep_feat_config, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集格式验证:\n",
      "  特征数量: 12\n",
      "  - hour: shape=(256,), dtype=<dtype: 'int32'>\n",
      "  - weekday: shape=(256,), dtype=<dtype: 'int32'>\n",
      "  - user_watch_stk_code_hash: shape=(256, 5), dtype=<dtype: 'int32'>\n",
      "  - country_hash: shape=(256,), dtype=<dtype: 'int32'>\n",
      "  - prefer_bid_code_hash: shape=(256, 5), dtype=<dtype: 'int32'>\n",
      "  - hold_bid_code_hash: shape=(256, 5), dtype=<dtype: 'int32'>\n",
      "  - user_propernoun_hash: shape=(256, 5), dtype=<dtype: 'int32'>\n",
      "  - push_title_hash: shape=(256,), dtype=<dtype: 'int32'>\n",
      "  - title_len: shape=(256,), dtype=<dtype: 'int32'>\n",
      "  - item_code_hash: shape=(256, 5), dtype=<dtype: 'int32'>\n",
      "  - submit_type_hash: shape=(256,), dtype=<dtype: 'int32'>\n",
      "  - tag_id_hash: shape=(256, 3), dtype=<dtype: 'int32'>\n",
      "  标签: shape=(256,), dtype=<dtype: 'int32'>\n"
     ]
    }
   ],
   "source": [
    "# 查看df数据集\n",
    "for features, labels in full_dataset.take(1):\n",
    "    print(\"数据集格式验证:\")\n",
    "    print(f\"  特征数量: {len(features)}\")\n",
    "    for name, tensor in features.items():\n",
    "        print(f\"  - {name}: shape={tensor.shape}, dtype={tensor.dtype}\")\n",
    "    print(f\"  标签: shape={labels.shape}, dtype={labels.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 模型结构定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7.1 Embedding层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- class FeaturePipelineBuilder:\n",
    "    实现了对 sparse；varlen_sparse；dense 三类特征的Embedding原子操作（基于tf2）\n",
    "    定义并返回一个新的特征embedding的pipeline\n",
    "- def process_feature_batch(features_dict: dict, pipelines: list) -> list:\n",
    "    原子操作合并成process函数作用与原始数据（这里就是前向传播的时候call()流过来的数据）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturePipelineBuilder:\n",
    "    \"\"\"特征处理管道构建器 - 基于原项目实现\"\"\"\n",
    "    \n",
    "    def __init__(self, feat_configs: list, verbose: bool = True):\n",
    "        self.feat_configs = feat_configs\n",
    "        self.verbose = verbose\n",
    "        self.embedding_layers = {}\n",
    "        self.pooling_layers = {}\n",
    "    \n",
    "    def build_feature_pipelines(self) -> list:\n",
    "        \"\"\"构建特征处理管道\"\"\"\n",
    "        pipelines = []\n",
    "        \n",
    "        for config in self.feat_configs:\n",
    "            feat_name = config.get('feat_name')\n",
    "            feat_type = config.get('feat_type')\n",
    "            \n",
    "            if not feat_name or not feat_type:\n",
    "                continue\n",
    "                \n",
    "            # 创建处理器序列\n",
    "            processors = self._create_processors(config)\n",
    "            if processors:\n",
    "                pipelines.append((feat_name, processors))\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"成功构建 {len(pipelines)} 个特征处理管道\")\n",
    "            for feat_name, processors in pipelines:\n",
    "                processor_names = [p.__class__.__name__ for p in processors]\n",
    "                print(f\"  {feat_name}: {' -> '.join(processor_names)}\")\n",
    "        \n",
    "        return pipelines\n",
    "    \n",
    "    def _create_processors(self, config: dict) -> list:\n",
    "        \"\"\"根据特征类型创建处理器\"\"\"\n",
    "        feat_name = config['feat_name']\n",
    "        feat_type = config['feat_type']\n",
    "        vocab_size = config.get('vocabulary_size', 1000)\n",
    "        embed_dim = config.get('embedding_dim', 8)\n",
    "        \n",
    "        if feat_type == 'sparse':\n",
    "            embedding = Embedding(\n",
    "                input_dim=vocab_size,\n",
    "                output_dim=embed_dim,\n",
    "                name=f'{feat_name}_embedding',\n",
    "                mask_zero=False\n",
    "            )\n",
    "            return [embedding]\n",
    "        \n",
    "        elif feat_type == 'varlen_sparse':\n",
    "            embedding = Embedding(\n",
    "                input_dim=vocab_size,\n",
    "                output_dim=embed_dim,\n",
    "                name=f'{feat_name}_embedding',\n",
    "                mask_zero=True  # 变长特征需要masking\n",
    "            )\n",
    "            pooling = GlobalAveragePooling1D(name=f'{feat_name}_pooling')\n",
    "            return [embedding, pooling]\n",
    "        \n",
    "        elif feat_type == 'dense':\n",
    "            # 密集特征直接通过，可以加BN\n",
    "            identity = tf.keras.layers.Lambda(lambda x: x, name=f'{feat_name}_identity')\n",
    "            return [identity]\n",
    "        \n",
    "        return []\n",
    "\n",
    "def process_feature_batch(features_dict: dict, pipelines: list) -> list:\n",
    "    \"\"\"处理特征批次数据\"\"\"\n",
    "    outputs = []\n",
    "    \n",
    "    for feat_name, processors in pipelines:\n",
    "        if feat_name not in features_dict:\n",
    "            continue\n",
    "        \n",
    "        # 依次应用处理器\n",
    "        feature_input = features_dict[feat_name]\n",
    "        for processor in processors:\n",
    "            feature_input = processor(feature_input)\n",
    "        \n",
    "        outputs.append(feature_input)\n",
    "    \n",
    "    return outputs\n",
    "    \"\"\"准备TensorFlow数据集用于深度模型训练\"\"\"\n",
    "    \n",
    "    # 获取需要的特征 - 适配不同的配置文件格式\n",
    "    if 'pipelines' in feat_config:\n",
    "        # feat.yml 格式\n",
    "        pipelines = feat_config['pipelines']\n",
    "    elif 'process' in feat_config and 'pipelines' in feat_config['process']:\n",
    "        # config.yml 格式\n",
    "        pipelines = feat_config['process']['pipelines']\n",
    "    else:\n",
    "        raise ValueError(\"无法找到特征配置中的 pipelines\")\n",
    "    \n",
    "    pipeline_feats = {p['feat_name']: p for p in pipelines}\n",
    "    features_dict = {}\n",
    "    \n",
    "    for feat_name, config in pipeline_feats.items():\n",
    "        if feat_name not in df.columns:\n",
    "            continue\n",
    "        \n",
    "        feat_type = config.get('feat_type', 'sparse')\n",
    "        \n",
    "        if feat_type == 'sparse':\n",
    "            # 单值特征，shape为 (batch_size,) - 这是关键！\n",
    "            values = df[feat_name].values.astype(np.int32)\n",
    "            features_dict[feat_name] = values\n",
    "            \n",
    "        elif feat_type == 'varlen_sparse':\n",
    "            # 变长特征，需要padding\n",
    "            sequences = df[feat_name].tolist()\n",
    "            max_len = max(len(seq) if isinstance(seq, list) else 1 for seq in sequences)\n",
    "            \n",
    "            padded_sequences = []\n",
    "            for seq in sequences:\n",
    "                if isinstance(seq, list):\n",
    "                    padded = seq + [0] * (max_len - len(seq))\n",
    "                else:\n",
    "                    padded = [int(seq)] + [0] * (max_len - 1)\n",
    "                padded_sequences.append(padded[:max_len])\n",
    "            \n",
    "            features_dict[feat_name] = np.array(padded_sequences, dtype=np.int32)\n",
    "            \n",
    "        elif feat_type == 'dense':\n",
    "            # 密集特征\n",
    "            values = df[feat_name].values.astype(np.float32).reshape(-1, 1)\n",
    "            features_dict[feat_name] = values\n",
    "    \n",
    "    # 准备标签\n",
    "    labels = df['label'].values.astype(np.int32)\n",
    "    \n",
    "    # 创建数据集\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features_dict, labels))\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'pipelines' in deep_feat_config:\n",
    "    deep_pipelines = deep_feat_config['pipelines']\n",
    "elif 'process' in deep_feat_config and 'pipelines' in deep_feat_config['process']:\n",
    "    deep_pipelines = deep_feat_config['process']['pipelines']\n",
    "else:\n",
    "    raise ValueError(\"无法找到深度模型特征配置中的 pipelines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7.2 模型构建（Embedding层 + MLP）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepMLP(tf.keras.Model):\n",
    "    \"\"\"深度MLP模型 - 基于原项目实现\"\"\"\n",
    "    \n",
    "    def __init__(self, feat_configs: list, train_config: dict = None, verbose: bool = True):\n",
    "        super(DeepMLP, self).__init__()\n",
    "        \n",
    "        # 构建特征处理管道\n",
    "        pipeline_builder = FeaturePipelineBuilder(feat_configs, verbose=verbose)\n",
    "        self.feature_pipelines = pipeline_builder.build_feature_pipelines()\n",
    "        \n",
    "        # 特征连接层\n",
    "        self.concat_layer = Concatenate(axis=1)\n",
    "        \n",
    "        # 获取模型参数 - 从train.yml动态读取\n",
    "        model_params = (train_config or {}).get('model', {})\n",
    "        hidden_layers = model_params.get('layers', [128, 64, 32])  # 使用train.yml的默认值\n",
    "        dropout_rates = model_params.get('dropout_rates', [0.3, 0.3, 0.2])  # 使用train.yml的默认值\n",
    "        l2_reg = model_params.get('l2_regularization', 0.001)\n",
    "        \n",
    "        # 构建分类器\n",
    "        self.classifier = self._build_classifier(hidden_layers, dropout_rates, l2_reg)\n",
    "    \n",
    "    def _build_classifier(self, hidden_layers: list, dropout_rates: list, l2_reg: float):\n",
    "        \"\"\"构建分类器网络\"\"\"\n",
    "        layers = []\n",
    "        \n",
    "        # 添加BatchNorm\n",
    "        layers.append(BatchNormalization())\n",
    "        \n",
    "        # 添加隐藏层\n",
    "        for i, units in enumerate(hidden_layers):\n",
    "            layers.append(Dense(\n",
    "                units, \n",
    "                activation='relu',\n",
    "                kernel_regularizer=l2(l2_reg)\n",
    "            ))\n",
    "            if i < len(dropout_rates):\n",
    "                layers.append(Dropout(dropout_rates[i]))\n",
    "        \n",
    "        # 输出层\n",
    "        layers.append(Dense(1, activation='sigmoid', kernel_regularizer=l2(l2_reg)))\n",
    "        \n",
    "        return tf.keras.Sequential(layers)\n",
    "    \n",
    "    def call(self, features, training=None):\n",
    "        \"\"\"前向传播\"\"\"\n",
    "        # 处理所有特征\n",
    "        processed_outputs = process_feature_batch(features, self.feature_pipelines)\n",
    "        \n",
    "        if not processed_outputs:\n",
    "            raise ValueError(\"没有有效的特征输出\")\n",
    "        \n",
    "        # 合并特征\n",
    "        if len(processed_outputs) > 1:\n",
    "            concat_features = self.concat_layer(processed_outputs)\n",
    "        else:\n",
    "            concat_features = processed_outputs[0]\n",
    "        \n",
    "        # 应用分类器\n",
    "        predictions = self.classifier(concat_features, training=training)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功构建 12 个特征处理管道\n",
      "  hour: Embedding\n",
      "  weekday: Embedding\n",
      "  user_watch_stk_code_hash: Embedding -> GlobalAveragePooling1D\n",
      "  country_hash: Embedding\n",
      "  prefer_bid_code_hash: Embedding -> GlobalAveragePooling1D\n",
      "  hold_bid_code_hash: Embedding -> GlobalAveragePooling1D\n",
      "  user_propernoun_hash: Embedding -> GlobalAveragePooling1D\n",
      "  push_title_hash: Embedding\n",
      "  title_len: Embedding\n",
      "  item_code_hash: Embedding -> GlobalAveragePooling1D\n",
      "  submit_type_hash: Embedding\n",
      "  tag_id_hash: Embedding -> GlobalAveragePooling1D\n"
     ]
    }
   ],
   "source": [
    "deep_model = DeepMLP(deep_pipelines, train_config, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7.3 编译模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 编译模型\n",
    "learning_rate = train_config.get('training', {}).get('lr', 0.0005)  # 使用train.yml的默认值\n",
    "deep_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        tf.keras.metrics.AUC(name='auc'),\n",
    "        tf.keras.metrics.BinaryAccuracy(name='accuracy')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8.1 数据集划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 数据使用情况:\n",
      "  总数据量: 50,000\n",
      "  训练集: 40,000 (80.0%)\n",
      "  验证集: 10,000 (20.0%)\n",
      "  训练批次数: 156\n",
      "  验证批次数: 39\n"
     ]
    }
   ],
   "source": [
    "validation_split = train_config.get('training', {}).get('validation_split', 0.2)\n",
    "train_size = int((1 - validation_split) * len(df_deep_processed))\n",
    "\n",
    "print(f\"\\n📊 数据使用情况:\")\n",
    "print(f\"  总数据量: {len(df_deep_processed):,}\")\n",
    "print(f\"  训练集: {train_size:,} ({(1-validation_split)*100:.1f}%)\")\n",
    "print(f\"  验证集: {len(df_deep_processed)-train_size:,} ({validation_split*100:.1f}%)\")\n",
    "print(f\"  训练批次数: {train_size // batch_size}\")\n",
    "print(f\"  验证批次数: {(len(df_deep_processed)-train_size) // batch_size}\")\n",
    "\n",
    "train_dataset = full_dataset.take(train_size // batch_size)\n",
    "val_dataset = full_dataset.skip(train_size // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8.2 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "开始训练深度模型... (epochs=2)\n",
      "Epoch 1/2\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - accuracy: 0.7465 - auc: 0.7228 - loss: 0.7396 - val_accuracy: 0.8024 - val_auc: 0.8264 - val_loss: 0.6571\n",
      "Epoch 2/2\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.7938 - auc: 0.8093 - loss: 0.6395 - val_accuracy: 0.8044 - val_auc: 0.8282 - val_loss: 0.5839\n",
      "\n",
      "--- 深度模型训练完成 ---\n",
      "最终训练集 AUC: 0.8203\n",
      "最终验证集 AUC: 0.8282\n",
      "AUC差异: -0.0079\n",
      "深度模型架构总结:\n",
      "特征处理管道: 12 个\n",
      "分类器层数: 4\n",
      "总参数量: 505,417\n",
      "深度模型训练结果:\n",
      "验证集AUC: 0.8282\n",
      "训练集AUC: 0.8203\n"
     ]
    }
   ],
   "source": [
    "epochs = train_config.get('training', {}).get('epochs', 2)  # 使用train.yml的默认值\n",
    "print(f\"\\n开始训练深度模型... (epochs={epochs})\")\n",
    "\n",
    "history = deep_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 6. 输出结果\n",
    "print(\"\\n--- 深度模型训练完成 ---\")\n",
    "if 'val_auc' in history.history:\n",
    "    final_train_auc = history.history['auc'][-1]\n",
    "    final_val_auc = history.history['val_auc'][-1]\n",
    "    print(f\"最终训练集 AUC: {final_train_auc:.4f}\")\n",
    "    print(f\"最终验证集 AUC: {final_val_auc:.4f}\")\n",
    "    print(f\"AUC差异: {final_train_auc - final_val_auc:.4f}\")\n",
    "else:\n",
    "    final_auc = history.history['auc'][-1]\n",
    "    print(f\"最终 AUC: {final_auc:.4f}\")\n",
    "\n",
    "print(f\"深度模型架构总结:\")\n",
    "print(f\"特征处理管道: {len(deep_model.feature_pipelines)} 个\")\n",
    "print(f\"分类器层数: {len([l for l in deep_model.classifier.layers if isinstance(l, Dense)])}\")\n",
    "print(f\"总参数量: {deep_model.count_params():,}\")\n",
    "\n",
    "# 显示深度模型训练结果\n",
    "print(f\"深度模型训练结果:\")\n",
    "if 'val_auc' in history.history:\n",
    "    print(f\"验证集AUC: {final_val_auc:.4f}\")\n",
    "    print(f\"训练集AUC: {history.history['auc'][-1]:.4f}\")\n",
    "else:\n",
    "    print(f\"AUC: {final_auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

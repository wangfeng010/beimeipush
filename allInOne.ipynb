{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 完整版本项目梳理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 树模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 环境设置与导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "from hashlib import md5\n",
    "from typing import Any, Dict, List, Union\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from glob import glob  # 添加glob模块导入\n",
    "import time\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加项目路径\n",
    "project_root = os.getcwd()\n",
    "env_path = os.path.join(project_root, 'env')\n",
    "if env_path not in sys.path:\n",
    "    sys.path.insert(0, env_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 加载原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检测到CSV文件，使用本地环境配置\n",
      "分隔符: ',', 表头行: 0, 文件数量: 5\n",
      "形状(50000, 13), 列名: ['user_id', 'create_time', 'log_type', 'watchlists', 'holdings', 'country', 'prefer_bid', 'user_propernoun', 'push_title', 'push_content', 'item_code', 'item_tags', 'submit_type']\n",
      "{'user_id': 1800001088, 'create_time': '2025-05-31 08:39:07', 'log_type': 'PR', 'watchlists': nan, 'holdings': nan, 'country': 'Germany', 'prefer_bid': nan, 'user_propernoun': 'germany#3.06|mid-america#1.02', 'push_title': 'Ainvest Newswire', 'push_content': 'Hims & Hers Health Lays Off 4% of Staff Amid Strategy Shift', 'item_code': '[{\"market\":\"169\",\"score\":0,\"code\":\"HIMS\",\"tagId\":\"U000012934\",\"name\":\"Hims & Hers Health\",\"type\":0,\"parentId\":\"US_ROBOT0f37d7fd3fca6a41\"},{\"market\":\"169\",\"score\":0,\"code\":\"NVO\",\"tagId\":\"U000002999\",\"name\":\"Novo Nordisk\",\"type\":0,\"parentId\":\"US_ROBOT0f37d7fd3fca6a41\"}]', 'item_tags': '[{\"score\":0.7803922295570374,\"tagId\":\"51510\",\"name\":\"us_high_importance\",\"type\":4,\"parentId\":\"US_ROBOT0f37d7fd3fca6a41\"},{\"score\":0.7803922295570374,\"tagId\":\"57967\",\"name\":\"Fusion\",\"type\":4,\"parentId\":\"US_ROBOT0f37d7fd3fca6a41\"},{\"tagId\":\"1002\",\"name\":\"no_penny_stock\",\"type\":4,\"parentId\":\"US_ROBOT0f37d7fd3fca6a41\"}]', 'submit_type': 'autoFlash'}\n"
     ]
    }
   ],
   "source": [
    "# 02 根据配置文件加载原始数据\n",
    "def load_raw_data_from_config():\n",
    "    \"\"\"根据data.yml配置文件加载原始数据，支持本地和线上环境\"\"\"\n",
    "    \n",
    "    # 加载数据配置\n",
    "    with open('config/data.yml', 'r', encoding='utf-8') as f:\n",
    "        data_config = yaml.safe_load(f)\n",
    "    \n",
    "    # 检测文件类型和环境\n",
    "    train_dir = data_config['train_dir']\n",
    "    \n",
    "    # 检查CSV和TXT文件\n",
    "    csv_files = glob(os.path.join(train_dir, '*.csv'))\n",
    "    txt_files = glob(os.path.join(train_dir, '*.txt'))\n",
    "    \n",
    "    if csv_files:\n",
    "        # 本地环境 - 使用CSV格式\n",
    "        print(\"检测到CSV文件，使用本地环境配置\")\n",
    "        csv_config = data_config['csv_format']\n",
    "        separator, header = csv_config['separator'], csv_config['header']\n",
    "        \n",
    "        print(f\"分隔符: '{separator}', 表头行: {header}, 文件数量: {len(csv_files)}\")\n",
    "        \n",
    "        # 读取CSV文件\n",
    "        dfs = [pd.read_csv(f, sep=separator, header=header) for f in csv_files]\n",
    "        \n",
    "    elif txt_files:\n",
    "        # 线上环境 - 使用TXT格式\n",
    "        print(\"检测到TXT文件，使用线上环境配置\")\n",
    "        txt_config = data_config.get('txt_format', {'separator': '\\t', 'header': None})\n",
    "        separator, header = txt_config['separator'], txt_config['header']\n",
    "        raw_columns = list(data_config['raw_data_columns'].keys())\n",
    "        \n",
    "        print(f\"分隔符: '{separator}', 表头行: {header}, 文件数量: {len(txt_files)}\")\n",
    "        print(f\"预定义列名: {raw_columns}\")\n",
    "        \n",
    "        # 读取TXT文件\n",
    "        dfs = [pd.read_csv(f, sep=separator, header=header, names=raw_columns) for f in txt_files]\n",
    "    else:\n",
    "        raise ValueError(f\"在目录 {train_dir} 中未找到CSV或TXT文件\")\n",
    "    \n",
    "    df_raw = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"形状{df_raw.shape}, 列名: {list(df_raw.columns)}\")\n",
    "    \n",
    "    return df_raw\n",
    "\n",
    "# 加载原始数据\n",
    "df_raw = load_raw_data_from_config()\n",
    "\n",
    "# 显示数据样例（简化版）\n",
    "print(df_raw.iloc[0].to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 加载并解析YAML配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'process': {'embedding_dim': 8, 'pooling_type': 'sum', 'pipelines': [{'feat_name': 'hour', 'feat_type': 'sparse', 'vocabulary_size': 24, 'embedding_dim': 8, 'input_sample': '2024-08-02 00:44:05', 'operations': [{'col_in': 'create_time', 'col_out': 'create_time', 'func_name': 'fillna', 'func_parameters': {'na_value': '2024-08-02 00:16:34'}}, {'col_in': 'create_time', 'col_out': 'hour', 'func_name': 'to_hour', 'func_parameters': {}}]}, {'feat_name': 'weekday', 'feat_type': 'sparse', 'vocabulary_size': 7, 'embedding_dim': 8, 'input_sample': '2024-08-02 00:44:05', 'operations': [{'col_in': 'create_time', 'col_out': 'weekday', 'func_name': 'to_weekday', 'func_parameters': {}}]}, {'feat_name': 'user_watch_stk_code_hash', 'feat_type': 'varlen_sparse', 'vocabulary_size': 10000, 'embedding_dim': 8, 'input_sample': 'AAPL_185 & TSLA_185', 'operations': [{'col_in': 'watchlists', 'col_out': 'watchlists', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null_0 & null_0'}}, {'col_in': 'watchlists', 'col_out': 'watchlists', 'func_name': 'split', 'func_parameters': {'sep': ' & '}}, {'col_in': 'watchlists', 'col_out': 'watchlists', 'func_name': 'seperation', 'func_parameters': {'sep': '_'}}, {'col_in': 'watchlists', 'col_out': 'user_watch_stk_code', 'func_name': 'list_get', 'func_parameters': {'item_index': 0}}, {'col_in': 'user_watch_stk_code', 'col_out': 'user_watch_stk_code', 'func_name': 'remove_items', 'func_parameters': {'target_values': ['AAPL', 'AMZN', 'GOOGL', 'TSLA']}}, {'col_in': 'user_watch_stk_code', 'col_out': 'user_watch_stk_code', 'func_name': 'padding', 'func_parameters': {'max_len': 5, 'pad_value': 'null'}}, {'col_in': 'user_watch_stk_code', 'col_out': 'user_watch_stk_code_hash', 'func_name': 'list_hash', 'func_parameters': {'vocabulary_size': 10000}}]}, {'feat_name': 'country_hash', 'feat_type': 'sparse', 'vocabulary_size': 200, 'embedding_dim': 8, 'input_sample': 'United States', 'operations': [{'col_in': 'country', 'col_out': 'country', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null'}}, {'col_in': 'country', 'col_out': 'country_hash', 'func_name': 'str_hash', 'func_parameters': {'vocabulary_size': 200}}]}, {'feat_name': 'prefer_bid_code_hash', 'feat_type': 'varlen_sparse', 'vocabulary_size': 10000, 'embedding_dim': 8, 'input_sample': 'AAPL#0.24809|AMZN#0.24809|GOOGL#0.24809|TSLA#0.24809', 'operations': [{'col_in': 'prefer_bid', 'col_out': 'prefer_bid', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null#0'}}, {'col_in': 'prefer_bid', 'col_out': 'prefer_bid', 'func_name': 'split', 'func_parameters': {'sep': '|'}}, {'col_in': 'prefer_bid', 'col_out': 'prefer_bid', 'func_name': 'seperation', 'func_parameters': {'sep': '#'}}, {'col_in': 'prefer_bid', 'col_out': 'prefer_bid_code', 'func_name': 'list_get', 'func_parameters': {'item_index': 0}}, {'col_in': 'prefer_bid_code', 'col_out': 'prefer_bid_code', 'func_name': 'padding', 'func_parameters': {'max_len': 5, 'pad_value': 'null'}}, {'col_in': 'prefer_bid_code', 'col_out': 'prefer_bid_code_hash', 'func_name': 'list_hash', 'func_parameters': {'vocabulary_size': 10000}}]}, {'feat_name': 'hold_bid_code_hash', 'feat_type': 'varlen_sparse', 'vocabulary_size': 10000, 'embedding_dim': 8, 'input_sample': 'JD,185|BAC,169|TSLA,185', 'operations': [{'col_in': 'holdings', 'col_out': 'holdings', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null,0'}}, {'col_in': 'holdings', 'col_out': 'holdings', 'func_name': 'split', 'func_parameters': {'sep': '|'}}, {'col_in': 'holdings', 'col_out': 'holdings', 'func_name': 'seperation', 'func_parameters': {'sep': ','}}, {'col_in': 'holdings', 'col_out': 'hold_bid_code', 'func_name': 'list_get', 'func_parameters': {'item_index': 0}}, {'col_in': 'hold_bid_code', 'col_out': 'hold_bid_code', 'func_name': 'padding', 'func_parameters': {'max_len': 5, 'pad_value': 'null'}}, {'col_in': 'hold_bid_code', 'col_out': 'hold_bid_code_hash', 'func_name': 'list_hash', 'func_parameters': {'vocabulary_size': 10000}}]}, {'feat_name': 'user_propernoun_hash', 'feat_type': 'varlen_sparse', 'vocabulary_size': 10000, 'embedding_dim': 8, 'input_sample': 'apple#1.02|nike#1.02', 'operations': [{'col_in': 'user_propernoun', 'col_out': 'user_propernoun', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null#0'}}, {'col_in': 'user_propernoun', 'col_out': 'user_propernoun', 'func_name': 'split', 'func_parameters': {'sep': '|'}}, {'col_in': 'user_propernoun', 'col_out': 'user_propernoun', 'func_name': 'seperation', 'func_parameters': {'sep': '#'}}, {'col_in': 'user_propernoun', 'col_out': 'user_propernoun_code', 'func_name': 'list_get', 'func_parameters': {'item_index': 0}}, {'col_in': 'user_propernoun_code', 'col_out': 'user_propernoun_code', 'func_name': 'padding', 'func_parameters': {'max_len': 5, 'pad_value': 'null'}}, {'col_in': 'user_propernoun_code', 'col_out': 'user_propernoun_hash', 'func_name': 'list_hash', 'func_parameters': {'vocabulary_size': 10000}}]}, {'feat_name': 'push_title_hash', 'feat_type': 'sparse', 'vocabulary_size': 8, 'embedding_dim': 8, 'input_sample': 'Breaking News', 'operations': [{'col_in': 'push_title', 'col_out': 'push_title', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null'}}, {'col_in': 'push_title', 'col_out': 'push_title_hash', 'func_name': 'str_hash', 'func_parameters': {'vocabulary_size': 8}}]}, {'feat_name': 'title_len', 'feat_type': 'sparse', 'vocabulary_size': 32, 'embedding_dim': 8, 'input_sample': \"Hong Kong's Leading Broker Futu Securities Introduces Bitcoin and XRP Trading with Lucrative Incentives\", 'operations': [{'col_in': 'push_content', 'col_out': 'push_content', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null'}}, {'col_in': 'push_content', 'col_out': 'push_content', 'func_name': 'split', 'func_parameters': {'sep': ' '}}, {'col_in': 'push_content', 'col_out': 'title_len', 'func_name': 'list_len', 'func_parameters': {}}, {'col_in': 'title_len', 'col_out': 'title_len', 'func_name': 'int_max', 'func_parameters': {'max_value': 31}}]}, {'feat_name': 'item_code_hash', 'feat_type': 'varlen_sparse', 'vocabulary_size': 10000, 'embedding_dim': 8, 'input_sample': '[{\"market\":\"185\",\"score\":1,\"code\":\"META\",\"name\":\"Meta\",\"type\":0,\"parentId\":\"0339437d07195361\"}]', 'operations': [{'col_in': 'item_code', 'col_out': 'item_code', 'func_name': 'fillna', 'func_parameters': {'na_value': '[{\"market\":\"null\",\"score\":0,\"code\":\"null\",\"name\":\"null\",\"type\":\"null\",\"parentId\":\"null\"}]'}}, {'col_in': 'item_code', 'col_out': 'item_code', 'func_name': 'json_object_to_list', 'func_parameters': {'key': 'code'}}, {'col_in': 'item_code', 'col_out': 'item_code', 'func_name': 'padding', 'func_parameters': {'max_len': 5, 'pad_value': 'null'}}, {'col_in': 'item_code', 'col_out': 'item_code_hash', 'func_name': 'list_hash', 'func_parameters': {'vocabulary_size': 10000}}]}, {'feat_name': 'submit_type_hash', 'feat_type': 'sparse', 'vocabulary_size': 10, 'embedding_dim': 8, 'input_sample': 'auto_flash', 'operations': [{'col_in': 'submit_type', 'col_out': 'submit_type', 'func_name': 'fillna', 'func_parameters': {'na_value': 'null'}}, {'col_in': 'submit_type', 'col_out': 'submit_type_hash', 'func_name': 'str_hash', 'func_parameters': {'vocabulary_size': 10}}]}, {'feat_name': 'tag_id_hash', 'feat_type': 'varlen_sparse', 'vocabulary_size': 10000, 'embedding_dim': 8, 'input_sample': '[{\"score\":0,\"tagId\":\"57967\",\"name\":\"Fusion\",\"type\":4,\"parentId\":\"0339437d07195361\"}]', 'operations': [{'col_in': 'item_tags', 'col_out': 'item_tags', 'func_name': 'fillna', 'func_parameters': {'na_value': '[{\"score\":0,\"tagId\":\"null\",\"name\":\"null\",\"type\":0,\"parentId\":\"null\"}]'}}, {'col_in': 'item_tags', 'col_out': 'tagIds', 'func_name': 'json_object_to_list', 'func_parameters': {'key': 'tagId'}}, {'col_in': 'tagIds', 'col_out': 'tagIds', 'func_name': 'padding', 'func_parameters': {'max_len': 3, 'pad_value': 'null'}}, {'col_in': 'tagIds', 'col_out': 'tag_id_hash', 'func_name': 'list_hash', 'func_parameters': {'vocabulary_size': 100}}]}]}, 'interactions': {'embedding_dim': 8, 'pooling_type': 'sum', 'pipelines': [{'feat_name': 'preder_bid_cross', 'feat_type': 'sparse', 'vocabulary_size': 2, 'embedding_dim': 8, 'operations': [{'col_in': ['item_code', 'prefer_bid_code'], 'col_out': 'preder_bid_cross', 'func_name': 'has_intersection', 'func_parameters': {'exclude': ['null', '0']}}]}, {'feat_name': 'watch_bid_cross', 'feat_type': 'sparse', 'vocabulary_size': 2, 'embedding_dim': 8, 'operations': [{'col_in': ['item_code', 'user_watch_stk_code'], 'col_out': 'watch_bid_cross', 'func_name': 'has_intersection', 'func_parameters': {'exclude': ['null', '0']}}]}, {'feat_name': 'hold_bid_cross', 'feat_type': 'sparse', 'vocabulary_size': 2, 'embedding_dim': 8, 'operations': [{'col_in': ['item_code', 'hold_bid_code'], 'col_out': 'hold_bid_cross', 'func_name': 'has_intersection', 'func_parameters': {'exclude': ['null', '0']}}]}]}, 'label_process': {'pipelines': [{'feat_name': 'log_type', 'feat_type': 'sparse', 'vocabulary_size': 2, 'embedding_dim': 8, 'input_sample': '0', 'operations': [{'col_in': 'log_type', 'col_out': 'log_type', 'func_name': 'fillna', 'func_parameters': {'na_value': 'PR'}}, {'col_in': 'log_type', 'col_out': 'log_type', 'func_name': 'map_to_int', 'func_parameters': {'map_dict': {'PR': 0, 'PC': 1}, 'default_code': 0}}]}]}}\n"
     ]
    }
   ],
   "source": [
    "# 加载树模型配置 (config.yml 包含特征和训练配置)\n",
    "with open('config/config.yml', 'r', encoding='utf-8') as f:\n",
    "    config_yml = yaml.safe_load(f)\n",
    "# 从 config.yml 提取树模型的特征配置\n",
    "tree_feat_config = config_yml['features']\n",
    "# 加载深度模型的特征配置 (feat.yml)\n",
    "with open('config/feat.yml', 'r', encoding='utf-8') as f:\n",
    "    deep_feat_config = yaml.safe_load(f)\n",
    "\n",
    "# 当前使用树模型的特征配置进行演示\n",
    "feat_config = tree_feat_config\n",
    "\n",
    "print(feat_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_dim': 8,\n",
      " 'feat_name': 'hour',\n",
      " 'feat_type': 'sparse',\n",
      " 'input_sample': '2024-08-02 00:44:05',\n",
      " 'operations': [{'col_in': 'create_time',\n",
      "                 'col_out': 'create_time',\n",
      "                 'func_name': 'fillna',\n",
      "                 'func_parameters': {'na_value': '2024-08-02 00:16:34'}},\n",
      "                {'col_in': 'create_time',\n",
      "                 'col_out': 'hour',\n",
      "                 'func_name': 'to_hour',\n",
      "                 'func_parameters': {}}],\n",
      " 'vocabulary_size': 24}\n"
     ]
    }
   ],
   "source": [
    "# 看一下一个pipeline中对一个特征的操作 他被解析成了什么结构\n",
    "# 适配 config.yml 和 feat.yml 的不同结构\n",
    "if 'pipelines' in feat_config:\n",
    "    # feat.yml 格式\n",
    "    example_pipeline = feat_config['pipelines'][0]\n",
    "elif 'process' in feat_config and 'pipelines' in feat_config['process']:\n",
    "    # config.yml 格式\n",
    "    example_pipeline = feat_config['process']['pipelines'][0]\n",
    "else:\n",
    "    print(\"无法找到特征配置格式\")\n",
    "    example_pipeline = None\n",
    "\n",
    "if example_pipeline:\n",
    "    pprint(example_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 进行数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1 定义原子操作函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OP_HUB 构建完成，包含 14 个操作函数\n",
      "可用函数: ['fillna', 'split', 'seperation', 'list_get', 'remove_items', 'padding', 'list_hash', 'str_hash', 'to_hour', 'to_weekday', 'list_len', 'int_max', 'json_object_to_list', 'map_to_int']\n"
     ]
    }
   ],
   "source": [
    "MISSING_VALUE = [None, '', 'null', 'NULL', 'None', np.nan]\n",
    "\n",
    "def fillna(x: Union[float, int, str], na_value: Union[float, int, str]) -> Union[float, int, str]:\n",
    "    \"\"\"填充缺失值\"\"\"\n",
    "    if x in MISSING_VALUE or (isinstance(x, float) and pd.isna(x)):\n",
    "        return na_value\n",
    "    return x\n",
    "\n",
    "def split(x: str, sep: str) -> List[str]:\n",
    "    \"\"\"字符串分割\"\"\"\n",
    "    return str(x).split(sep)\n",
    "\n",
    "def seperation(x: List[str], sep: str) -> List[List[str]]:\n",
    "    \"\"\"列表元素二次分割\"\"\"\n",
    "    if not isinstance(x, list):\n",
    "        return []\n",
    "    return [item.split(sep) for item in x]\n",
    "\n",
    "def list_get(x: List[List[Any]], item_index: int) -> List[Any]:\n",
    "    \"\"\"获取嵌套列表中指定位置的元素\"\"\"\n",
    "    if not isinstance(x, list):\n",
    "        return []\n",
    "    result = []\n",
    "    for sublist in x:\n",
    "        if isinstance(sublist, list) and len(sublist) > item_index:\n",
    "            result.append(sublist[item_index])\n",
    "        else:\n",
    "            result.append('null')\n",
    "    return result\n",
    "\n",
    "def remove_items(x: List[str], target_values: List[str]) -> List[str]:\n",
    "    \"\"\"移除列表中的指定元素\"\"\"\n",
    "    if not isinstance(x, list):\n",
    "        return []\n",
    "    return [item for item in x if item not in target_values]\n",
    "\n",
    "def padding(x: List[Any], pad_value: Union[str, float, int], max_len: int) -> List[Any]:\n",
    "    \"\"\"列表填充到指定长度\"\"\"\n",
    "    if not isinstance(x, list):\n",
    "        x = []\n",
    "    if len(x) >= max_len:\n",
    "        return x[:max_len]\n",
    "    else:\n",
    "        return x + [pad_value] * (max_len - len(x))\n",
    "\n",
    "def list_hash(x: List[str], vocabulary_size: int) -> List[int]:\n",
    "    \"\"\"对列表中每个元素进行哈希\"\"\"\n",
    "    if not isinstance(x, list):\n",
    "        return []\n",
    "    result = []\n",
    "    for item in x:\n",
    "        hash_val = int(md5(str(item).encode()).hexdigest(), 16) % vocabulary_size\n",
    "        result.append(hash_val)\n",
    "    return result\n",
    "\n",
    "def str_hash(x: str, vocabulary_size: int) -> int:\n",
    "    \"\"\"字符串哈希\"\"\"\n",
    "    return int(md5(str(x).encode()).hexdigest(), 16) % vocabulary_size\n",
    "\n",
    "def to_hour(x: str) -> int:\n",
    "    \"\"\"提取时间中的小时\"\"\"\n",
    "    try:\n",
    "        dt = pd.to_datetime(x)\n",
    "        return dt.hour\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def to_weekday(x: str) -> int:\n",
    "    \"\"\"提取时间中的星期\"\"\"\n",
    "    try:\n",
    "        dt = pd.to_datetime(x)\n",
    "        return dt.weekday()\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def list_len(x: List) -> int:\n",
    "    \"\"\"列表长度\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return len(x)\n",
    "    return 0\n",
    "\n",
    "def int_max(x: int, max_value: int) -> int:\n",
    "    \"\"\"限制整数最大值\"\"\"\n",
    "    return min(int(x), max_value)\n",
    "\n",
    "def json_object_to_list(x: str, key: str) -> List[str]:\n",
    "    \"\"\"从JSON对象列表中提取指定键的值\"\"\"\n",
    "    try:\n",
    "        data = json.loads(x)\n",
    "        if isinstance(data, list):\n",
    "            return [item.get(key, 'null') for item in data if isinstance(item, dict)]\n",
    "        return ['null']\n",
    "    except:\n",
    "        return ['null']\n",
    "\n",
    "def map_to_int(x: Union[str, List], map_dict: Dict[str, int], default_code: int = 0) -> Union[List[int], int]:\n",
    "    \"\"\"映射到整数\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return [map_dict.get(item, default_code) for item in x]\n",
    "    else:\n",
    "        return map_dict.get(str(x), default_code)\n",
    "\n",
    "# 构建操作中心 (OP_HUB)\n",
    "OP_HUB = {\n",
    "    'fillna': fillna,\n",
    "    'split': split,\n",
    "    'seperation': seperation,\n",
    "    'list_get': list_get,\n",
    "    'remove_items': remove_items,\n",
    "    'padding': padding,\n",
    "    'list_hash': list_hash,\n",
    "    'str_hash': str_hash,\n",
    "    'to_hour': to_hour,\n",
    "    'to_weekday': to_weekday,\n",
    "    'list_len': list_len,\n",
    "    'int_max': int_max,\n",
    "    'json_object_to_list': json_object_to_list,\n",
    "    'map_to_int': map_to_int\n",
    "}\n",
    "\n",
    "print(f\"OP_HUB 构建完成，包含 {len(OP_HUB)} 个操作函数\")\n",
    "print(f\"可用函数: {list(OP_HUB.keys())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 实现原子操作作用与df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_op(df: pd.DataFrame, operation: dict) -> pd.DataFrame:\n",
    "    \"\"\"执行单个特征操作\"\"\"\n",
    "    # 获取操作配置\n",
    "    col_in = operation['col_in']\n",
    "    col_out = operation['col_out']\n",
    "    func_name = operation['func_name']\n",
    "    parameters = operation.get('func_parameters', {})\n",
    "    \n",
    "    # 检查函数是否存在\n",
    "    if func_name not in OP_HUB:\n",
    "        return df\n",
    "    \n",
    "    # 检查输入列是否存在\n",
    "    input_cols = [col_in] if isinstance(col_in, str) else col_in\n",
    "    if not all(col in df.columns for col in input_cols):\n",
    "        return df\n",
    "    \n",
    "    # 准备特征转换函数\n",
    "    transform_func = partial(OP_HUB[func_name], **parameters)\n",
    "    \n",
    "    # 执行特征转换\n",
    "    if isinstance(col_in, list):\n",
    "        df[col_out] = df[col_in].apply(lambda row: transform_func(*row), axis=1)\n",
    "    else:\n",
    "        df[col_out] = df[col_in].apply(transform_func)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.3 实现原子操作拼接成完整的process函数作用与df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_feature_pipelines(df_raw: pd.DataFrame, feat_config: dict) -> tuple[pd.DataFrame, list]:\n",
    "    \"\"\"执行特征工程流水线 - 适配不同的配置文件格式\"\"\"\n",
    "    # 创建数据副本\n",
    "    df = df_raw.copy()\n",
    "    \n",
    "    # 获取需要处理的流水线 - 适配不同格式\n",
    "    if 'pipelines' in feat_config:\n",
    "        # feat.yml 格式: { pipelines: [...] }\n",
    "        pipelines = feat_config['pipelines']\n",
    "    elif 'process' in feat_config and 'pipelines' in feat_config['process']:\n",
    "        # config.yml 格式: { process: { pipelines: [...] } }\n",
    "        pipelines = feat_config['process']['pipelines']\n",
    "    else:\n",
    "        print(\"⚠️ 无法找到特征配置中的 pipelines\")\n",
    "        return df, []\n",
    "\n",
    "    # 记录成功处理的特征\n",
    "    processed_features = []\n",
    "    \n",
    "    # 执行每个特征处理流水线\n",
    "    for pipeline in pipelines:\n",
    "        feat_name = pipeline['feat_name']\n",
    "        operations = pipeline['operations']\n",
    "        \n",
    "        # 执行流水线中的每个操作\n",
    "        for operation in operations:\n",
    "            df = run_one_op(df, operation)\n",
    "\n",
    "        # 记录处理成功的特征\n",
    "        processed_features.append(feat_name)\n",
    "    \n",
    "    return df, processed_features\n",
    "\n",
    "df_processed, processed_features = process_feature_pipelines(df_raw, feat_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 特征处理后数据集分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据结构对比:\n",
      "原始列数: 13\n",
      "处理后列数: 30\n",
      "新增列数: 17\n",
      "\n",
      "原始列名:\n",
      "['user_id', 'create_time', 'log_type', 'watchlists', 'holdings', 'country', 'prefer_bid', 'user_propernoun', 'push_title', 'push_content', 'item_code', 'item_tags', 'submit_type']\n",
      "\n",
      "新增列名:\n",
      "['hour', 'weekday', 'user_watch_stk_code', 'user_watch_stk_code_hash', 'country_hash', 'prefer_bid_code', 'prefer_bid_code_hash', 'hold_bid_code', 'hold_bid_code_hash', 'user_propernoun_code', 'user_propernoun_hash', 'push_title_hash', 'title_len', 'item_code_hash', 'submit_type_hash', 'tagIds', 'tag_id_hash']\n",
      "成功生成的特征详情:\n",
      "  hour: int64 = 8\n",
      "  weekday: int64 = 5\n",
      "  user_watch_stk_code_hash: list = [8381, 8381, 8381, 8381, 8381]\n",
      "  country_hash: int64 = 71\n",
      "  prefer_bid_code_hash: list = [8381, 8381, 8381, 8381, 8381]\n",
      "  hold_bid_code_hash: list = [8381, 8381, 8381, 8381, 8381]\n",
      "  user_propernoun_hash: list = [178, 417, 8381, 8381, 8381]\n",
      "  push_title_hash: int64 = 7\n",
      "  title_len: int64 = 12\n",
      "  item_code_hash: list = [6837, 3491, 8381, 8381, 8381]\n",
      "  submit_type_hash: int64 = 6\n",
      "  tag_id_hash: list = [39, 93, 80]\n",
      "最终处理结果预览:\n",
      "      user_id log_type  hour  weekday        user_watch_stk_code_hash  \\\n",
      "0  1800001088       PR     8        5  [8381, 8381, 8381, 8381, 8381]   \n",
      "1  1800001417       PR    22        5  [8381, 8381, 8381, 8381, 8381]   \n",
      "2  1800001501       PC    10        5  [1895, 8808, 1021, 8381, 8381]   \n",
      "3  1800001501       PR    22        5  [1895, 8808, 1021, 8381, 8381]   \n",
      "4  1800001819       PR    21        5  [8381, 8381, 8381, 8381, 8381]   \n",
      "\n",
      "   country_hash            prefer_bid_code_hash  \\\n",
      "0            71  [8381, 8381, 8381, 8381, 8381]   \n",
      "1           145  [8381, 8381, 8381, 8381, 8381]   \n",
      "2           145  [8381, 8381, 8381, 8381, 8381]   \n",
      "3           145  [8381, 8381, 8381, 8381, 8381]   \n",
      "4           145  [8381, 8381, 8381, 8381, 8381]   \n",
      "\n",
      "               hold_bid_code_hash            user_propernoun_hash  \\\n",
      "0  [8381, 8381, 8381, 8381, 8381]    [178, 417, 8381, 8381, 8381]   \n",
      "1  [8381, 8381, 8381, 8381, 8381]  [8381, 8381, 8381, 8381, 8381]   \n",
      "2  [8381, 8381, 8381, 8381, 8381]   [323, 9351, 3453, 8381, 8381]   \n",
      "3  [8381, 8381, 8381, 8381, 8381]   [323, 9351, 3453, 8381, 8381]   \n",
      "4  [8381, 8381, 8381, 8381, 8381]  [8381, 8381, 8381, 8381, 8381]   \n",
      "\n",
      "   push_title_hash  title_len                  item_code_hash  \\\n",
      "0                7         12  [6837, 3491, 8381, 8381, 8381]   \n",
      "1                7         10  [7762, 6902, 6157, 1986, 5551]   \n",
      "2                5         17  [9724, 8381, 8381, 8381, 8381]   \n",
      "3                7         10  [7762, 6902, 6157, 1986, 5551]   \n",
      "4                4         11   [916, 8381, 8381, 8381, 8381]   \n",
      "\n",
      "   submit_type_hash   tag_id_hash  \n",
      "0                 6  [39, 93, 80]  \n",
      "1                 4   [34, 6, 27]  \n",
      "2                 1   [1, 80, 97]  \n",
      "3                 4   [34, 6, 27]  \n",
      "4                 1  [93, 80, 81]  \n"
     ]
    }
   ],
   "source": [
    "print(\"数据结构对比:\")\n",
    "print(f\"原始列数: {len(df_raw.columns)}\")\n",
    "print(f\"处理后列数: {len(df_processed.columns)}\")\n",
    "print(f\"新增列数: {len(df_processed.columns) - len(df_raw.columns)}\")\n",
    "\n",
    "print(\"\\n原始列名:\")\n",
    "print(list(df_raw.columns))\n",
    "\n",
    "print(\"\\n新增列名:\")\n",
    "new_columns = [col for col in df_processed.columns if col not in df_raw.columns]\n",
    "print(new_columns)\n",
    "\n",
    "# 查看成功生成的特征\n",
    "print(\"成功生成的特征详情:\")\n",
    "for feat_name in processed_features:\n",
    "    if feat_name in df_processed.columns:\n",
    "        sample_data = df_processed[feat_name].iloc[0]\n",
    "        data_type = type(sample_data).__name__\n",
    "        print(f\"  {feat_name}: {data_type} = {sample_data}\")\n",
    "\n",
    "print(\"最终处理结果预览:\")\n",
    "display_cols = ['user_id', 'log_type'] + processed_features\n",
    "display_cols = [col for col in display_cols if col in df_processed.columns]\n",
    "\n",
    "print(df_processed[display_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 模型训练与评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(df_processed, processed_features, max_list_length=5):\n",
    "    \"\"\"展开列表特征 树模型的需求\"\"\"\n",
    "    df_tree = df_processed[processed_features].copy()\n",
    "    \n",
    "    for feat in processed_features:\n",
    "        if isinstance(df_tree[feat].iloc[0], list):\n",
    "            expanded = df_tree[feat].apply(pd.Series).iloc[:, :max_list_length]\n",
    "            expanded.columns = [f\"{feat}_{i}\" for i in range(expanded.shape[1])]\n",
    "            df_tree = df_tree.drop(columns=[feat]).join(expanded)\n",
    "    \n",
    "    return df_tree\n",
    "\n",
    "def train_model(X, y, train_params):\n",
    "    \"\"\"训练LightGBM模型\"\"\"\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train, y_train)\n",
    "    val_data = lgb.Dataset(X_val, y_val, reference=train_data)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        train_params,\n",
    "        train_data,\n",
    "        num_boost_round=train_params.pop('num_iterations', 1000),\n",
    "        callbacks=[lgb.early_stopping(train_params.pop('early_stopping_rounds', 100))],\n",
    "        valid_sets=[train_data, val_data],\n",
    "        valid_names=['train', 'valid']\n",
    "    )\n",
    "    \n",
    "    return model, X_train, X_val, y_train, y_val\n",
    "\n",
    "def evaluate_model(model, X_train, X_val, y_train, y_val):\n",
    "    \"\"\"评估模型性能\"\"\"\n",
    "    y_train_pred = model.predict(X_train, num_iteration=model.best_iteration)\n",
    "    y_val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    \n",
    "    train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "    val_auc = roc_auc_score(y_val, y_val_pred)\n",
    "    \n",
    "    print(f\"训练集 AUC: {train_auc:.4f}\")\n",
    "    print(f\"验证集 AUC: {val_auc:.4f}\")\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'feature': model.feature_name(),\n",
    "        'importance': model.feature_importance(importance_type='gain')\n",
    "    }).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[300]\ttrain's auc: 0.864733\tvalid's auc: 0.847046\n",
      "训练集 AUC: 0.8647\n",
      "验证集 AUC: 0.8470\n",
      "\n",
      "特征重要性 (Top 20):\n",
      "                       feature     importance\n",
      "23      user_propernoun_hash_2  346798.388273\n",
      "21      user_propernoun_hash_0  115543.097794\n",
      "2                 country_hash   60724.277898\n",
      "22      user_propernoun_hash_1   55430.613762\n",
      "1                      weekday   23594.930362\n",
      "0                         hour   10044.143828\n",
      "32               tag_id_hash_1    5167.244439\n",
      "5             submit_type_hash    4476.428500\n",
      "10  user_watch_stk_code_hash_4    4473.375479\n",
      "31               tag_id_hash_0    4001.311887\n",
      "33               tag_id_hash_2    3325.424608\n",
      "4                    title_len    3310.380234\n",
      "6   user_watch_stk_code_hash_0    3213.638852\n",
      "3              push_title_hash    3065.194983\n",
      "7   user_watch_stk_code_hash_1    2862.350930\n",
      "8   user_watch_stk_code_hash_2    2614.035170\n",
      "9   user_watch_stk_code_hash_3    1983.679067\n",
      "11      prefer_bid_code_hash_0    1954.205699\n",
      "13      prefer_bid_code_hash_2    1802.084255\n",
      "26            item_code_hash_0    1788.037595\n"
     ]
    }
   ],
   "source": [
    "# 树模型使用config.yml配置\n",
    "with open('config/config.yml', 'r', encoding='utf-8') as f:\n",
    "    tree_config = yaml.safe_load(f)\n",
    "\n",
    "# 准备数据\n",
    "df_processed['label'] = df_processed['log_type'].apply(lambda x: 1 if x == 'PC' else 0)\n",
    "X = prepare_features(df_processed, processed_features)\n",
    "y = df_processed['label']\n",
    "\n",
    "# 训练模型\n",
    "train_params = {**tree_config['train'], 'verbose': -1, 'n_jobs': -1, 'seed': 42}\n",
    "model, X_train, X_val, y_train, y_val = train_model(X, y, train_params)\n",
    "\n",
    "# 评估并输出结果\n",
    "feature_importance = evaluate_model(model, X_train, X_val, y_train, y_val)\n",
    "print(\"\\n特征重要性 (Top 20):\")\n",
    "print(feature_importance.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 MLP模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
